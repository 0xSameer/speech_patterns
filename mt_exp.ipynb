{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import cPickle as pickle\n",
    "import json\n",
    "import subprocess\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "import bisect\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "from matplotlib.ticker import MultipleLocator, \\\n",
    "     FormatStrFormatter, AutoMinorLocator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"config.json\") as json_data_file:\n",
    "    config = json.load(json_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodes_fname = config[\"es\"]['nodes_fname']\n",
    "seg_nodes_fname = config[\"es\"]['seg_nodes_fname']\n",
    "nodes_dict_fname = config[\"es\"]['nodes_dict_fname']\n",
    "\n",
    "edges_utd_fname = config[\"es\"]['edges_utd_fname']\n",
    "edges_olap_fname = config[\"es\"]['edges_olap_fname']\n",
    "edges_all_fname = config[\"es\"]['edges_all_fname']\n",
    "edges_score_fname = config[\"es\"]['edges_score_fname']\n",
    "\n",
    "clusters_utd_fname = config['es']['clusters_utd_fname']\n",
    "clusters_fname = config['es']['clusters_fname']\n",
    "clusters_stats_fname = config['es']['clusters_stats_fname']\n",
    "\n",
    "pairs_fname = config['es']['score_pairs_fname']\n",
    "eval_fname = config['es']['eval_pairs_fname']\n",
    "\n",
    "feats_fname = config['es']['feats_fname']\n",
    "feats_dict_fname = config['es']['feats_dict_fname']\n",
    "\n",
    "gold_probs_fname = config['es']['mt_probs_gold']\n",
    "gold_probs_dict_fname = config['es']['mt_probs_dict_gold']\n",
    "\n",
    "pseudo_probs_fname = config['es']['mt_probs_pseudo']\n",
    "pseudo_probs_dict_fname = config['es']['mt_probs_dict_pseudo']\n",
    "\n",
    "train_segment_list_fname = config['es']['mt_train_files']\n",
    "gold_corpus_fname = config['es']['mt_corpus_train_gold']\n",
    "pseudo_corpus_fname = config['es']['mt_corpus_train_pseudo']\n",
    "\n",
    "gold_feats_dict_fname = config['es']['gold_feats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Align = namedtuple('Align', ['word', 'start', 'end'])\n",
    "Node = namedtuple('Node', ['file', 'seg', 'start', 'end', 'es', 'es_cnt'])\n",
    "Eval = namedtuple('Eval', ['n1', 'n2', 'dtw', 'es_sim', 'es_cnt_sim', 'en_j_sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segment_map = pickle.load(open(config['es']['segment_dict_fname'], \"rb\"))\n",
    "align_dict = pickle.load(open(config['es']['align_dict_fname'], \"rb\"))\n",
    "nodes_dict = pickle.load(open(nodes_dict_fname, \"rb\"))\n",
    "pairs_list = pickle.load(open(pairs_fname, \"rb\"))\n",
    "eval_dict = pickle.load(open(eval_fname, \"rb\"))\n",
    "clusters = pickle.load(open(clusters_fname, \"rb\"))\n",
    "clusters_stats = pickle.load(open(clusters_stats_fname, \"rb\"))\n",
    "feats_dict = pickle.load(open(feats_dict_fname, \"rb\"))\n",
    "gold_feats_dict = pickle.load(open(gold_feats_dict_fname, \"rb\"))\n",
    "\n",
    "if os.path.exists(gold_probs_dict_fname):\n",
    "    gold_probs_dict = pickle.load(open(gold_probs_dict_fname, \"rb\"))\n",
    "\n",
    "if os.path.exists(pseudo_probs_dict_fname):\n",
    "    pseudo_probs_dict = pickle.load(open(pseudo_probs_dict_fname, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create parallel corpus\n",
    "\n",
    "- Use list of files specified for training\n",
    "- Create golden parallel corpus, using es transcriptions\n",
    "- Create pseudotext ||| english parallel corpus\n",
    "- For English, filter for content words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def create_parallel_corpus(segment_list_fname, corpus_fname, pseudotext=False, es_w_key='es', en_w_key='en'):\n",
    "#     with open(corpus_fname, \"w\") as out_f, open(segment_list_fname) as in_f:\n",
    "#         for seg_id in in_f:\n",
    "#             seg_id = seg_id.strip()\n",
    "#             if pseudotext:\n",
    "#                 es_words = map(str, feats_dict[seg_id])\n",
    "#             else:\n",
    "#                 es_words = [w.word for w in align_dict[seg_id.split('.')[0]][seg_id][es_w_key]]\n",
    "#                 if not es_words:\n",
    "#                     es_words = ['-1']\n",
    "            \n",
    "#             en_words = [w.word for w in align_dict[seg_id.split('.')[0]][seg_id][en_w_key]]\n",
    "#             if not en_words:\n",
    "#                 en_words = [w.word for w in align_dict[seg_id.split('.')[0]][seg_id]['en']]\n",
    "            \n",
    "#             if es_words != ['-1']:\n",
    "#                 outline = \" \".join(es_words) + \" ||| \" + \" \".join(en_words) + \"\\n\"\n",
    "#                 out_f.write(outline)\n",
    "#     print(\"Finished generating MT corpus\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_parallel_corpus(segment_list_fname, corpus_fname, feats_dict, en_w_key='en'):\n",
    "    total_errors = 0\n",
    "    with open(corpus_fname, \"w\") as out_f, open(segment_list_fname) as in_f:\n",
    "        for seg_id in in_f:\n",
    "            seg_id = seg_id.strip()\n",
    "            es_words = feats_dict[seg_id]\n",
    "            if not es_words:\n",
    "                es_words = ['-1']\n",
    "                print('Aha')\n",
    "            \n",
    "            en_words = [w.word for w in align_dict[seg_id.split('.')[0]][seg_id][en_w_key]]\n",
    "            if not en_words:\n",
    "                en_words = [w.word for w in align_dict[seg_id.split('.')[0]][seg_id]['en']]\n",
    "            \n",
    "            if es_words != ['-1']:\n",
    "                outline = \" \".join(es_words) + \" ||| \" + \" \".join(en_words) + \"\\n\"\n",
    "                out_f.write(outline)\n",
    "            else:\n",
    "                total_errors += 1\n",
    "    print(\"Finished generating MT corpus\")\n",
    "    print(\"Not found for: %d utterances\" % total_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lol = [item for lst in feats_dict.values() for item in lst]\n",
    "'-1' in map(str, lol[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating MT corpus\n",
      "Not found for: 0 utterances\n"
     ]
    }
   ],
   "source": [
    "create_parallel_corpus(train_segment_list_fname, gold_corpus_fname, gold_feats_dict, en_w_key='en_cnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished generating MT corpus\n",
      "Not found for: 5506 utterances\n"
     ]
    }
   ],
   "source": [
    "create_parallel_corpus(train_segment_list_fname, pseudo_corpus_fname, feats_dict, en_w_key='en_cnt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MT model using *fast_align*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_mt(corpus_fname, probs_fname):\n",
    "    FASTALIGN = config['base']['fast_align']\n",
    "    subprocess.call([FASTALIGN,\"-i\", corpus_fname, \"-v\", \"-N\", \"-p\", probs_fname])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mt(gold_corpus_fname, gold_probs_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mt(pseudo_corpus_fname, pseudo_probs_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_mt_probs(probs_fname, probs_dict_fname):\n",
    "    probs_dict = {}\n",
    "    with open(probs_fname, \"r\") as in_f:\n",
    "        for i, line in enumerate(in_f):\n",
    "            line_items = line.strip().split()\n",
    "            es_w = line_items[0]\n",
    "            if es_w not in probs_dict:\n",
    "                probs_dict[es_w] = {}\n",
    "            en_w = line_items[1]\n",
    "            log_prob_val = float(line_items[2])\n",
    "            probs_dict[es_w][en_w] = log_prob_val\n",
    "    print(\"Finished reading mt probs file: %s\" % os.path.basename(probs_fname))\n",
    "    pickle.dump(probs_dict, open(probs_dict_fname, \"wb\"))\n",
    "    print(\"Finished saving mt probs dictionary: %s\" % os.path.basename(probs_dict_fname))\n",
    "    return probs_dict\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading mt probs file: mt_probs_gold.txt\n",
      "Finished saving mt probs dictionary: mt_probs_dict_gold.p\n"
     ]
    }
   ],
   "source": [
    "gold_probs_dict = save_mt_probs(gold_probs_fname, gold_probs_dict_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading mt probs file: mt_probs_pseudo.txt\n",
      "Finished saving mt probs dictionary: mt_probs_dict_pseudo.p\n"
     ]
    }
   ],
   "source": [
    "pseudo_probs_dict = save_mt_probs(pseudo_probs_fname, pseudo_probs_dict_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTERVENCI\\xc3\\xb3N',\n",
       " 'AGRADECIMIENTO',\n",
       " '\\xc3\\xa9XITO',\n",
       " 'BUSQUE',\n",
       " 'DILUY\\xc3\\xb3']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['1869', '13357', '13356', '19719', '11542']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'HOMEWORK': -1.0866}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(gold_probs_dict.keys()[:5])\n",
    "display(pseudo_probs_dict.keys()[:5])\n",
    "pseudo_probs_dict['1869']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-97-3205508e7f86>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-97-3205508e7f86>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    if pseudotext:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def mt_predict(pred_fname, pred_dict_fname, pseudotext=False, probs_dict):\n",
    "    \n",
    "    for fid in align_dict:\n",
    "        for seg_id in align_dict[fid]:\n",
    "            if pseudotext:\n",
    "                es_words = map(str, feats_dict[seg_id])\n",
    "            else:\n",
    "                es_words = [w.word for w in align_dict[seg_id.split('.')[0]][seg_id][es_w_key]]\n",
    "                if not es_words:\n",
    "                    es_words = ['-1']\n",
    "\n",
    "            en_words = [w.word for w in align_dict[seg_id.split('.')[0]][seg_id][en_w_key]]\n",
    "            if not en_words:\n",
    "                en_words = [w.word for w in align_dict[seg_id.split('.')[0]][seg_id]['en']]\n",
    "    \n",
    "    if k not in pred_dict:\n",
    "        pred_dict[k] = {}\n",
    "    \n",
    "    for fid in align_dict:\n",
    "        for segid in align_dict[fid]:\n",
    "            pred_dict[k][se] = \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([seg for fid in align_dict for seg in align_dict[fid]]) == sorted(seg for fid in segment_map for seg in segment_map[fid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'en': [Align(word='AND', start=3, end=11),\n",
       "  Align(word='THAT', start=11, end=39)],\n",
       " 'en_cnt': [],\n",
       " 'es': [Align(word='Y', start=3, end=11), Align(word='YA', start=11, end=39)],\n",
       " 'es_cnt': []}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_dict['041']['041.093']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485\n",
      "16909\n",
      "17394\n"
     ]
    }
   ],
   "source": [
    "haha = [1 for fid in align_dict for segid in align_dict[fid] if align_dict[fid][segid]['en_cnt'] == []]\n",
    "print(sum(haha))\n",
    "haha = [1 for fid in align_dict for segid in align_dict[fid] if align_dict[fid][segid]['en_cnt'] != []]\n",
    "print(sum(haha))\n",
    "print(485+16909)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['090',\n",
       " '091',\n",
       " '092',\n",
       " '093',\n",
       " '094',\n",
       " '095',\n",
       " '096',\n",
       " '097',\n",
       " '010',\n",
       " '011',\n",
       " '012',\n",
       " '013',\n",
       " '014',\n",
       " '015',\n",
       " '018',\n",
       " '025',\n",
       " '024',\n",
       " '027',\n",
       " '026',\n",
       " '021',\n",
       " '023',\n",
       " '022',\n",
       " '029',\n",
       " '028',\n",
       " '115',\n",
       " '114',\n",
       " '038',\n",
       " '116',\n",
       " '111',\n",
       " '110',\n",
       " '113',\n",
       " '112',\n",
       " '032',\n",
       " '033',\n",
       " '030',\n",
       " '031',\n",
       " '036',\n",
       " '037',\n",
       " '034',\n",
       " '035',\n",
       " '108',\n",
       " '049',\n",
       " '048',\n",
       " '047',\n",
       " '046',\n",
       " '100',\n",
       " '101',\n",
       " '043',\n",
       " '042',\n",
       " '104',\n",
       " '105',\n",
       " '058',\n",
       " '059',\n",
       " '103',\n",
       " '054',\n",
       " '056',\n",
       " '057',\n",
       " '050',\n",
       " '051',\n",
       " '052',\n",
       " '053',\n",
       " '044',\n",
       " '106',\n",
       " '107',\n",
       " '041',\n",
       " '040',\n",
       " '061',\n",
       " '060',\n",
       " '063',\n",
       " '062',\n",
       " '065',\n",
       " '064',\n",
       " '067',\n",
       " '066',\n",
       " '117',\n",
       " '039',\n",
       " '076',\n",
       " '077',\n",
       " '075',\n",
       " '072',\n",
       " '073',\n",
       " '070',\n",
       " '071',\n",
       " '045',\n",
       " '078',\n",
       " '079',\n",
       " '119',\n",
       " '118',\n",
       " '089',\n",
       " '088',\n",
       " '083',\n",
       " '082',\n",
       " '081',\n",
       " '087',\n",
       " '086',\n",
       " '085',\n",
       " '084',\n",
       " '002',\n",
       " '001',\n",
       " '007',\n",
       " '006',\n",
       " '005',\n",
       " '009',\n",
       " '120']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for fid in segment_map:\n",
    "    for seg_id in segment_map[fid]:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search speech\n",
    "\n",
    "- Take english words as a list, output speech utterances containing that English word\n",
    "- Calculate precision recall for retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
