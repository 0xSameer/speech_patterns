{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech pattern discovery and evaluation on the CallHome dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Term Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Time Warping algorithm for detecting repeated speech segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech data is recorded as a time series of signal amplitude (continuous) values. Segmental DTW was proposed by Alex Park et al. as an effective method of extracting patterns from speech data. The repeated terms can be word, sub-word or compound-word like units. In order to use speech data for downstream applications such as: Machine Translation, Query-by-Example, among others, these discovered terms provide a psuedo tokenisation of the continuous data. In this work, we use the Zero Resource Toolkit made available by Aren et al. The toolkit allows us to perform UTD in relativly quick time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use CALLHOME Spanish corpus, which consists of speech from telephone conversations between native speakers. Using the SPRACHcore (#insert_ref#) software package, we generate 39-dimensional Relative Spectral Transform - Perceptual Linear Prediction (PLP) feature vectors. Each dimension is further mean and variance normalised.\n",
    "\n",
    "For UTD experiment, we use speech from 20 telephone calls, with a total of 2.70 hours of voice activity. We use a force alignment of the speech input with transcriptions to generate a gold standard and also for determining the voice active regions. For all experiments, we assume that ground truth voice activity information is available. (#insert how to explain gold standard alignments#). To improve the output of the UTD system, we use the English translations provided as part of (#insert_ref#).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO (or not todo)\n",
    "\n",
    "- Suffix array over transcripts to extract gold standard pattern?\n",
    "- Alternately, extract words longer than 500 ms, and repeated atleast twice, in different utterance ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our goal is to quantitavely evaluate the discovered speech pairs. For an intrinsic evaluation, we use the force aligned transcriptions to compute an Average Precision over UTD output. To test extrinsically, we generate a UTD to English translation model and compute precision and recall metrics.\n",
    "\n",
    "We also attempt to quantify the improvements in UTD output, which can be achieved using additional information, which in this case is English translations. The translations can be considered as a form of noisy label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing UTD parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "| DTW score        | Duration (ms) |  \n",
    "| ------------- |:-------------:|\n",
    "| 0.80     | 500 |\n",
    "\n",
    "UTD systems are sensitive to silent and background speech regions .To eliminate these regions from the discovery process, we use the following two methods for Voice Activity Detection (VAD):\n",
    "1. VAD information extracted from forced alignment with transcriptions\n",
    "2. VAD information extracted using an energy detection script\n",
    "\n",
    "| VAD using force alignment with transcriptions    | VAD using energy detection |  \n",
    "| :------------- |:-------------|\n",
    "| Moderately strict filtering | Aggressive filtering. Removes a lot of voice activity regions in addition to silence and background noise |\n",
    "| Voice Active regions detected = 7 hours out of 8.50 hours |Voice Active regions detected = 2.30 hours out of 8.50 hours |\n",
    "|Due to imperfect alignments, some silent regions left unfiltered. These lead to invalid UTD output which is difficult to identify | Better quality input data for UTD |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTD output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th>Description</th>\n",
    "        <th># of pairs</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Total pairs</td>\n",
    "        <td>2173</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>With sil, sp, or no matches</td>\n",
    "        <td>78</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Pairs with speech segments and ES words</td>\n",
    "        <td>2095</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Example, where acoustic similarity is high, but translation is low:\n",
    "34\t45\t46\t0.908\tDIFíCILES POR EJEMPLO\tFILóCTETES POR EJEMPLO P\t1\t0.857\t1\t0.092\t0.826\t1.049.007\t1.049.014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving UTD with noisy labels - English translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
