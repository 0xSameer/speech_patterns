{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import cPickle as pickle\n",
    "import json\n",
    "import subprocess\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "import bisect\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"config.json\") as json_data_file:\n",
    "    config = json.load(json_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodes_fname = config[\"es\"]['nodes_fname']\n",
    "seg_nodes_fname = config[\"es\"]['seg_nodes_fname']\n",
    "nodes_dict_fname = config[\"es\"]['nodes_dict_fname']\n",
    "\n",
    "edges_utd_fname = config[\"es\"]['edges_utd_fname']\n",
    "edges_olap_fname = config[\"es\"]['edges_olap_fname']\n",
    "edges_all_fname = config[\"es\"]['edges_all_fname']\n",
    "edges_score_fname = config[\"es\"]['edges_score_fname']\n",
    "\n",
    "clusters_utd_fname = config['es']['clusters_utd_fname']\n",
    "clusters_fname = config['es']['clusters_fname']\n",
    "clusters_stats_fname = config['es']['clusters_stats_fname']\n",
    "\n",
    "pairs_fname = config['es']['score_pairs_fname']\n",
    "eval_fname = config['es']['eval_pairs_fname']\n",
    "\n",
    "feats_fname = config['es']['feats_fname']\n",
    "feats_dict_fname = config['es']['feats_dict_fname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Align = namedtuple('Align', ['word', 'start', 'end'])\n",
    "Node = namedtuple('Node', ['file', 'seg', 'start', 'end', 'es', 'es_cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Eval = namedtuple('Eval', ['n1', 'n2', 'dtw', 'es_sim', 'es_cnt_sim', 'en_j_sim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segment_map = pickle.load(open(config['es']['segment_dict_fname'], \"rb\"))\n",
    "align_dict = pickle.load(open(config['es']['align_dict_fname'], \"rb\"))\n",
    "nodes_dict = pickle.load(open(nodes_dict_fname, \"rb\"))\n",
    "pairs_list = pickle.load(open(pairs_fname, \"rb\"))\n",
    "clusters = pickle.load(open(clusters_fname, \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate all valid pairs discovered\n",
    "\n",
    "- Calculate es sim, es content word sim, en content word sim\n",
    "- Calculate num correct pairs, num total, num content correct, at D=80, and D=87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity_jaccard(w_list1, w_list2):\n",
    "    common_keys = set(w_list1) & set(w_list2)\n",
    "    union_keys = set(w_list1) | set(w_list2)\n",
    "    jaccard_dist = (0 if len(union_keys) == 0 else len(common_keys) / len(union_keys))\n",
    "    return jaccard_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity_match_any(w_list1, w_list2):\n",
    "    common_words_len = len(set(w_list1) & set(w_list2))\n",
    "    return max(min(1, common_words_len), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_utd_pairs(pairs_list, nodes_dict, segment_map, eval_fname):\n",
    "    eval_list = []\n",
    "    \n",
    "    display_den = int(len(pairs_list) / 10)\n",
    "    \n",
    "    for pair_num, pair in enumerate(pairs_list, start=1):\n",
    "        \n",
    "        if pair_num % display_den == 0:\n",
    "            print(\"Evluating pair number: %d\" % pair_num)\n",
    "            \n",
    "        n1_id = pair[0]\n",
    "        n2_id = pair[1]\n",
    "        n1 = nodes_dict[pair[0]]\n",
    "        n2 = nodes_dict[pair[1]]\n",
    "        dtw = pair[2]\n",
    "        \n",
    "        es_sim = similarity_match_any(n1.es, n2.es)\n",
    "        es_cnt_sim = similarity_match_any(n1.es_cnt, n2.es_cnt)\n",
    "        \n",
    "        if n1.seg in align_dict[n1.file] and n2.seg in align_dict[n2.file]:\n",
    "            en_n1 = [a.word for a in align_dict[n1.file][n1.seg]['en_cnt']]\n",
    "            en_n2 = [a.word for a in align_dict[n2.file][n2.seg]['en_cnt']]\n",
    "            en_j_sim = similarity_jaccard(en_n1, en_n2)\n",
    "        else:\n",
    "            en_j_sim = 0\n",
    "        \n",
    "        #eval_dict[pair_num] = Eval(n1_id, n2_id, dtw, es_sim, es_cnt_sim, en_j_sim)\n",
    "        eval_list.append(Eval(n1_id, n2_id, dtw, es_sim, es_cnt_sim, en_j_sim))\n",
    "    \n",
    "    # Saving eval\n",
    "    pickle.dump(eval_list, open(eval_fname, \"wb\"))\n",
    "    print(\"Finished evaluating %d pairs\" % pair_num)\n",
    "    return eval_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evluating pair number: 2660\n",
      "Evluating pair number: 5320\n",
      "Evluating pair number: 7980\n",
      "Evluating pair number: 10640\n",
      "Evluating pair number: 13300\n",
      "Evluating pair number: 15960\n",
      "Evluating pair number: 18620\n",
      "Evluating pair number: 21280\n",
      "Evluating pair number: 23940\n",
      "Evluating pair number: 26600\n",
      "Finished evaluating 26604 pairs\n"
     ]
    }
   ],
   "source": [
    "eval_list = eval_utd_pairs(pairs_list, nodes_dict, segment_map, eval_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(eval_list, columns=Eval._fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_current_config():\n",
    "    config_params = os.path.basename(config[\"es\"][\"zrt_out_path\"]).split('-')\n",
    "    #dtw_thresh = \"DTW=\"+config_params[0][-4:]\n",
    "    #dur_thresh = \"DUR=%dms\" % (int(config_params[-1])*10)\n",
    "    return config_params[0][-4:] + \", %dms\" % (int(config_params[-1])*10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs evaluated: 26604\n",
      "pairs with es word match: 7520\n",
      "pairs with es content word match: 4431\n",
      "+--------------------+---------+----------+--------------+-------+\n",
      "| config (DTW, dur.) | # pairs | es match | es cnt match | prec. |\n",
      "+--------------------+---------+----------+--------------+-------+\n",
      "|    0.87, 500ms     |  26604  |   7520   |     4431     | 0.167 |\n",
      "+--------------------+---------+----------+--------------+-------+\n",
      "\n",
      "Latex format\n",
      "config (DTW, dur.) & # pairs & es match & es cnt match & prec.\n",
      "0.87, 500ms & 26604 & 7520 & 4431 & 0.167 \\\\\n"
     ]
    }
   ],
   "source": [
    "# All matches\n",
    "\n",
    "res_columns = [\"config (DTW, dur.)\", \"# pairs\", \"es match\", \"es cnt match\", \"prec.\"]\n",
    "pp_pairs = PrettyTable(res_columns)\n",
    "\n",
    "print('Total pairs evaluated: %d' % len(eval_list))\n",
    "\n",
    "es_match = [i for i, e in enumerate(eval_list) if e.es_sim == 1]\n",
    "es_cnt_match = [i for i, e in enumerate(eval_list) if e.es_cnt_sim == 1]\n",
    "print('pairs with es word match: %d' % len(es_match))\n",
    "print('pairs with es content word match: %d' % len(es_cnt_match))\n",
    "\n",
    "# # D=0.87\n",
    "# es_87_pairs = [i for i, e in enumerate(eval_list) if e.dtw >= 0.88]\n",
    "# es_87_match = [i for i, e in enumerate(eval_list) if e.es_sim == 1 and e.dtw >= 0.88]\n",
    "# es_87_cnt_match = [i for i, e in enumerate(eval_list) if e.es_cnt_sim == 1 and e.dtw >= 0.88]\n",
    "# print('pairs with D >= 0.87: %d' % len(es_87_pairs))\n",
    "# print('pairs with es word match: %d' % len(es_87_match))\n",
    "# print('pairs with es content word match: %d' % len(es_87_cnt_match))\n",
    "\n",
    "prec = \"%0.3f\" % (len(es_cnt_match)/len(eval_list))\n",
    "res_row = [get_current_config(), len(eval_list), len(es_match), len(es_cnt_match), prec]\n",
    "pp_pairs.add_row(res_row)\n",
    "print(pp_pairs)\n",
    "\n",
    "print('\\nLatex format')\n",
    "print(\" & \".join(res_columns))\n",
    "print(\" & \".join(map(str, res_row)), \"\\\\\\\\\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clusters\n",
    "\n",
    "- calculate cluster purity, and most common word\n",
    "- generate features, parallel corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_cluster_stats(clusters, nodes_dict):\n",
    "    cluster_stats = {'words':[], 'purity':[], 'fids': [], 'most_common':[], 'avg_purity':0.0}\n",
    "    avg_purity_num = 0\n",
    "    avg_purity_den = 0\n",
    "    for i, nodes in enumerate(clusters):\n",
    "        cnt_words = []\n",
    "        fids = []\n",
    "        for node in nodes:\n",
    "            cnt_words.extend(list(nodes_dict[node].es_cnt))\n",
    "            fids.append(nodes_dict[node].file)\n",
    "        cnt_words = [w.lower().decode(\"utf-8\") for w in cnt_words]\n",
    "        cluster_stats['words'].append(cnt_words)\n",
    "        cluster_stats['fids'].append(fids)\n",
    "        counter_words = Counter(cnt_words)\n",
    "        most_common_word = counter_words.most_common(1)[0] if len(cnt_words) > 0 else (': (', 0)\n",
    "        #print(i, most_common_word)\n",
    "        cluster_stats['most_common'].append(most_common_word[0])\n",
    "        temp_len = len(cnt_words) if len(cnt_words) > 0 else 1\n",
    "        cluster_stats['purity'].append(most_common_word[1] / (temp_len * 1.0))\n",
    "        avg_purity_num += most_common_word[1]\n",
    "        avg_purity_den += temp_len\n",
    "    cluster_stats['avg_purity'] = avg_purity_num / avg_purity_den\n",
    "    print('Finished calculation cluster stats')\n",
    "    print('Average cluster purity: %0.3f' % cluster_stats['avg_purity'])\n",
    "    pickle.dump(cluster_stats, open(clusters_stats_fname, \"wb\"))\n",
    "    return cluster_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Node(file='001', seg='001.033', start=94, end=149, es=('EL', 'RADIOTERAPISTA'), es_cnt=('RADIOTERAPISTA',))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_dict[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished calculation cluster stats\n",
      "Average cluster purity: 0.336\n"
     ]
    }
   ],
   "source": [
    "clusters_stats = calc_cluster_stats(clusters, nodes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of pseudowords\n",
    "\n",
    "- For each segment, generate a bag of cluster ids based on the nodes discovered\n",
    "- default cluster id: -1 when no nodes found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'../../../ZRTools/exp/callhome/matches/config0.87-0.90-0.80-50/master_graph.dedups',\n",
       " u'../../../ZRTools/exp/callhome/matches/config0.87-0.90-0.80-50/nodes_dict.p')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_utd_fname, config[\"es\"]['nodes_dict_fname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
