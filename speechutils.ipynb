{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from prettytable import PrettyTable\n",
    "import textwrap\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import subprocess\n",
    "import cPickle\n",
    "import editdistance\n",
    "import nltk\n",
    "import time\n",
    "import math\n",
    "from collections import Counter\n",
    "import IPython\n",
    "from IPython import display\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from timeit import default_timer as timer\n",
    "from matplotlib import rcParams\n",
    "import networkx as nx\n",
    "from IPython.display import display\n",
    "from matplotlib.ticker import MultipleLocator, \\\n",
    "     FormatStrFormatter, AutoMinorLocator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZRT output utilities:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class - ZRTPrep\n",
    "\n",
    "Prepare for ZRT experiments\n",
    "- Generate:\n",
    " - lst file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ZRTPrep(object):\n",
    "    def __init__(self, config):\n",
    "        self.base_config = config[\"base\"]\n",
    "        \n",
    "    def read_file_list(self):\n",
    "        pass\n",
    "    # Convert sph files to wav files\n",
    "    def gen_sph2wav(self):\n",
    "        pass\n",
    "    \n",
    "    # Split audio into multiple channels\n",
    "    def split_channels(self):\n",
    "        pass\n",
    "    \n",
    "    def zrt_init_file_list(self, lst_file):\n",
    "        self.b_wav_files = []\n",
    "        self.b_base_name = []\n",
    "        self.b_vad_files = []\n",
    "        self.b_exp_name = os.path.splitext(os.path.basename(lst_file))[0]\n",
    "        with open(lst_file,\"r\") as in_f:\n",
    "            for fil in in_f:\n",
    "                fil = fil.strip()\n",
    "                self.b_wav_files.append(fil)\n",
    "                self.b_base_name.append(os.path.splitext(os.path.basename(fil))[0])\n",
    "#                 self.b_vad_files.append(fil.replace(\".wav\", \".vad\"))\n",
    "                self.b_vad_files.append(fil.replace(\".wav\", \".evad\"))\n",
    "        \n",
    "    def zrt_create_exp_dirs(self, exp_path):\n",
    "        self.b_exp_path = exp_path\n",
    "        self.b_feats_path = os.path.join(exp_path, \"feats\")\n",
    "        self.b_lsh_path = os.path.join(exp_path, \"lsh\")\n",
    "        self.b_matches_path = os.path.join(exp_path, \"matches\")\n",
    "        if not os.path.exists(self.b_feats_path):\n",
    "            os.makedirs(self.b_feats_path)\n",
    "        if not os.path.exists(self.b_lsh_path):\n",
    "            os.makedirs(self.b_lsh_path)\n",
    "        if not os.path.exists(os.path.join(exp_path, \"results\")):\n",
    "            os.makedirs(os.path.join(exp_path, \"results\"))\n",
    "        if not os.path.exists(os.path.join(exp_path, \"matches\")):\n",
    "            os.makedirs(self.b_matches_path)\n",
    "            \n",
    "    def zrt_gen_files_base(self):\n",
    "        if os.path.exists(self.b_exp_path):\n",
    "            with open(os.path.join(self.b_exp_path, \"files.base\"), \"w\") as out_f:\n",
    "                for fil_base_name in sorted(self.b_base_name):\n",
    "                    out_f.write(\"{0:s}\\n\".format(fil_base_name))\n",
    "            \n",
    "            \n",
    "    def zrt_gen_lsh_proj_file(self):\n",
    "        self.b_proj_fil_name = os.path.join(self.b_exp_path, \"proj_S64xD39_seed1\")\n",
    "        subprocess.call([self.base_config[\"lsh_genproj\"], \\\n",
    "                         \"-D\",\"39\",\"-S\",\"64\",\"-seed\", \\\n",
    "                         \"1\",\"-projfile\", self.b_proj_fil_name])\n",
    "    \n",
    "    def zrt_gen_plp_files(self):\n",
    "        FEACALC = self.base_config[\"feacalc\"]\n",
    "        STANDFEAT = self.base_config[\"standfeat\"]\n",
    "        num_files = len(self.b_wav_files)\n",
    "        for i, wav_fil in enumerate(self.b_wav_files):\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Completed: {0:d} out of: {1:d}\".format(i, num_files))\n",
    "            feat_fil = os.path.join(self.b_feats_path, self.b_base_name[i] + \".binary\")\n",
    "            std_feat_fil = os.path.join(self.b_feats_path, self.b_base_name[i] + \".std.binary\")\n",
    "            #print(wav_fil, feat_fil, std_feat_fil, self.b_vad_files[i])\n",
    "            \n",
    "            # Generate feat file\n",
    "#             print(\" \".join([FEACALC,\"-plp\", \\\n",
    "#                             \"12\", \"-cep\", \"13\", \"-dom\", \"cep\", \"-deltaorder\", \\\n",
    "#                             \"2\", \"-dither\", \"-frqaxis\", \"bark\", \"-samplerate\", \\\n",
    "#                             \"8000\", \"-win\", \"25\", \"-step\", \"10\", \"-ip\", \\\n",
    "#                             \"MSWAVE\", \"-rasta\", \"false\", \"-compress\", \\\n",
    "#                             \"true\", \"-op\", \"swappedraw\", \"-o\", feat_fil, wav_fil]))\n",
    "            subprocess.call([FEACALC,\"-plp\", \\\n",
    "                            \"12\", \"-cep\", \"13\", \"-dom\", \"cep\", \"-deltaorder\", \\\n",
    "                            \"2\", \"-dither\", \"-frqaxis\", \"bark\", \"-samplerate\", \\\n",
    "                            \"8000\", \"-win\", \"25\", \"-step\", \"10\", \"-ip\", \\\n",
    "                            \"MSWAVE\", \"-rasta\", \"false\", \"-compress\", \\\n",
    "                            \"true\", \"-op\", \"swappedraw\", \"-o\", feat_fil, wav_fil])\n",
    "\n",
    "            # Standardize binary file, for VAD regions only\n",
    "            subprocess.call([STANDFEAT, \"-D\", \"39\", \"-infile\", \\\n",
    "                            feat_fil, \"-outfile\", std_feat_fil, \\\n",
    "                            \"-vadfile\", self.b_vad_files[i]])\n",
    "            \n",
    "            print(print(\"Completed - FEAT generation\"))\n",
    "    \n",
    "    def zrt_gen_lsh_files(self):\n",
    "        LSH = self.base_config[\"lsh\"]\n",
    "        num_files = len(self.b_wav_files)\n",
    "        for i, wav_fil in enumerate(self.b_wav_files):\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Completed: {0:d} out of: {1:d}\".format(i, num_files))\n",
    "            lsh_fil = os.path.join(self.b_lsh_path, self.b_base_name[i] + \".std.lsh64\")\n",
    "            std_feat_fil = os.path.join(self.b_feats_path, self.b_base_name[i] + \".std.binary\")\n",
    "            if os.path.exists(std_feat_fil):\n",
    "                pass\n",
    "                subprocess.call([LSH, \"-D\", \"39\", \"-S\", \"64\", \\\n",
    "                                \"-projfile\", self.b_proj_fil_name, \\\n",
    "                                \"-featfile\", std_feat_fil, \"-sigfile\", \\\n",
    "                                lsh_fil, \"-vadfile\", self.b_vad_files[i]])\n",
    "            else:\n",
    "                print(\"File not found: %s\" % std_feat_fil)\n",
    "        print(\"Completed - LSH\")\n",
    "    \n",
    "    def zrt_gen_disc_cmd(self, num_splits=1):\n",
    "        disc_file = os.path.join(self.b_exp_path, \"disc.cmd\")\n",
    "        disc_file_split_base = \"disc_{0:d}.cmd\"\n",
    "        disc_file_split = os.path.join(self.b_exp_path, disc_file_split_base)\n",
    "        disc_split_file = os.path.join(self.b_exp_path, \"disc_split.txt\")\n",
    "        num_files = len(self.b_base_name)\n",
    "        exp_local_path = os.path.join(\"exp\", self.b_exp_name)\n",
    "        cmd_string = \"scripts/plebdisc_filepair \\\"{0:s}\\\" \\\"{1:s}\\\" {2:s} 39\\n\"\n",
    "        \n",
    "        total_lines = num_files * num_files\n",
    "        lines_per_file = total_lines // num_splits\n",
    "        smallfile = None\n",
    "        curr_line = 0\n",
    "        curr_file_num = 0\n",
    "        \n",
    "        for i in xrange(num_files) :\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Progress: {0:d} out of: {1:d}\".format(curr_line+1, total_lines))\n",
    "            for j in xrange(num_files):\n",
    "                out_line = cmd_string.format(self.b_base_name[i], \\\n",
    "                                                  self.b_base_name[j], \\\n",
    "                                                  exp_local_path)\n",
    "                if curr_line % lines_per_file == 0:\n",
    "                    if smallfile:\n",
    "                        smallfile.close()\n",
    "                    small_filename = disc_file_split.format(curr_file_num)\n",
    "                    smallfile = open(small_filename, \"w\")\n",
    "                    curr_file_num += 1\n",
    "                smallfile.write(out_line)\n",
    "                curr_line += 1\n",
    "        if smallfile:\n",
    "            smallfile.close()\n",
    "        \n",
    "        # Making a list of commands to execute the split disc list\n",
    "        full_split_cmd_string = \"nice sh {0:s} 1> {1:s} 2>{2:s} &\\n\"\n",
    "        split_cmd = os.path.join(exp_local_path, \"matches\",\"{0:s}.{1:d}\")\n",
    "        with open(disc_split_file, \"w\") as out_f:\n",
    "            for i in xrange(curr_file_num):\n",
    "                curr_split_file = os.path.join(exp_local_path, disc_file_split_base.format(i))\n",
    "                split_cmd_out = split_cmd.format(\"out\", i)\n",
    "                #split_cmd_err = split_cmd.format(\"err\", i)\n",
    "                split_cmd_err = \"/dev/null\"\n",
    "                \n",
    "                out_line = \"nice sh \"\n",
    "                out_f.write(full_split_cmd_string.format(curr_split_file, \\\n",
    "                                                        split_cmd_out, \\\n",
    "                                                        split_cmd_err))\n",
    "        \n",
    "        print(\"Completed - disc.cmd\")\n",
    "    \n",
    "    \n",
    "    def zrt_calc_dur_from_evad(self):\n",
    "        dur_ms = 0\n",
    "        for fil in self.b_vad_files:\n",
    "            with open(fil, \"r\") as in_f:\n",
    "                for line in in_f:\n",
    "                    line_items = map(int, line.strip().split())\n",
    "                    dur_ms += ((line_items[1] - line_items[0]) * 10)\n",
    "#         print(\"Total number of files: %d\" % len(self.b_vad_files))\n",
    "#         print(\"duration: %d (ms), %.2f (hours)\" %(dur_ms, dur_ms / (1000 * 3600)))\n",
    "        return dur_ms\n",
    "    \n",
    "    def zrt_init_out_folders(self, zrt_out_path):\n",
    "        self.nodes_file = os.path.join(zrt_out_path, \"master_graph.nodes\")\n",
    "        self.edges_file = os.path.join(zrt_out_path, \"master_graph.edges\")\n",
    "        self.clusters_file = os.path.join(zrt_out_path, \"master_graph.clusters\")\n",
    "        self.matches_file = os.path.join(zrt_out_path, \"master_graph\")\n",
    "        print(self.nodes_file)\n",
    "    \n",
    "    \n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class - CallHomeZRTPrep\n",
    "\n",
    "CallHome dataset specific processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Data structure to store mapping between CallHome data, translations\n",
    "and transcriptions\n",
    "'''\n",
    "class VadInfo(object):\n",
    "    def __init__(self, start=0, end=0, chid=0):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.chid = chid\n",
    "    def __str__(self):\n",
    "        return ' '.join(map(str,[self.start, self.end, self.chid]))\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "class TranscriptInfo(object):\n",
    "    def __init__(self, start=0, end=0, word=''):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.word = word\n",
    "    def __str__(self):\n",
    "#         y = PrettyTable([\"start(ms)\", \"end(ms)\", \"word\"], hrules=True)\n",
    "#         y.align[\"start(ms)\"] = \"r\"\n",
    "#         y.add_row([self.start*10, self.end*10, self.word])\n",
    "#         return str(y)\n",
    "        return (\"%d---%d (ms) :\\t%s\\n\" %(self.start*10, self.end*10, self.word))\n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "        \n",
    "class FileInfo(object):\n",
    "        def __init__(self):\n",
    "            self.source_file = ''\n",
    "            self.target_file = ''\n",
    "            self.vad = VadInfo()\n",
    "            self.trim_pairs = ()\n",
    "        def __str__(self):\n",
    "            return \"{0:s} {1:s} {2:s}\".format(self.source_file, \\\n",
    "                                             self.target_file, \\\n",
    "                                             self.vad)\n",
    "        def __repr__(self):\n",
    "            return str(self)\n",
    "'''\n",
    "Class to manage all CallHome specific data processing\n",
    "'''\n",
    "class CallHomeZRTPrep(ZRTPrep):\n",
    "    '''\n",
    "    Constructor - read config file\n",
    "    Path to required files, folders and utilities\n",
    "    '''\n",
    "    def __init__(self, config_file):\n",
    "        with open(config_file) as json_data_file:\n",
    "            config = json.load(json_data_file)\n",
    "        super(CallHomeZRTPrep, self).__init__(config)\n",
    "        self.ch_config = config[\"es\"]\n",
    "    '''\n",
    "    Initialize filemap: filename to integer id mapping\n",
    "    This id is used to refer to all transcriptions\n",
    "    '''\n",
    "    def read_filemap(self):\n",
    "        self.filename2i = {}\n",
    "        with open(self.ch_config['filename_map'], \"r\") as in_f:\n",
    "            for i, file_name in enumerate(in_f, start=1):\n",
    "                self.filename2i[file_name.strip()] = i\n",
    "    '''\n",
    "    The fisher-callhome corpus merges some of the vad regions\n",
    "    These regions need to be merged while generating single channel\n",
    "    audio files and the names need to be consistent with the \n",
    "    transcriptions\n",
    "    '''\n",
    "    def read_mapping_for_vad_trans(self):\n",
    "        with open(self.ch_config[\"trans_map\"], \"r\") as in_f:\n",
    "            self.trans_map = {}\n",
    "            self.vad_map = {}\n",
    "            trans_count = {}\n",
    "            for i, line in enumerate(in_f, start=0):\n",
    "                line_items = line.strip().split()\n",
    "                sp_fil = line_items[0]\n",
    "                vad_ids = map(int, line_items[1].split('_'))\n",
    "                if sp_fil not in self.trans_map:\n",
    "                    self.trans_map[sp_fil] = {}\n",
    "                    self.vad_map[sp_fil] = {}\n",
    "                    trans_count[sp_fil] = 1\n",
    "                self.vad_map[sp_fil][trans_count[sp_fil]] = vad_ids\n",
    "                self.trans_map[sp_fil][trans_count[sp_fil]] = i\n",
    "                trans_count[sp_fil] += 1\n",
    "    \n",
    "    def read_vad_info(self, filename):\n",
    "        vad_info_dict = {}\n",
    "        with open(filename, 'r') as in_f:\n",
    "            for i, line in enumerate(in_f, start=1):\n",
    "                line = line.split(None, 3)\n",
    "                if len(line) > 3:\n",
    "                    start = int(line[0].replace('.',''))\n",
    "                    end = int(line[1].replace('.',''))\n",
    "                    if ('A' in line[2]) and (':' in line[2]):\n",
    "                        chid = 1\n",
    "                    elif ('B' in line[2]) and (':' in line[2]):\n",
    "                        chid = 2\n",
    "                    else:\n",
    "                        print(\"Channel id not found\")\n",
    "                        return {}\n",
    "                    vad_info_dict[i] = VadInfo(start,end,chid)\n",
    "            return vad_info_dict\n",
    "        \n",
    "    def get_transcript_path(self, sp_fil):\n",
    "        # Check train, evltest, devtest folders\n",
    "        sp_fil_ext = sp_fil + \".txt\"\n",
    "        sub_folders = [\"train\", \"evltest\", \"devtest\"]\n",
    "        corpus_path = self.ch_config[\"data_path\"]\n",
    "        check_paths = \\\n",
    "                [os.path.join(corpus_path,x,sp_fil_ext) \\\n",
    "                 for x in sub_folders]\n",
    "        check_paths_exists = map(os.path.isfile,check_paths)\n",
    "        if any(check_paths_exists):\n",
    "            sp_fil_path = [f for t, f in enumerate(check_paths) \\\n",
    "                     if check_paths_exists[t] == True][0]\n",
    "            return sp_fil_path\n",
    "        else:\n",
    "            print('%s not found' % sp_fil)\n",
    "            return ''\n",
    "        \n",
    "    def read_file_info(self):\n",
    "        self.vad_info = {}\n",
    "        self.source_wav_full_path = {}\n",
    "        for sp_fil in self.filename2i:\n",
    "            # Get complete filename:\n",
    "            sp_fil_path = self.get_transcript_path(sp_fil)\n",
    "            # Read vad info\n",
    "            self.vad_info[sp_fil] = self.read_vad_info(sp_fil_path)\n",
    "            self.source_wav_full_path[sp_fil] = sp_fil_path.replace('.txt', '')\n",
    "    \n",
    "    def read_translations(self):\n",
    "        with open(self.ch_config[\"trans_file\"], \"r\") as in_f:\n",
    "            self.trans_lines = in_f.readlines()\n",
    "    \n",
    "    def read_transcript(self, filename):\n",
    "        transcrpt_info_list = []\n",
    "        with open(filename, \"r\") as in_f:\n",
    "            for line in in_f:\n",
    "                line_items = line.strip().split()\n",
    "                start = int(line_items[1])\n",
    "                end = int(line_items[2])\n",
    "                word = line_items[0]\n",
    "                transcrpt_info_list.append(TranscriptInfo(start, \\\n",
    "                               end=end, word=word))\n",
    "        return transcrpt_info_list\n",
    "    \n",
    "    def read_transcripts(self, folder_path):\n",
    "        transcrpt_dict = {}\n",
    "        file_list = [f for \\\n",
    "                     f in os.listdir(folder_path) if \\\n",
    "                     os.path.isfile(os.path.join(folder_path, f)) and \\\n",
    "                    f.endswith((\"words\", \"phones\"))]\n",
    "        for f in file_list:\n",
    "            dict_key, _ = os.path.splitext(f)\n",
    "            transcrpt_dict[dict_key] = \\\n",
    "                    self.read_transcript(os.path.join(folder_path, f))\n",
    "        \n",
    "        return transcrpt_dict\n",
    "    \n",
    "    def read_en_words(self):\n",
    "        pass\n",
    "    \n",
    "    def filter_es_content_words(self):\n",
    "        print(\"Creating Spanish content words dictionary ...\")\n",
    "        self.es_cnt_words_dict = {}\n",
    "        for i, sp_fil in enumerate(self.es_words_dict):\n",
    "            if i % 5000 == 0:\n",
    "                print(\"Processed %d speech utterances\" % i)\n",
    "            \n",
    "            self.es_cnt_words_dict[sp_fil] = []\n",
    "            for t in self.es_words_dict[sp_fil]:\n",
    "                if t.word.lower().decode(\"utf-8\") not in stopwords.words('spanish'):\n",
    "                    self.es_cnt_words_dict[sp_fil].append(t)        \n",
    "        print(\"Finished generating Spanish content words dict ...\")\n",
    "    \n",
    "    def read_es_words(self):\n",
    "        print(\"Reading Spanish word transcriptions ...\")\n",
    "        self.es_words_dict = \\\n",
    "            self.read_transcripts(self.ch_config[\"es_word_path\"])\n",
    "        print(\"Finished reading Spanish transcriptions ...\")\n",
    "\n",
    "    def read_es_phones(self):\n",
    "        print(\"Reading Spanish phone transcriptions ...\")\n",
    "        self.es_phones_dict = \\\n",
    "            self.read_transcripts(self.ch_config[\"es_phone_path\"])\n",
    "        print(\"Finished reading Spanish phone transcriptions ...\")\n",
    "    \n",
    "    def create_file_dict(self):\n",
    "        # Loop through translation map\n",
    "        # We need to create a file for each trans map location\n",
    "        self.file_dict = {}\n",
    "        self.en_words_dict = {}\n",
    "        self.en_cnt_words_dict = {}\n",
    "        for sp_fil in self.vad_map:\n",
    "            # Get integer name in 3 digits\n",
    "            fil_id = \"{0:03d}\".format(self.filename2i[sp_fil])\n",
    "            # Loop over all translation lines\n",
    "            for i, vids in enumerate(self.vad_map[sp_fil].values(),\\\n",
    "                                    start=1):\n",
    "                # Get starting vad id\n",
    "                vad_id = \"{0:03d}\".format(vids[0])\n",
    "                # For each vad entry, generate an individual\n",
    "                # file entry\n",
    "                key = \"%s.%s\" %(fil_id, vad_id)\n",
    "                if key not in self.file_dict:\n",
    "                    self.file_dict[key] = FileInfo()\n",
    "                    self.en_words_dict[key] = []\n",
    "                # Get start and end time:\n",
    "                start_time = self.vad_info[sp_fil][vids[0]].start\n",
    "                total_time = 0\n",
    "                trim_pairs_list = []\n",
    "                for vid in vids:\n",
    "                    total_time += (self.vad_info[sp_fil][vid].end - self.vad_info[sp_fil][vid].start)\n",
    "                    # Add start and end times in order\n",
    "                    trim_pairs_list.append(self.vad_info[sp_fil][vid].start)\n",
    "                    trim_pairs_list.append(self.vad_info[sp_fil][vid].end)\n",
    "                \n",
    "                \n",
    "                for h in xrange(1, len(trim_pairs_list)-1,2):\n",
    "                    if trim_pairs_list[h] > trim_pairs_list[h+1]:\n",
    "                        trim_pairs_list[h+1] = trim_pairs_list[h]\n",
    "                        print(sp_fil)\n",
    "                        print(trim_pairs_list)\n",
    "                \n",
    "                total_time = 0\n",
    "                for h in xrange(1, len(trim_pairs_list),2):\n",
    "                    total_time += (trim_pairs_list[h] - trim_pairs_list[h-1])\n",
    "                    \n",
    "                \n",
    "                # Calculate end time based on duration\n",
    "                end_time = start_time + total_time\n",
    "                # Channel id\n",
    "                chid = self.vad_info[sp_fil][vids[0]].chid\n",
    "                # Set up file info object\n",
    "                self.file_dict[key].source_file = sp_fil\n",
    "                \n",
    "                self.file_dict[key].target_file = \\\n",
    "                    os.path.join(self.ch_config[\"out_path\"], \\\n",
    "                                key)\n",
    "                self.file_dict[key].vad.start = start_time\n",
    "                self.file_dict[key].vad.end = end_time\n",
    "                self.file_dict[key].vad.chid = chid\n",
    "                # Assign the trim list\n",
    "                self.file_dict[key].trim_pairs = tuple(trim_pairs_list)\n",
    "                # Get translations\n",
    "                en_line = \\\n",
    "                    self.trans_lines[self.trans_map[sp_fil][i]]\n",
    "                en_line = en_line.translate(string.maketrans(\"\",\"\"),\\\n",
    "                                            string.punctuation)\n",
    "                en_line = en_line.lower()\n",
    "                self.en_words_dict[key] = en_line.strip().split()\n",
    "                \n",
    "                # Get English content words\n",
    "                en_w_in_uttr = self.en_words_dict[key]\n",
    "                en_w_cnt_in_uttr = \\\n",
    "                    [word for word in en_w_in_uttr if word.decode(\"utf-8\") not in stopwords.words('english')]\n",
    "\n",
    "                self.en_cnt_words_dict[key] = en_w_cnt_in_uttr\n",
    "    \n",
    "    def read_speaker_info(self):\n",
    "        pass\n",
    "    \n",
    "    def read_call_info(self):\n",
    "        pass\n",
    "    \n",
    "    def create_out_folders(self):\n",
    "        out_path = self.ch_config[\"out_path\"]\n",
    "        tmp_path = os.path.join(self.ch_config[\"out_path\"], \"tmp\")\n",
    "        if not os.path.exists(self.ch_config[\"out_path\"]):\n",
    "            os.makedirs(self.ch_config[\"out_path\"])\n",
    "        if not os.path.exists(tmp_path):\n",
    "            os.makedirs(tmp_path)\n",
    "        return out_path, tmp_path\n",
    "        \n",
    "    def convert_sph_to_wav(self):\n",
    "        _, tmp_path = self.create_out_folders()\n",
    "        for i, (sp_fil, sp_fil_path) in enumerate(self.source_wav_full_path.items()):\n",
    "            if i % 10 == 0:\n",
    "                print(\"Converted: %d files\" %i)\n",
    "            sph_file = sp_fil_path + \".sph\"\n",
    "            wav_file = os.path.join(tmp_path, sp_fil+\".wav\")\n",
    "            # If file exists, delete it\n",
    "            if os.path.exists(wav_file):\n",
    "                os.remove(wav_file)\n",
    "            \n",
    "            # Create wav file\n",
    "            subprocess.call([self.base_config[\"sph2pipe\"], \"-f\", \\\n",
    "                             \"rif\", \"-p\", sph_file, wav_file])\n",
    "            \n",
    "            # Create low res wav file\n",
    "            low_res_wav_file = os.path.join(tmp_path, sp_fil+\"_low.wav\")\n",
    "            subprocess.call([self.base_config[\"sox\"], \"-t\", \\\n",
    "                 \"wav\", wav_file, \"-t\", \"wav\", \\\n",
    "                 \"-e\", \"signed-integer\", \"-b\", \\\n",
    "                \"16\", \"-c\", \"2\", \"-r\", \"8000\", \\\n",
    "                \"--no-dither\", low_res_wav_file])\n",
    "\n",
    "            # Create channel specific wav files\n",
    "            ch1_wav_file = os.path.join(tmp_path, sp_fil+\"_1.wav\")\n",
    "            ch2_wav_file = os.path.join(tmp_path, sp_fil+\"_2.wav\")\n",
    "            subprocess.call([self.base_config[\"sox\"], \\\n",
    "                            low_res_wav_file, ch1_wav_file, \\\n",
    "                            \"remix\", \"1\"])\n",
    "            subprocess.call([self.base_config[\"sox\"], \\\n",
    "                            low_res_wav_file, ch2_wav_file, \\\n",
    "                            \"remix\", \"2\"])\n",
    "            \n",
    "            # delete dual channel wav file\n",
    "            os.remove(wav_file)\n",
    "            os.remove(low_res_wav_file)\n",
    "            print(\"Finished\")\n",
    "    \n",
    "    def gen_wav_files(self):\n",
    "        # we have the details of all the files to be created in\n",
    "        # file_dict\n",
    "        # Create output folder if it does not exist:\n",
    "        out_path, tmp_path = self.create_out_folders()\n",
    "        # iterate over files in es words, as some files in the corpus are corrupt\n",
    "        # file_dict has all 20K files, whereas, es_words_dict has the correct\n",
    "        # 17K files\n",
    "        # Use self.es_words_dict instead of self.file_dict\n",
    "        for i, sp_fil in enumerate(self.es_words_dict):\n",
    "            # Read file info\n",
    "            fil_info = self.file_dict[sp_fil]\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Completed: %d files\" %i)\n",
    "            \n",
    "            SOXBIN = self.base_config[\"sox\"]\n",
    "            source_file = fil_info.source_file\n",
    "            chid = fil_info.vad.chid\n",
    "            \n",
    "            in_wav = os.path.join(tmp_path, \\\n",
    "                                  \"{0:s}_{1:d}.wav\".format(source_file,chid))\n",
    "            out_wav = fil_info.target_file + \".wav\"\n",
    "            out_vad = fil_info.target_file + \".vad\"\n",
    "            out_evad = fil_info.target_file + \".evad\"\n",
    "\n",
    "            if not os.path.exists(in_wav):\n",
    "                print(\"File: %s not found!\" % in_wav)\n",
    "                return\n",
    "\n",
    "            # Remove existing files\n",
    "            if os.path.exists(out_wav):\n",
    "                os.remove(out_wav)\n",
    "            if os.path.exists(out_vad):\n",
    "                os.remove(out_vad)\n",
    "\n",
    "            # Generate list of trim pairs\n",
    "            GEN_CMD_BASE = [SOXBIN, in_wav, out_wav, \"trim\"]\n",
    "            GEN_CMD_TRIM_INFPO = [str(fil_info.trim_pairs[0]/100)]\n",
    "            for t in fil_info.trim_pairs[1:]:\n",
    "                GEN_CMD_TRIM_INFPO.append(\"={0:.2f}\".format((t/100)))\n",
    "\n",
    "            # Generate start and end info for VAD file\n",
    "            start = str(fil_info.vad.start / 100)\n",
    "            #dur = str((fil_info.vad.end - fil_info.vad.start) / 100)\n",
    "            #print(in_wav, out_wav, start, dur)\n",
    "            # Create wav\n",
    "            #subprocess.call([SOXBIN, in_wav, out_wav, \"trim\", start, dur])\n",
    "            GEN_CMD = GEN_CMD_BASE + GEN_CMD_TRIM_INFPO\n",
    "            #print(' '.join(GEN_CMD))\n",
    "            subprocess.call(GEN_CMD)\n",
    "            # Create vad\n",
    "            #print(\"{0:d} {1:d}\\n\".format(0, (fil_info.vad.end - fil_info.vad.start + 1)))\n",
    "            with open(out_vad, \"w\") as out_f:\n",
    "                out_f.write(\"{0:d} {1:d}\\n\".format(0, (fil_info.vad.end - fil_info.vad.start)))\n",
    "                \n",
    "            # Create energy based VAD\n",
    "            energy_script = \"../../ZRTools/scripts/mark_energy.py\"\n",
    "            subprocess.call([\"python\",energy_script,\"-i\", out_wav,\"-o\", out_evad,\"-s\",\"0.4\",\"-e\",\"0\"]) \n",
    "        print(\"Finished %d files \" % (i+1))\n",
    "    \n",
    "    def get_file_list(self, to_save=False, num_speakers=0, start_speaker=0):\n",
    "        # Get the keys from align dict\n",
    "        # This list excludes the corrupted files, and hence, we do not need to \n",
    "        # process them\n",
    "        \n",
    "        set_align = set([x.split('.')[0] for x in self.es_words_dict.keys()])\n",
    "        if num_speakers > 0:\n",
    "            list_align = sorted(list(set_align))[start_speaker:(start_speaker+num_speakers)]\n",
    "            set_align = set(list_align)\n",
    "        \n",
    "        wav_fil_list = []\n",
    "        out_path = self.ch_config[\"out_path\"]\n",
    "        for wav_fil in self.es_words_dict:\n",
    "            if wav_fil.split('.')[0] in set_align:\n",
    "                wav_fil_list.append(os.path.join(out_path, wav_fil+\".wav\"))\n",
    "        wav_fil_list = sorted(wav_fil_list)\n",
    "        \n",
    "        if to_save:\n",
    "            with open(self.ch_config[\"lst_file\"], \"w\") as out_f:\n",
    "                for wav_fil in wav_fil_list:\n",
    "                    out_f.write(\"%s\\n\" % wav_fil)\n",
    "        \n",
    "        return wav_fil_list\n",
    "        \n",
    "\n",
    "    def save_dicts(self):\n",
    "        # Save all the important dictionaries\n",
    "        dicts_path = self.ch_config[\"dicts_path\"]\n",
    "        if not os.path.exists(dicts_path):\n",
    "            os.makedirs(dicts_path)\n",
    "        # Save dicts\n",
    "        names_dict = self.ch_config[\"dict_names\"]\n",
    "        dict_name_map = [(self.file_dict,names_dict[\"file_info\"]), \\\n",
    "                     (self.en_words_dict,names_dict[\"en_words\"]), \\\n",
    "                     (self.es_words_dict,names_dict[\"es_words\"]), \\\n",
    "                     (self.es_phones_dict,names_dict[\"es_phones\"])]\n",
    "        for fil, fil_name in dict_name_map:\n",
    "            print(\"Saving file: %s\" % fil_name)\n",
    "            cPickle.dump(fil, open(os.path.join(dicts_path,fil_name), \"wb\"))\n",
    "        print(\"Finished saving all dictionaries\")\n",
    "    \n",
    "    def save_state(self):\n",
    "        state_dict = {}\n",
    "        state_dict['file_info'] = self.file_dict\n",
    "        state_dict['es_phones'] = self.es_phones_dict\n",
    "        state_dict['es_words'] = self.es_words_dict\n",
    "        state_dict['en_words'] = self.en_words_dict\n",
    "        state_dict['es_cnt_words'] = self.es_cnt_words_dict\n",
    "        state_dict['en_cnt_words'] = self.en_cnt_words_dict\n",
    "        \n",
    "        print(\"Saving prep state dictionary ...\")\n",
    "        prep_state_dict_path = os.path.join(self.ch_config[\"exp_path\"], \"prep_state.dict\")\n",
    "        cPickle.dump(state_dict, open(prep_state_dict_path, \"wb\"))\n",
    "        print(\"Finished saving prep state dictionary ...\")\n",
    "\n",
    "    \n",
    "    def load_state(self):\n",
    "        print(\"Loading prep state dictionary ...\")\n",
    "        prep_state_dict_path = os.path.join(self.ch_config[\"exp_path\"], \"prep_state.dict\")\n",
    "        state_dict = cPickle.load(open(prep_state_dict_path, \"rb\"))\n",
    "        self.file_dict = state_dict['file_info']\n",
    "        self.es_phones_dict = state_dict['es_phones']\n",
    "        self.es_words_dict = state_dict['es_words']\n",
    "        self.en_words_dict = state_dict['en_words']\n",
    "        self.es_cnt_words_dict = state_dict['es_cnt_words']\n",
    "        self.en_cnt_words_dict = state_dict['en_cnt_words']\n",
    "        \n",
    "        print(\"Finished loading prep state dictionary ...\")\n",
    "        \n",
    "        # Setting up ZRT base\n",
    "        #_ = self.get_file_list(to_save=True, num_speakers=20, start_speaker=41)\n",
    "        # set the exp path and the lst file\n",
    "        self.zrt_create_exp_dirs(self.ch_config[\"exp_path\"])\n",
    "        self.zrt_init_file_list(self.ch_config[\"lst_file\"])\n",
    "        self.zrt_init_out_folders(self.ch_config['zrt_out_path'])\n",
    "        \n",
    "    def something_to_do(self):\n",
    "        # Read file map        \n",
    "        self.read_filemap()\n",
    "        self.read_mapping_for_vad_trans()\n",
    "        self.read_translations()\n",
    "        self.read_file_info()\n",
    "        self.create_file_dict()\n",
    "        self.read_es_words()\n",
    "        self.filter_es_content_words()\n",
    "        self.read_es_phones()\n",
    "        _ = self.get_file_list(to_save=True, num_speakers=20, start_speaker=41)\n",
    "        # set the exp path and the lst file\n",
    "        self.zrt_create_exp_dirs(self.ch_config[\"exp_path\"])\n",
    "        self.zrt_init_file_list(self.ch_config[\"lst_file\"])\n",
    "        self.zrt_init_out_folders(self.ch_config['zrt_out_path'])\n",
    "        self.save_state()\n",
    "    \n",
    "    def something_else_to_do(self):\n",
    "        self.convert_sph_to_wav()\n",
    "    \n",
    "    def some_more_stuff(self):\n",
    "        self.gen_wav_files()\n",
    "        \n",
    "    def ch_zrt_gen_exp_command(self):\n",
    "#         self.zrt_create_exp_dirs(self.ch_config[\"exp_path\"])\n",
    "#         self.zrt_init_file_list(self.ch_config[\"lst_file\"])\n",
    "        self.zrt_gen_files_base()\n",
    "        self.zrt_gen_disc_cmd(num_splits=25)\n",
    "        \n",
    "    def ch_zrt_plp(self):  \n",
    "        self.zrt_gen_plp_files()\n",
    "    \n",
    "    def ch_zrt_lsh(self):\n",
    "        self.zrt_gen_lsh_proj_file()\n",
    "        self.zrt_gen_lsh_files()\n",
    "        \n",
    "    def print_config(self):\n",
    "        print('\\n'.join(self.base_config.values()))\n",
    "        print('\\n'.join(self.ch_config.values()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class CallHomeEval\n",
    "\n",
    "Evaluate the ZRT on CallHome data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NodeInfo(object):\n",
    "    def __init__(self, wav_fil, start, end):\n",
    "        self.wav_fil = wav_fil\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        # es words from transcription\n",
    "        self.es_words = ()\n",
    "        self.es_cnt_words = ()\n",
    "        self.es_phones = ()\n",
    "        # Future use - add speaker info\n",
    "        self.spkrinfo = 0\n",
    "        # Future use - add call info\n",
    "        self.callinfo = 0\n",
    "        \n",
    "    def __str__(self):\n",
    "        return ' '.join(map(str,[self.wav_fil, self.start, self.end]))\n",
    "    \n",
    "    def get_es_words(self):\n",
    "        return ' '.join(self.es_words)\n",
    "    \n",
    "    def get_es_phones(self):\n",
    "        return ' '.join(self.es_phones)\n",
    "    \n",
    "    def get_dur_ms(self):\n",
    "        return (self.end - self.start) * 10\n",
    "    \n",
    "#     def get_content_words(self):\n",
    "#         es_words_tokenized = \" \".join(self.es_words).decode('utf-8')\n",
    "#         return tuple([token for token in nltk.word_tokenize(es_words_tokenized) \\\n",
    "#                 if token.lower() not in stopwords.words('spanish')])\n",
    "    \n",
    "#     def has_content_words(self):\n",
    "#         return (len(self.get_content_words()) > 0)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "class PairInfo(object):\n",
    "    def __init__(self, param_dict):\n",
    "        pass\n",
    "    \n",
    "\n",
    "class CallHomeEval(ZRTPrep):\n",
    "    '''\n",
    "    Constructor - read config file\n",
    "    Path to required files, folders and utilities\n",
    "    '''\n",
    "    def __init__(self, config_file):\n",
    "        with open(config_file) as json_data_file:\n",
    "            config = json.load(json_data_file)\n",
    "        super(CallHomeEval, self).__init__(config)\n",
    "        self.ch_config = config[\"es\"]\n",
    "        # set the exp path and the lst file\n",
    "        self.zrt_create_exp_dirs(self.ch_config[\"exp_path\"])\n",
    "        self.zrt_init_file_list(self.ch_config[\"lst_file\"])\n",
    "        self.zrt_init_out_folders(self.ch_config['zrt_out_path'])\n",
    "    \n",
    "    def find_aligned_words(wav_fil, start, end, transcript_words):\n",
    "        aligned_words = []\n",
    "        for word in transcript_words:\n",
    "            if (word.start >= start and word.start <= end) \\\n",
    "            or (word.end >= start and word.end <= end) \\\n",
    "            or (start >= word.start and start <= word.end) \\\n",
    "            or (end >= word.start and end <= word.end):\n",
    "                aligned_words.append(word.word)\n",
    "        return tuple(aligned_words)\n",
    "    \n",
    "    def read_nodes(self):\n",
    "        self.node_dict = {}\n",
    "        with open(self.nodes_file, \"r\") as in_f:\n",
    "            for i, line in enumerate(in_f, start=1):\n",
    "                if i % 100000 == 0:\n",
    "                    print(\"Processing line: %d\" % i)\n",
    "                line_items = line.strip().split()\n",
    "                wav_fil = line_items[0]\n",
    "                start = int(line_items[1])\n",
    "                end = int(line_items[2])\n",
    "                node = NodeInfo(wav_fil=wav_fil, start=start, end=end)\n",
    "                node.es_words = self.find_aligned_words(start, end, self.es_words_dict[wav_fil])\n",
    "                node.es_cnt_words = self.find_aligned_words(start, end, self.es_cnt_words_dict[wav_fil])\n",
    "                node.es_phones = self.find_aligned_words(start, end, self.es_phones_dict[wav_fil])\n",
    "                #if i < 5:\n",
    "                    #print(node.es_words)\n",
    "                    #print(self.find_aligned_words(start, end, self.es_words_dict[wav_fil]))\n",
    "                self.node_dict[i] = node\n",
    "        \n",
    "        print(\"Finished - reading nodes ...\")\n",
    "\n",
    "    \n",
    "    def read_edges(self):\n",
    "        clusters_dict = {}\n",
    "        # Pairs list is used for scoring discovered pairs\n",
    "        # It contains only one entry per discovered pair\n",
    "        # The node ids in a discovered pair are stored as a single entry\n",
    "        # in the form of a tuple\n",
    "        self.pairs_list = []\n",
    "        # Edges dict is used for label spreading/propagation\n",
    "        # We add an entry for each node in a discovered pair\n",
    "        # Therefore, it will contain double the entries as pairs dict\n",
    "        # It serves as an adjancency list for each node\n",
    "        self.edges_dict = {}\n",
    "        # process clusters file\n",
    "        with open(self.clusters_file, \"r\") as in_f:\n",
    "            for i, line in enumerate(in_f):\n",
    "                line_items = line.strip().split()\n",
    "                line_items = map(int, line_items)\n",
    "                clusters_dict[line_items[0]] = line_items[0]\n",
    "                if len(line_items) > 1:\n",
    "                    for j in line_items[1:]:\n",
    "                        clusters_dict[j] = line_items[0]\n",
    "        \n",
    "        # Read edges dict\n",
    "        with open(self.edges_file, \"r\") as in_f:\n",
    "            for i, line in enumerate(in_f):\n",
    "                if i % 100000 == 0:\n",
    "                    print(\"Processing line: %d\" % (i+1))\n",
    "                line_items = line.strip().split()\n",
    "                node_1 = int(line_items[0])\n",
    "                node_2 = int(line_items[1])\n",
    "                if node_1 not in clusters_dict:\n",
    "                    clusters_dict[node_1] = node_1\n",
    "                if node_2 not in clusters_dict:\n",
    "                    clusters_dict[node_2] = node_2\n",
    "                dtw_val = float(line_items[2]) / 1000.0\n",
    "                \n",
    "                node_1 = clusters_dict[node_1]\n",
    "                node_2 = clusters_dict[node_2]\n",
    "                \n",
    "                # Add to pairs list as a tuple\n",
    "                self.pairs_list.append((min(node_1, node_2), max(node_1, node_2)))\n",
    "\n",
    "                # Add to edges dict\n",
    "                if node_1 not in self.edges_dict:\n",
    "                    self.edges_dict[node_1] = {}\n",
    "                if node_2 not in self.edges_dict:\n",
    "                    self.edges_dict[node_2] = {}\n",
    "                \n",
    "                self.edges_dict[node_1][node_2] = dtw_val\n",
    "                self.edges_dict[node_2][node_1] = dtw_val\n",
    "    \n",
    "        print(\"Finished - reading edges ...\")\n",
    "        self.clusters_dict = clusters_dict\n",
    "        \n",
    "        print(\"Removing duplicates in pairs list\")\n",
    "        set_pairs = set(self.pairs_list)\n",
    "        print(\"Set length: %d and List length: %d\" %(len(set_pairs), len(self.pairs_list)))\n",
    "        self.pairs_list = sorted(list(set_pairs))\n",
    "        \n",
    "    def similarity_edit_distance(self, w_list1, w_list2):\n",
    "        edit_dist = editdistance.eval(w_list1, w_list2)\n",
    "        max_len = max(len(w_list1), len(w_list2))\n",
    "        if max_len > 0:\n",
    "            return (max_len-edit_dist) / (max_len * 1.0), edit_dist\n",
    "        else:\n",
    "            return 0, -1\n",
    "    \n",
    "    def have_words_in_common(self, w_list1, w_list2):\n",
    "        common_words_len = len(set(w_list1) & set(w_list2))\n",
    "        return max(min(1, common_words_len), 0)\n",
    "        \n",
    "    def eval_pairs(self):\n",
    "        self.eval_pairs_list = [{} for i in xrange(len(self.pairs_list))]\n",
    "        no_match_count = 0\n",
    "        sil_match_count = 0\n",
    "        # loop through the pairs list\n",
    "        for i, (n1, n2) in enumerate(self.pairs_list):\n",
    "            if i % 3000 == 0:\n",
    "                print(\"Processing line: %d\" % (i+1))\n",
    "            # local dict\n",
    "            eval_dict = {}\n",
    "            # Add node ids\n",
    "            eval_dict['n1'] = n1\n",
    "            eval_dict['n2'] = n2\n",
    "            # Add speech files\n",
    "            sp1_chid = self.file_info_dict[self.node_dict[n1].wav_fil].vad.chid\n",
    "            sp2_chid = self.file_info_dict[self.node_dict[n2].wav_fil].vad.chid\n",
    "            eval_dict['uid1'] = self.node_dict[n1].wav_fil\n",
    "            eval_dict['uid2'] = self.node_dict[n2].wav_fil\n",
    "            eval_dict['chid1'] = sp1_chid\n",
    "            eval_dict['chid2'] = sp2_chid\n",
    "            # Add similarity based on DTW from the ZRT output\n",
    "            eval_dict['zrt_sim'] = self.edges_dict[n1][n2]\n",
    "            # add duration\n",
    "            # Use the minimum duration from the two nodes\n",
    "            eval_dict['dur'] = min(self.node_dict[n1].get_dur_ms(), self.node_dict[n2].get_dur_ms())\n",
    "            # add score over the es words\n",
    "            # Check for ['sil', 'sp']\n",
    "            esw_1 = self.node_dict[n1].es_words\n",
    "            esw_2 = self.node_dict[n2].es_words\n",
    "            set_esw_1 = set(esw_1)-set(['sil', 'sp'])\n",
    "            set_esw_2 = set(esw_2)-set(['sil', 'sp'])\n",
    "            eval_dict['es_w_sim'] = self.have_words_in_common(set_esw_1, \\\n",
    "                                                               set_esw_2)\n",
    "            # add score over content es words            \n",
    "            content_words_n1 = set(self.node_dict[n1].es_cnt_words) - set(['sil', 'sp'])\n",
    "            content_words_n2 = set(self.node_dict[n2].es_cnt_words) - set(['sil', 'sp'])\n",
    "            \n",
    "            eval_dict['cnt_es_w_sim'] = self.have_words_in_common(content_words_n1, \\\n",
    "                                                                        content_words_n2)\n",
    "            eval_dict['cnt_es_w_check'] = ((len(content_words_n1) > 0) & (len(content_words_n2) > 0))\n",
    "            # add edit distance score over es phones\n",
    "            eval_dict['es_p_sim'], _ = self.similarity_edit_distance(self.node_dict[n1].es_phones, \\\n",
    "                                                                 self.node_dict[n2].es_phones)\n",
    "            # add similarity based on english translations at time 0\n",
    "            eval_dict['en_w_hgr_sim_0'] = self.en_w_hgr_sim_0[n1][n2]\n",
    "#             eval_dict['en_w_jcrd_sim_0'] = self.en_w_jcrd_sim_0[n1][n2]\n",
    "#             eval_dict['en_w_count_sim_0'] = self.en_w_count_sim_0[n1][n2]\n",
    "            eval_dict['en_w_count_sim_0'] = (0 if self.en_w_hgr_sim_0[n1][n2] ==  0 else 1)\n",
    "            \n",
    "            # add similarity based on english translations at time 0\n",
    "            eval_dict['en_w_cnt_hgr_sim_0'] = self.en_w_cnt_hgr_sim_0[n1][n2]\n",
    "#             eval_dict['en_w_cnt_jcrd_sim_0'] = self.en_w_cnt_jcrd_sim_0[n1][n2]\n",
    "#             eval_dict['en_w_cnt_count_sim_0'] = self.en_w_cnt_count_sim_0[n1][n2]\n",
    "            eval_dict['en_w_cnt_count_sim_0'] = (0 if self.en_w_cnt_hgr_sim_0[n1][n2] == 0 else 1)\n",
    "            \n",
    "            # add es words\n",
    "            eval_dict['es_w_n1'] = ' '.join(self.node_dict[n1].es_words)\n",
    "            eval_dict['es_w_n2'] = ' '.join(self.node_dict[n2].es_words)\n",
    "            \n",
    "            eval_dict['no_mtch'] = False\n",
    "            eval_dict['sil_only'] = False\n",
    "            \n",
    "            # No match counter\n",
    "            if ((len(esw_1) == 0) or (len(esw_2) == 0)):\n",
    "                no_match_count += 1\n",
    "                eval_dict['no_mtch'] = True\n",
    "            \n",
    "            if (((len(set(esw_1)) > 0) and (len(set_esw_1)==0)) or \\\n",
    "                ((len(set(esw_2)) > 0) and (len(set_esw_2)==0))):\n",
    "                sil_match_count += 1\n",
    "                eval_dict['sil_only'] = True\n",
    "            \n",
    "            # Add eval pair\n",
    "            self.eval_pairs_list[i] = eval_dict\n",
    "        \n",
    "        print(\"Total matches: {0:d}\".format((i+1)))\n",
    "        print(\"matches with missing transcriptions: {0:d}\".format(no_match_count))\n",
    "        print(\"matches with only sil, sp: {0:d}\".format(sil_match_count))\n",
    "        print(\"Finished - evaluating %d pairs ...\" % (i+1))        \n",
    "        print(\"Setting up eval data frame\")\n",
    "        self.eval_df_full = pd.DataFrame(self.eval_pairs_list)\n",
    "        self.eval_df = self.eval_df_full[(self.eval_df_full['no_mtch'] == False) & \\\n",
    "                                         (self.eval_df_full['sil_only'] == False)]\n",
    "        print(\"Finished ...\")\n",
    "    \n",
    "#     def play_audio_segment(self, fil, start, end):\n",
    "#         start_time = str(start)\n",
    "#         end_time = str(end)\n",
    "#         start_time = start_time[:-2] + '.' + start_time[-2:]\n",
    "#         end_time = \"=\" + end_time[:-2] + '.' + end_time[-2:]\n",
    "#         play_params = \"play \" + fil + ' ' + 'trim ' + \\\n",
    "#                                     start_time + \" \" + end_time\n",
    "#         print(play_params)\n",
    "#         subprocess.call([\"play\", fil, \\\n",
    "#                          \"trim\", start_time, end_time])\n",
    "    \n",
    "#     def play_nodes_pair(self, indx):\n",
    "#         n1 = self.node_dict[self.eval_pairs_list[indx]['n1']]\n",
    "#         print(\"Playing node 1: %s\" % str(n1))\n",
    "#         print(self.es_words_dict[n1.wav_fil])\n",
    "#         self.play_audio_segment(self.file_info_dict[n1.wav_fil].target_file+\".wav\", n1.start, n1.end)\n",
    "#         time.sleep(0.5)\n",
    "#         n2 = self.node_dict[self.eval_pairs_list[indx]['n2']]\n",
    "#         print(\"Playing node 2: %s\" % str(n2))\n",
    "#         print(self.es_words_dict[n2.wav_fil])\n",
    "#         self.play_audio_segment(self.file_info_dict[n2.wav_fil].target_file+\".wav\", n2.start, n2.end)\n",
    "#         print(\"Finished playing ...\")\n",
    "    \n",
    "    def gen_segment_wav(self, fil, start, end, out_fil):\n",
    "        start_time = \"{0:0.2f}\".format(start/100)\n",
    "        end_time = \"={0:0.2f}\".format(end/100)\n",
    "#         print(' '.join([\"sox\", fil, out_fil, \\\n",
    "#                          \"trim\", start_time, \"{0:s}\".format(end_time)]))\n",
    "        subprocess.call([\"sox\", fil, out_fil, \\\n",
    "                         \"trim\", start_time, \"{0:s}\".format(end_time)])\n",
    "    \n",
    "    \n",
    "    def play_node_wav(self, nid, detail=False):\n",
    "        print(\"Playing node id: %d\" % nid)\n",
    "        pair_wavs_path = os.path.join(self.ch_config[\"zrt_out_path\"], \"wavs\")\n",
    "\n",
    "        \n",
    "        if not os.path.exists(pair_wavs_path):\n",
    "            os.makedirs(pair_wavs_path)\n",
    "        \n",
    "        n1 = self.node_dict[nid]\n",
    "\n",
    "        out_table_1 = PrettyTable([\"node id\", \"ES transcript\", \"EN translation\"], hrules=True)\n",
    "        \n",
    "        out_table_1.padding_width = 1\n",
    "        \n",
    "        out_table_1.add_row([nid, ' '.join(n1.es_words), textwrap.fill(' '.join(self.en_words_dict[n1.wav_fil]),50)])\n",
    "                \n",
    "        print(\"Transcript + Translation details\")\n",
    "        print(out_table_1)\n",
    "        print(\"Node details\")\n",
    "        \n",
    "        chid = self.file_info_dict[n1.wav_fil].vad.chid            \n",
    "        uid = n1.wav_fil\n",
    "        \n",
    "        out_table_3 = PrettyTable([\"node id\", \"uttrnce id\", \"spk id\", \"start(ms)\", \"end(ms)\", \"dur(ms)\"], hrules=True)\n",
    "        out_table_3.add_row([nid, uid, \\\n",
    "                             chid, n1.start*10, n1.end*10, ((n1.end - n1.start)*10)])        \n",
    "        print(out_table_3)\n",
    "        \n",
    "        if detail:\n",
    "            print(\"ES transcript for node: %d\" % nid)\n",
    "            print(self.print_es_words(nid))\n",
    "        \n",
    "        out_fil_n1 = os.path.join(pair_wavs_path, \"n{0:d}.wav\".format(nid))\n",
    "        if not os.path.exists(out_fil_n1):\n",
    "            self.gen_segment_wav(self.file_info_dict[n1.wav_fil].target_file+\".wav\", \\\n",
    "                                 n1.start, n1.end, out_fil_n1)\n",
    "        return IPython.display.Audio(out_fil_n1)\n",
    "    \n",
    "    def play_pair_wav(self, pair_id, detail=False):\n",
    "        print(\"Playing evaluation pair id: %d\" % pair_id)\n",
    "        pair_wavs_path = os.path.join(self.ch_config[\"zrt_out_path\"], \"wavs\")\n",
    "        out_fil = os.path.join(self.ch_config[\"zrt_out_path\"], \"wavs\", \"p{0:d}.wav\".format(pair_id))\n",
    "        \n",
    "        if not os.path.exists(pair_wavs_path):\n",
    "            os.makedirs(pair_wavs_path)\n",
    "        \n",
    "        eval_pid_dict = self.eval_pairs_list[pair_id]\n",
    "        n1 = self.node_dict[eval_pid_dict['n1']]\n",
    "        n2 = self.node_dict[eval_pid_dict['n2']]\n",
    "\n",
    "        out_table_1 = PrettyTable([\"node id\", \"ES transcript\", \"EN translation\"], hrules=True)\n",
    "        out_table_2 = PrettyTable([\"Similarity Type\", \"Similarity Value\"], hrules=True)\n",
    "        \n",
    "        out_table_1.padding_width = 1\n",
    "        out_table_2.padding_width = 1\n",
    "        \n",
    "        out_table_1.add_row([eval_pid_dict['n1'], eval_pid_dict['es_w_n1'],\\\n",
    "                             textwrap.fill(' '.join(self.en_words_dict[n1.wav_fil]),50)])\n",
    "        out_table_1.add_row([eval_pid_dict['n2'], eval_pid_dict['es_w_n2'],\\\n",
    "                             textwrap.fill(' '.join(self.en_words_dict[n2.wav_fil]),50)])\n",
    "        \n",
    "        out_table_2.add_row([\"ZRT\", \"{0:0.3f}\".format(eval_pid_dict['zrt_sim'])])\n",
    "        out_table_2.add_row([\"ES content* word match\", eval_pid_dict['cnt_es_w_sim']])\n",
    "        out_table_2.add_row([\"ES word match\", eval_pid_dict['es_w_sim']])\n",
    "        out_table_2.add_row([\"ES phoneme Edit Similarity\", \"{0:0.3f}\".format(eval_pid_dict['es_p_sim'])])\n",
    "        out_table_2.add_row([\"EN content* word match\", eval_pid_dict['en_w_cnt_count_sim_0']])\n",
    "        out_table_2.add_row([\"EN content* words Hellinger Similarity\", \\\n",
    "                             \"{0:0.3f}\".format(eval_pid_dict['en_w_cnt_hgr_sim_0'])])\n",
    "        out_table_2.add_row([\"EN word match\", eval_pid_dict['en_w_count_sim_0']])\n",
    "        out_table_2.add_row([\"EN words Hellinger Similarity\", \"{0:0.3f}\".format(eval_pid_dict['en_w_hgr_sim_0'])])\n",
    "        \n",
    "        print(\"Pair text details\")\n",
    "        print(out_table_1)\n",
    "        print(\"Similarity metrics\")\n",
    "        print(out_table_2)\n",
    "        print(\"Node details\")\n",
    "        \n",
    "        out_table_3 = PrettyTable([\"node id\", \"uttrnce id\", \"spk id\", \"start(ms)\", \"end(ms)\", \"dur(ms)\"], hrules=True)\n",
    "        out_table_3.add_row([eval_pid_dict['n1'], eval_pid_dict[\"uid1\"], \\\n",
    "                             eval_pid_dict[\"chid1\"], n1.start*10, n1.end*10, ((n1.end - n1.start)*10)])\n",
    "        out_table_3.add_row([eval_pid_dict['n2'], eval_pid_dict[\"uid2\"], \\\n",
    "                             eval_pid_dict[\"chid2\"], n2.start*10, n2.end*10, ((n2.end - n2.start)*10)])\n",
    "        \n",
    "        print(out_table_3)\n",
    "        \n",
    "        if detail:\n",
    "            print(\"ES transcript for node: %d\" % eval_pid_dict['n1'])\n",
    "            print(self.print_es_words(eval_pid_dict['n1']))\n",
    "            print(\"\\nES transcript for node: %d\" % eval_pid_dict['n2'])\n",
    "            print(self.print_es_words(eval_pid_dict['n2']))\n",
    "        \n",
    "        if not os.path.exists(out_fil):\n",
    "            out_fil_n1 = os.path.join(pair_wavs_path, \"n{0:d}.wav\".format(self.eval_pairs_list[pair_id]['n1']))\n",
    "            self.gen_segment_wav(self.file_info_dict[n1.wav_fil].target_file+\".wav\", \\\n",
    "                                 n1.start, n1.end, out_fil_n1)\n",
    "\n",
    "            out_fil_n2 = os.path.join(pair_wavs_path, \"n{0:d}.wav\".format(self.eval_pairs_list[pair_id]['n2']))\n",
    "            self.gen_segment_wav(self.file_info_dict[n2.wav_fil].target_file+\".wav\", \\\n",
    "                                 n2.start, n2.end, out_fil_n2)\n",
    "    #                 \"sox short.ogg -p pad 0 6|sox - long.ogg output.ogg\"\n",
    "#             print(\"merge command {0:s}\".format(' '.join([\"sox\", out_fil_n1, \"-p\", \"pad\", \"0\", \"0.5\", \"|\", \\\n",
    "#                          \"sox\", \"-\", out_fil_n2, out_fil])))\n",
    "            ps = subprocess.Popen((\"sox\", out_fil_n1, \"-p\", \"pad\", \"0\", \"0.5\"), stdout=subprocess.PIPE)\n",
    "            output = subprocess.check_output((\"sox\", \"-\", out_fil_n2, out_fil), stdin=ps.stdout)\n",
    "            ps.wait()\n",
    "            os.remove(out_fil_n1)\n",
    "            os.remove(out_fil_n2)\n",
    "        return IPython.display.Audio(out_fil)\n",
    "    \n",
    "    def print_es_words(self, nid):\n",
    "        out_table_1 = PrettyTable([\"start(ms)\", \"end(ms)\", \"word\"], hrules=True)\n",
    "        for v in self.es_words_dict[self.node_dict[nid].wav_fil]:\n",
    "            out_table_1.add_row([v.start*10, v.end*10, v.word])\n",
    "        return out_table_1\n",
    "    \n",
    "    def play_node_source_wav(self, nid):\n",
    "        print(\"File: %des \" % (self.file_info_dict[self.node_dict[nid].wav_fil].target_file + \".wav\"))\n",
    "        return IPython.display.Audio(self.file_info_dict[self.node_dict[nid].wav_fil].target_file + \".wav\")\n",
    "    \n",
    "    def save_state(self):\n",
    "        state_dict = {}\n",
    "        state_dict['nodes'] = self.node_dict\n",
    "        state_dict['edges'] = self.edges_dict\n",
    "        state_dict['pairs_list'] = self.pairs_list\n",
    "        state_dict['eval_pairs'] = self.eval_pairs_list\n",
    "        state_dict['w2i'] = self.w2i\n",
    "        state_dict['i2w'] = self.i2w\n",
    "        state_dict['en_words_stats'] = self.en_words_stats\n",
    "        state_dict['en_vocab'] = self.en_vocab\n",
    "        \n",
    "        print(\"Saving state dictionary ...\")\n",
    "        post_state_dict_path = os.path.join(self.ch_config[\"zrt_out_path\"], \"post_state.dict\")\n",
    "        cPickle.dump(state_dict, open(post_state_dict_path, \"wb\"))\n",
    "        print(\"Finished saving state dictionary ...\")\n",
    "\n",
    "    \n",
    "    def load_state(self, full=False):\n",
    "        print(\"Loading prep state dictionary ...\")\n",
    "        prep_state_dict_path = os.path.join(self.ch_config[\"exp_path\"], \"prep_state.dict\")\n",
    "        state_dict = cPickle.load(open(prep_state_dict_path, \"rb\"))\n",
    "        self.file_info_dict = state_dict['file_info']\n",
    "        self.es_phones_dict = state_dict['es_phones']\n",
    "        self.en_words_dict = state_dict['en_words']\n",
    "        self.es_words_dict = state_dict['es_words']\n",
    "        self.es_cnt_words_dict = state_dict['es_cnt_words']\n",
    "        self.en_cnt_words_dict = state_dict['en_cnt_words']\n",
    "        print(\"Finished loading prep state dictionary ...\")\n",
    "        \n",
    "        if full:\n",
    "            print(\"Loading post state dictionary ...\")\n",
    "            post_state_dict_path = os.path.join(self.ch_config[\"zrt_out_path\"], \"post_state.dict\")\n",
    "            state_dict = cPickle.load(open(post_state_dict_path, \"rb\"))\n",
    "            self.node_dict = state_dict['nodes']\n",
    "            self.edges_dict = state_dict['edges']\n",
    "            self.pairs_list = state_dict['pairs_list']\n",
    "            self.eval_pairs_list = state_dict['eval_pairs']\n",
    "            self.w2i = state_dict['w2i']\n",
    "            self.i2w = state_dict['i2w']\n",
    "            self.en_words_stats = state_dict['en_words_stats']\n",
    "            self.en_vocab = state_dict['en_vocab']\n",
    "\n",
    "            print(\"Finished loading post state dictionary ...\")\n",
    "        \n",
    "            print(\"Setting up eval data frame ...\")\n",
    "            self.eval_df_full = pd.DataFrame(self.eval_pairs_list)\n",
    "            self.eval_df = self.eval_df_full[(self.eval_df_full['no_mtch'] == False) & \\\n",
    "                                             (self.eval_df_full['sil_only'] == False)]\n",
    "            print(\"Saving dataframe ...\")\n",
    "            df_filename = os.path.join(self.ch_config[\"zrt_out_path\"], \"zrt.df\")\n",
    "            self.eval_df_full.to_pickle(df_filename)\n",
    "            print(\"Finished saving dataframe ...\")\n",
    "        print(\"Finished ...\")\n",
    "    \n",
    "    def eval_precision_recall(self, sim_col='zrt_sim', step=0.005):\n",
    "        min_sim = self.eval_df[sim_col].min()\n",
    "        max_sim = self.eval_df[sim_col].max()\n",
    "        \n",
    "        # Get the full count of good scores\n",
    "        num_good_scores = len(self.eval_df[self.eval_df['cnt_es_w_sim'] > 0])\n",
    "        num_bad_scores = len(self.eval_df[self.eval_df['cnt_es_w_sim'] == 0])\n",
    "        print(\"# eval scores: %d\" % len(self.eval_df))\n",
    "        print(\"# good scores: %d\" % num_good_scores)\n",
    "        print(\"#  bad scores: %d\" % num_bad_scores)\n",
    "        \n",
    "        # Generate a range of threshold values\n",
    "        thresh_array = np.arange(min_sim-step, max_sim+step, step)\n",
    "        precision_array = np.zeros(len(thresh_array))\n",
    "        recall_array = np.zeros(len(thresh_array))\n",
    "        inverse_recall_array = np.zeros(len(thresh_array))\n",
    "        for i, thresh in enumerate(thresh_array):\n",
    "            # Get the count of pairs which meet the threshold\n",
    "            num_thresh_scores = len(self.eval_df[self.eval_df[sim_col] >= thresh])\n",
    "            # Get the count of pairs which are good based on content word matches\n",
    "            num_good_thresh_scores = len(self.eval_df[(self.eval_df[sim_col] >= thresh) & \\\n",
    "                                                  (self.eval_df['cnt_es_w_sim'] > 0)])\n",
    "            if num_thresh_scores > 0:\n",
    "                precision_array[i] = (num_good_thresh_scores * 1.0) / num_thresh_scores\n",
    "            else:\n",
    "                precision_array[i] = 0.0\n",
    "            recall_array[i] = (num_good_thresh_scores * 1.0) / num_good_scores\n",
    "            \n",
    "            num_bad_scores_thresh = len(self.eval_df[(self.eval_df[sim_col] < thresh) & \\\n",
    "                                                  (self.eval_df['cnt_es_w_sim'] == 0)])\n",
    "            \n",
    "            inverse_recall_array[i] = \\\n",
    "                1.0 - ((num_bad_scores_thresh * 1.0) / num_bad_scores)\n",
    "            \n",
    "        print('Average Precision - using sim: %s' % sim_col)\n",
    "        print('-' * 40)\n",
    "        print('\\t%0.5f' % \n",
    "              np.trapz(precision_array[::-1], recall_array[::-1]))\n",
    "        print('-' * 40)\n",
    "\n",
    "        return precision_array, recall_array, inverse_recall_array\n",
    "    \n",
    "    def plot_precision_recall(self, both=False):\n",
    "        fig = plt.figure()\n",
    "        plt.tick_params(which='both', labelsize=14)\n",
    "        fig.set_size_inches(18.5, 10.5)\n",
    "        \n",
    "        p_vals, r_vals, ir_vals = self.eval_precision_recall()\n",
    "        if both:\n",
    "            p_i_vals, r_i_vals, ir_i_vals = \\\n",
    "                        self.eval_precision_recall(sim_col='sim mixed')\n",
    "        \n",
    "\n",
    "        ax_0 = plt.subplot2grid((3,2),(0, 0))\n",
    "        ax_0.plot(p_vals, label='Precision')\n",
    "        ax_0.plot(r_vals, label='Recall')\n",
    "        ax_0.plot(ir_vals, label='Inverse Recall')\n",
    "        ax_0.set_ylabel(\"original\", fontsize=20)\n",
    "        ax_0.set_xlabel(\"Threshold\", fontsize=20)\n",
    "        plt.tick_params(which='both', labelsize=14)\n",
    "        ax_0.legend(fontsize=14)\n",
    "        \n",
    "        if both:\n",
    "            ax_i = plt.subplot2grid((3,2),(0, 1))\n",
    "            ax_i.plot(p_i_vals, label='Precision')\n",
    "            ax_i.plot(r_i_vals, label='Recall')\n",
    "            ax_i.plot(ir_i_vals, label='Inverse Recall')\n",
    "            ax_i.set_ylabel(\"interpolated\", fontsize=20)\n",
    "            ax_i.set_xlabel(\"Threshold\", fontsize=20)\n",
    "            plt.tick_params(which='both', labelsize=14)\n",
    "            ax_i.legend(fontsize=14)\n",
    "#         plt.legend(fontsize=14)\n",
    "        \n",
    "        ax_pr = plt.subplot2grid((3,2),(1, 0), colspan=2)\n",
    "        ax_pr.plot(r_vals, p_vals, label='orig')\n",
    "        if both:\n",
    "            ax_pr.plot(r_i_vals, p_i_vals, label='interp')\n",
    "        ax_pr.set_ylabel(\"Precision\", fontsize=20)\n",
    "        ax_pr.set_xlabel(\"Recall\", fontsize=20)\n",
    "        plt.xlim(0.0,1.0)\n",
    "        plt.ylim(0.0,1.0)\n",
    "        plt.tick_params(which='both', labelsize=14)\n",
    "        ax_pr.legend(fontsize=14)\n",
    "#         plt.legend(fontsize=14)\n",
    "\n",
    "        ax_roc = plt.subplot2grid((3,2),(2, 0), colspan=2)\n",
    "        ax_roc.plot(ir_vals, r_vals, label='orig')\n",
    "        if both:\n",
    "            ax_roc.plot(ir_i_vals, r_i_vals, label='interp')\n",
    "        ax_roc.set_ylabel(\"TPR\", fontsize=20)\n",
    "        ax_roc.set_xlabel(\"FPR\", fontsize=20)\n",
    "        plt.xlim(0.0,1.0)\n",
    "        plt.ylim(0.0,1.0)\n",
    "        plt.tick_params(which='both', labelsize=14)\n",
    "        ax_roc.legend(fontsize=14)\n",
    "#         plt.legend(fontsize=14)\n",
    "\n",
    "        fig.tight_layout()\n",
    "    \n",
    "    def gen_vocab(self):\n",
    "        print(\"Generating vocabulary of English words ... \")\n",
    "        # Loop through node list, and append en words\n",
    "        # to list\n",
    "        en_words_in_all_nodes = []\n",
    "        for nid, node_info in self.node_dict.items():\n",
    "            en_words_in_all_nodes.extend(self.en_words_dict[node_info.wav_fil])\n",
    "        \n",
    "        # set en vocab\n",
    "        self.en_words_stats = Counter(en_words_in_all_nodes)\n",
    "        self.en_vocab = set(en_words_in_all_nodes)\n",
    "        \n",
    "        # word to integer, and integer to word mapping\n",
    "        self.w2i = {}\n",
    "        self.i2w = {}\n",
    "        for i,w in enumerate(self.en_vocab):\n",
    "            self.w2i[w] = i\n",
    "            self.i2w[i] = w\n",
    "        print(\"Finished vocab ...\")\n",
    "    \n",
    "    def gen_en_cnt_w_dict(self):\n",
    "        print(\"Generating dictionary of English content words ... \")\n",
    "        # Loop through node list, and append en words\n",
    "        # to list\n",
    "        self.en_cnt_words_dict = {}\n",
    "        for i, sp_fil in enumerate(self.en_words_dict):\n",
    "            if i % 5000 == 0:\n",
    "                print(\"Processed %d speech utterances\" % i)\n",
    "            en_w_in_uttr = self.en_words_dict[sp_fil]\n",
    "            en_w_cnt_in_uttr = \\\n",
    "                [word for word in en_w_in_uttr if word.lower().decode(\"utf-8\") not in stopwords.words('english')]\n",
    "            \n",
    "            self.en_cnt_words_dict[sp_fil] = en_w_cnt_in_uttr\n",
    "        \n",
    "        print(\"Finished generating English content word dict ...\")\n",
    "        \n",
    "    def gen_en_words_belief(self):\n",
    "        print(\"Generating initial belief over English translations\")\n",
    "        self.en_w_belief = {}\n",
    "        self.en_w_cnt_belief = {}\n",
    "        for i, (nid, node_info) in enumerate(self.node_dict.items()):\n",
    "            if i % 100000 == 0:\n",
    "                print(\"Processed %d nodes\" % (i+1))\n",
    "            en_words_in_uttr = self.en_words_dict[node_info.wav_fil]\n",
    "            num_words = len(en_words_in_uttr)\n",
    "            self.en_w_belief[nid] = {self.w2i[w]: 1/num_words for w in en_words_in_uttr}\n",
    "            # Get the ids of content words\n",
    "            en_w_cnt_in_uttr = self.en_cnt_words_dict[node_info.wav_fil]\n",
    "            num_cnt_word = len(en_w_cnt_in_uttr)\n",
    "            self.en_w_cnt_belief[nid]  = {self.w2i[w]: 1/num_cnt_word for w in en_w_cnt_in_uttr}\n",
    "            \n",
    "    def similarity_hellinger(self, p1_dict, p2_dict):\n",
    "        common_keys = set(p1_dict.keys()) & set(p2_dict.keys())\n",
    "        b_coeff = 0.0\n",
    "        for key in common_keys:\n",
    "            b_coeff += math.sqrt(p1_dict[key] * p2_dict[key])\n",
    "\n",
    "        temp_val = max(0.0, (1.0-b_coeff))\n",
    "        hellinger_dist = math.sqrt(temp_val)\n",
    "        return (1.0-hellinger_dist)\n",
    "    \n",
    "    def similarity_jaccard(self, p1_dict, p2_dict):\n",
    "        common_keys = set(p1_dict.keys()) & set(p2_dict.keys())\n",
    "        union_keys = set(p1_dict.keys()) | set(p2_dict.keys())\n",
    "        jaccard_dist = (0 if len(union_keys) == 0 else len(common_keys) / len(union_keys))\n",
    "        return jaccard_dist\n",
    "    \n",
    "    def similarity_en_count(self, p1, p2):\n",
    "        en_cnt_match = len(set(p1.keys()) & set(p2.keys()))\n",
    "        return min(1, en_cnt_match)\n",
    "    \n",
    "    def gen_en_words_sim(self, en_w_belief_dict):\n",
    "        print(\"Generating similarity between nodes based on English translations ...\")\n",
    "        # create or reset sim matrix\n",
    "        en_w_hgr_sim = {}\n",
    "#         en_w_jcrd_sim = {}\n",
    "#         en_w_cnt_sim = {}\n",
    "        \n",
    "        # loop over the edges list, and add sim val\n",
    "        for n1, node_sim_dict in self.edges_dict.items():\n",
    "            en_w_hgr_sim[n1] = {}\n",
    "#             en_w_jcrd_sim[n1] = {}\n",
    "#             en_w_cnt_sim[n1] = {}\n",
    "            for n2, sim_val in node_sim_dict.items():\n",
    "                # compute belief similarity between n1 and n2\n",
    "                # only if in different utterances\n",
    "                if self.node_dict[n1].wav_fil != self.node_dict[n2].wav_fil:\n",
    "                    en_w_hgr_sim[n1][n2] = self.similarity_hellinger(en_w_belief_dict[n1], en_w_belief_dict[n2])\n",
    "#                     en_w_jcrd_sim[n1][n2] = self.similarity_jaccard(en_w_belief_dict[n1], en_w_belief_dict[n2])\n",
    "#                     en_w_cnt_sim[n1][n2] = self.similarity_en_count(en_w_belief_dict[n1], en_w_belief_dict[n2])\n",
    "                else:\n",
    "                    en_w_hgr_sim[n1][n2] = 0\n",
    "#                     en_w_jcrd_sim[n1][n2] = 0\n",
    "#                     en_w_cnt_sim[n1][n2] = 0\n",
    "        print(\"Finished generating similarity ...\")\n",
    "        #return en_w_hgr_sim, en_w_jcrd_sim, en_w_cnt_sim\n",
    "        return en_w_hgr_sim\n",
    "    \n",
    "    \n",
    "    def exp_wav_files_dur(self):\n",
    "        total_dur = 0; total_dur_trns = 0\n",
    "        # Get duration from VAD files\n",
    "        for key in sorted(self.b_base_name):\n",
    "            # Get duration from non sil, sp parts in transcriptions\n",
    "            for t in self.es_words_dict[key]:\n",
    "                total_dur_trns += (((t.end-t.start)*10) if t.word not in ['sil','sp'] else 0)\n",
    "                \n",
    "            # Get duration from utterance level VAD    \n",
    "            total_dur += ((self.file_info_dict[key].vad.end - self.file_info_dict[key].vad.start)*10)\n",
    "        \n",
    "        print(\"Total wav files: %d\" %(len(self.b_base_name)))\n",
    "        out_t = PrettyTable([\"Type\", \"Total duration (secs)\", \"Total duration (hours)\"], hrules=True)\n",
    "        out_t.add_row([\"Utterance level VAD\", total_dur//1000, \"{0:.2f}\".format(total_dur/(1000*3600))])\n",
    "        out_t.add_row([\"Transcript non sil/sp word durations\", total_dur_trns//1000, \\\n",
    "                       \"{0:.2f}\".format(total_dur_trns/(1000*3600))])\n",
    "        \n",
    "        evad_dur_ms = self.zrt_calc_dur_from_evad()\n",
    "        out_t.add_row([\"Energy based\", evad_dur_ms//1000, \\\n",
    "                       \"{0:.2f}\".format(evad_dur_ms/(1000*3600))])\n",
    "        \n",
    "        \n",
    "        print(out_t)\n",
    "\n",
    "    \n",
    "    def init_speaker_info(self):\n",
    "        # MISSING INFO and EXTRA INFO\n",
    "        # More than one gender per channel\n",
    "        # Age, and speaker id only available for evaltst files\n",
    "        print(\"Reading speaker and call info ...\")\n",
    "        # Read caller info:\n",
    "        # gender, age, id\n",
    "        # id for channel A is known, for channel B, always assign unique id per speech file\n",
    "        # Info to be stored in a dictionary\n",
    "        # Key: utterance id: speech_file.chid\n",
    "        # Set of files in the corpus:\n",
    "        keys_corpus_files = set([x.source_file for x in self.file_info_dict.values()])\n",
    "        self.spkrinfo = {}\n",
    "        keys_callinfo = []\n",
    "        keys_spkrinfo = []\n",
    "        # Process callinfo.tbl\n",
    "        with open(self.ch_config[\"callinfo\"], \"r\") as in_f:\n",
    "            for i, line in enumerate(in_f, start=0):\n",
    "                if any(substring in line for substring in [\"siA ntalkers\", \"siB ntalkers\"]):\n",
    "                    line_items = line.strip().split()\n",
    "                    sp_fil = line_items[0]\n",
    "                    if \"sp_\" not in sp_fil:\n",
    "                        sp_fil = \"sp_\" + sp_fil\n",
    "                    if sp_fil in keys_corpus_files:\n",
    "                        chid = line_items[1].replace('si', '')\n",
    "                        if chid == \"A\":\n",
    "                            chid = 1\n",
    "                        elif chid == \"B\":\n",
    "                            chid = 2\n",
    "                        else:\n",
    "                            print(\"Incorrect chid\")\n",
    "                        dict_key = \"{0:s}.{1:d}\".format(sp_fil, chid)\n",
    "                        if dict_key not in self.spkrinfo:\n",
    "                            self.spkrinfo[dict_key] = {}\n",
    "                            keys_callinfo.append(dict_key)\n",
    "                        nGender, nSpkrs = line_items[2].split(\"=\")\n",
    "                        self.spkrinfo[dict_key][nGender] = nSpkrs\n",
    "        \n",
    "        print(\"callinfo.tbl - # speakers: %d\" % (len(set(keys_callinfo))/2))\n",
    "        print(\"Finished call info ...\")\n",
    "        \n",
    "        # Process spkrinfo.tbl\n",
    "        with open(self.ch_config[\"spkrinfo\"], \"r\") as in_f:\n",
    "            for i, line in enumerate(in_f, start=0):\n",
    "                line_items = line.strip().split(',')\n",
    "                sp_fil = line_items[0]\n",
    "                if sp_fil in keys_corpus_files:\n",
    "                    # Add info for channel A\n",
    "                    dict_key = dict_key = \"{0:s}.{1:d}\".format(sp_fil, 1)\n",
    "                    if dict_key not in self.spkrinfo:\n",
    "                        self.spkrinfo[dict_key] = {}\n",
    "                        keys_spkrinfo.append(dict_key)\n",
    "                    self.spkrinfo[dict_key][\"age\"] = int(-1 if line_items[2] == '' else line_items[2])\n",
    "                    self.spkrinfo[dict_key][\"spid\"] = line_items[-1]\n",
    "                    # Add info for channel B\n",
    "                    dict_key = dict_key = \"{0:s}.{1:d}\".format(sp_fil, 2)\n",
    "                    if dict_key not in self.spkrinfo:\n",
    "                        self.spkrinfo[dict_key] = {}\n",
    "                        keys_spkrinfo.append(dict_key)\n",
    "                    # Info not available\n",
    "                    self.spkrinfo[dict_key][\"age\"] = -1\n",
    "                    self.spkrinfo[dict_key][\"spid\"] = \"uknown_\"+dict_key\n",
    "        \n",
    "        print(\"callinfo.tbl - # speakers: %d\" % (len(set(keys_spkrinfo))))\n",
    "        print(\"Finished speaker info ...\")\n",
    "        self.keys_callinfo = set(keys_callinfo)\n",
    "        self.keys_spkrinfo = set(keys_spkrinfo)\n",
    "#         print(set(keys_callinfo) & set(keys_spkrinfo))\n",
    "#         print(len(set(keys_callinfo)), len(set(keys_spkrinfo)))\n",
    "#         print(set(keys_callinfo)-set(keys_spkrinfo))\n",
    "#         print(set(keys_spkrinfo)-set(keys_callinfo))\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def init_label_prop(self):\n",
    "        self.label_prop_edges = LabelProp(self.edges_dict, alpha=0.1)\n",
    "        self.label_prop_en_words = LabelProp(self.en_w_belief, alpha=0.2)\n",
    "        self.label_prop_en_cnt_words = LabelProp(self.en_w_cnt_belief, alpha=0.2)\n",
    "    \n",
    "    \n",
    "    def init_zrt(self, fast=False):\n",
    "        if fast == False:        \n",
    "            # Load ZRT prep output\n",
    "            self.load_state(full=False)\n",
    "            # Read matches.nodes\n",
    "            self.read_nodes()\n",
    "            # Read matches.edges\n",
    "            self.read_edges()\n",
    "            # Generate english vocab\n",
    "            self.gen_vocab()\n",
    "            # Initialise uniform belief over the english translations\n",
    "            # for each disovered node\n",
    "            self.gen_en_words_belief()\n",
    "            # Generate similarity matrix between nodes, using the english translations\n",
    "            self.en_w_hgr_sim_0 = self.gen_en_words_sim(self.en_w_belief)\n",
    "            # Generate similarity matrix between nodes, using the english translations\n",
    "            self.en_w_cnt_hgr_sim_0 = self.gen_en_words_sim(self.en_w_cnt_belief)\n",
    "            #self.init_speaker_info()\n",
    "            self.eval_pairs()\n",
    "            self.save_state()\n",
    "        else:\n",
    "            self.load_state(full=True)\n",
    "            # Initialise uniform belief over the english translations\n",
    "            # for each disovered node\n",
    "            self.gen_en_words_belief()\n",
    "            # Generate similarity matrix between nodes, using the english translations\n",
    "            self.en_w_hgr_sim_0 = self.gen_en_words_sim(self.en_w_belief)\n",
    "            # Generate similarity matrix between nodes, using the english translations\n",
    "            self.en_w_cnt_hgr_sim_0 = self.gen_en_words_sim(self.en_w_cnt_belief)\n",
    "            #self.init_speaker_info()\n",
    "            self.eval_pairs()\n",
    "            \n",
    "#         self.init_label_prop()\n",
    "    \n",
    "    def interpolate_sim(self, alpha):\n",
    "        print(\"Interpolating similarity matrix ...\")\n",
    "        # Compute the similarity matrix as per follows:\n",
    "        # sim(n1,n2) = alpha * ZRT(n1,n2) + (1-alpha) * EN_translation_sim(n1,n2)\n",
    "        self.sim_mix = {}\n",
    "        self.sim_mix_cont = {}\n",
    "        for n1, node_sim_dict in self.edges_dict.items():\n",
    "            self.sim_mix[n1] = {}\n",
    "            self.sim_mix_cont[n1] = {}\n",
    "            for n2, val in node_sim_dict.items():\n",
    "                if self.node_dict[n1].wav_fil != self.node_dict[n2].wav_fil:\n",
    "                    # interpolate using similarity over entire English translation\n",
    "                    self.sim_mix[n1][n2] = ((1-alpha)*val) + (alpha*self.en_w_hgr_sim_0[n1][n2])\n",
    "                    # interpolate using content words in English translation\n",
    "                    self.sim_mix_cont[n1][n2] = ((1-alpha)*val) + (alpha*self.en_w_cnt_hgr_sim_0[n1][n2])\n",
    "                else:\n",
    "                    self.sim_mix[n1][n2] = val\n",
    "                    self.sim_mix_cont[n1][n2] = val\n",
    "        \n",
    "        print(\"Finished interpolating similarity matrix ...\")\n",
    "        \n",
    "        print(\"Updating data frame ...\")\n",
    "        eval_pairs_sim_mixed_list = []\n",
    "        for entry in self.eval_pairs_list:\n",
    "            eval_pairs_sim_mixed_list.append(self.sim_mix_cont[entry['n1']][entry['n2']])\n",
    "        self.eval_df_full['sim mixed'] = pd.Series(eval_pairs_sim_mixed_list, index=self.eval_df_full.index)\n",
    "        self.eval_df = self.eval_df_full[(self.eval_df_full['no_mtch'] == False) & \\\n",
    "                                         (self.eval_df_full['sil_only'] == False)]\n",
    "        print(\"Finished updating data frame ...\")\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def calc_node_overlap(self, n1, n2):\n",
    "        den = max(self.node_dict[n1].end, self.node_dict[n2].end) - \\\n",
    "              min(self.node_dict[n1].start, self.node_dict[n2].start)\n",
    "        num = (self.node_dict[n1].end - self.node_dict[n1].start) + \\\n",
    "              (self.node_dict[n2].end - self.node_dict[n2].start)\n",
    "        olap = (num / den) - 1\n",
    "        return olap\n",
    "    \n",
    "    def gen_zrt_df(self):\n",
    "        #df_filename = os.path.join(self.ch_config[\"zrt_out_path\"], \"zrt.df\")\n",
    "        df_zrt = self.eval_df_full.copy(deep=True)\n",
    "        df_zrt['spk1'] = df_zrt.apply(lambda row: (\"%s.%s\" % (row['chid1'], row['uid1'])), axis=1)\n",
    "        df_zrt['spk2'] = df_zrt.apply(lambda row: (\"%s.%s\" % (row['chid2'], row['uid2'])), axis=1)\n",
    "        cols_to_rename = {'zrt_sim':'ZRT', u'es_w_n1':u'ES words n1', u'es_w_n2':u'ES words n2', \n",
    "                   u'es_w_sim':'ES word match', u'es_p_sim':'ES phone edit sim',\n",
    "                   u'cnt_es_w_sim':'ES cont match', u'en_w_cnt_hgr_sim_0':'EN sim'}\n",
    "\n",
    "        cols_to_show = [u'n1', u'n2', 'ZRT', 'ES words n1', 'ES words n2', \n",
    "                'ES word match', 'ES phone edit sim', 'ES cont match', \n",
    "                'EN sim', 'sim mixed', u'spk1', u'spk2']\n",
    "        df_zrt = df_zrt.rename(columns=cols_to_rename)\n",
    "        return df_zrt, cols_to_show\n",
    "    \n",
    "    def label_prop_stuff(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LabelProp(object):\n",
    "    \n",
    "    def __init__(self, belief0, alpha):\n",
    "        self.belief_0 = belief0.copy()\n",
    "        self.belief_n = belief0.copy()\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def label_spread(self, sim_dict):\n",
    "        # Create a placeholder for the new belief\n",
    "        new_belief = {}\n",
    "        # Loop over all the entries in the similarity matrix\n",
    "        for i, (n1, node_sim_dict) in enumerate(sim_dict.items()):\n",
    "#             if i < 1:\n",
    "#                 print(\"Label spreading start ...\")\n",
    "#                 print(n1)\n",
    "#                 print(self.belief_n[n1])\n",
    "            \n",
    "            # Create new belief for node id\n",
    "            new_belief[n1] = {}\n",
    "            # Loop through all the similar nodes to this node id\n",
    "            temp_belief = {}\n",
    "            for n2, sim_val in node_sim_dict.items():\n",
    "                for bid, bval in self.belief_n[n2].items():\n",
    "                    if bid not in temp_belief:\n",
    "                        temp_belief[bid] = sim_val * bval\n",
    "                    else:\n",
    "                        temp_belief[bid] += sim_val * bval\n",
    "            # Normalize new belief\n",
    "            sum_belief = sum(temp_belief.values())\n",
    "            if sum_belief > 0:\n",
    "                temp_belief = {k:(v/sum_belief) for (k,v) in temp_belief.items()}\n",
    "            # Combine new belief: temp_belief, and current belief: self.belief_n\n",
    "            new_belief_keys = set(self.belief_n[n1].keys()) | set(temp_belief.keys())\n",
    "            for key in new_belief_keys:\n",
    "                new_belief[n1][key] = (self.alpha * temp_belief.get(key,0) + \\\n",
    "                                       ((1-self.alpha) * self.belief_0[n1].get(key,0)))\n",
    "#             if i < 1:\n",
    "#                 print(\"Label spreading end ...\")\n",
    "#                 print(n1)\n",
    "#                 print(new_belief[n1])\n",
    "\n",
    "            \n",
    "        # Update current belief\n",
    "        self.belief_n = new_belief\n",
    "    \n",
    "    \n",
    "    def normalize_dict(self, some_dict):\n",
    "        in_dict = some_dict.copy()\n",
    "        vals = []\n",
    "        for dict_1 in in_dict.values():\n",
    "            vals.extend(dict_1.values())\n",
    "        max_val = max(vals)\n",
    "        min_val = min(vals)\n",
    "        # Normalize values\n",
    "        for node_id in in_dict:      \n",
    "            in_dict[node_id] = \\\n",
    "                {k: (v-min_val) / (max_val-min_val) \\\n",
    "                 for k, v in in_dict[node_id].items()}\n",
    "        return in_dict\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_uttr_graph(edges_dict):\n",
    "    nodes = set(edges_dict.keys()[:5])\n",
    "    G = nx.Graph()\n",
    "    for node in nodes:\n",
    "        G.add_node(node)\n",
    "    \n",
    "    pos = nx.shell_layout(G)\n",
    "    nx.draw(G, pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "\n",
    "- Add function to create files.base\n",
    "- Add function to create wav files from pairs output, add a limit on the number of wavs created\n",
    "- Start adding evaluation metrics\n",
    "\n",
    "- Add a visualization to show connections between utterances, maybe like a heatmap? or a graph with nodes as the utterances, and edges as the number of pairs found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZRT Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ch_prep = CallHomeZRTPrep(\"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%timeit -n1 -r1\n",
    "# ch_prep.something_to_do()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%timeit -n1 -r1\n",
    "# ch_prep.load_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%timeit -n1 -r1\n",
    "# ch_prep.gen_wav_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ch_prep.b_vad_files[:1]\n",
    "# # ch_prep.b_vad_files[:-1]\n",
    "# print(len(ch_prep.b_vad_files))\n",
    "# print(ch_prep.b_vad_files[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%timeit -n1 -r1\n",
    "# ch_prep.ch_zrt_plp();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%timeit -n1 -r1\n",
    "# ch_prep.ch_zrt_lsh();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%timeit -n1 -r1\n",
    "# ch_prep.ch_zrt_gen_exp_command()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ZRT Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def callhome_eval_init(config_file = \"config.json\", fast=False):\n",
    "    ch = CallHomeEval(config_file)\n",
    "    ch.init_zrt(fast)\n",
    "    ch.zrt_calc_dur_from_evad()\n",
    "    ch.exp_wav_files_dur()\n",
    "    ch.interpolate_sim(.1)\n",
    "    return ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ch = CallHomeEval(\"config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%timeit -n1 -r1\n",
    "# ch.init_zrt(fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%timeit -n1 -r1\n",
    "# ch.init_zrt(fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(ch.b_vad_files[1])\n",
    "# ch.zrt_calc_dur_from_evad()\n",
    "# ch.exp_wav_files_dur()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%timeit -n1 -r1\n",
    "# ch.interpolate_sim(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%timeit -n1 -r1\n",
    "# ch.init_label_prop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%timeit -n1 -r1\n",
    "# df_filename = os.path.join(ch.ch_config[\"zrt_out_path\"], \"zrt.df\")\n",
    "# print(\"Saving dataframe ...\")\n",
    "# ch.eval_df_full.to_pickle(df_filename)\n",
    "# print(\"Finished saving dataframe ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dedup_file = os.path.join(ch.ch_config[\"zrt_out_path\"], \"master_graph.dedups\")\n",
    "# cluster_depth = []\n",
    "# with open(dedup_file, \"r\") as in_f:\n",
    "#     for line in in_f:\n",
    "#         cluster_depth.append(len(line.strip().split()))\n",
    "\n",
    "#     pass\n",
    "\n",
    "# print(np.max(cluster_depth))\n",
    "# print(np.mean(sorted(cluster_depth, reverse=True)[1:]))\n",
    "# print(sorted(cluster_depth, reverse=True)[:10])\n",
    "\n",
    "# set_valid_ids = set(ch.eval_df['n1'].values) | set(ch.eval_df['n2'].values)\n",
    "# edges_depth = [(k, len(v)) for k, v in ch.edges_dict.items() if k in set_valid_ids]\n",
    "# print(sorted(edges_depth, reverse=True, key=lambda t:t[1])[3000:3010])\n",
    "# # print(len(ch.eval_df))\n",
    "# # print(ch.eval_df['n1'].values[:5]))\n",
    "# print(len(set_valid_ids))\n",
    "\n",
    "# possible_bad_entries = 0\n",
    "# possible_good_entries = []\n",
    "# for i, entry in enumerate(ch.eval_pairs_list):\n",
    "#     if 'sil' in entry['es_w_n1'] or 'sil' in entry['es_w_n2'] or 'sp' in entry['es_w_n1'] or 'sp' in entry['es_w_n2']:\n",
    "#         possible_bad_entries += 1\n",
    "#     else:\n",
    "#         possible_good_entries.append(i)\n",
    "\n",
    "# print(possible_bad_entries, len(ch.eval_pairs_list))\n",
    "# print(possible_good_entries[:10])\n",
    "\n",
    "# set_valid_ids = set(ch.eval_df['n1'].ix[possible_good_entries].values) | set(ch.eval_df['n2'].ix[possible_good_entries].values)\n",
    "# edges_depth = [(k, len(v)) for k, v in ch.edges_dict.items() if k in set_valid_ids]\n",
    "# print(sorted(edges_depth, reverse=True, key=lambda t:t[1])[:10])\n",
    "# # print(len(ch.eval_df))\n",
    "# # print(ch.eval_df['n1'].values[:5]))\n",
    "# print(len(set_valid_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !python ../../ZRTools/scripts/mark_energy.py -i ../../corpora/callhome_es/out/062.216.wav  -o ../../corpora/callhome_es/out/062.216.tmp -s 0.4 -e 0 \n",
    "# !cat ../../corpora/callhome_es/out/062.216.tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LP utilities\n",
    "utilities to support label propagation over the ZRT output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Class lpexp:\n",
    "- read pair wise matches information\n",
    "- for each segment, read transcription info - both words and phonemes\n",
    "- for each segment, read corresponding English translation in the vad region\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notes:\n",
    "\n",
    "- For config0.80-0.97-0.80-50, nodes 1317, and 1318 are from different speakers, and also male and female"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Notes - 1-Aug-2016\n",
    "\n",
    "- For slideshow, watch: https://github.com/damianavila/RISE\n",
    "\n",
    "## Notes - 27-Jul-2016\n",
    "\n",
    "- For pairs which are good phonetic matches, but different semantically, how should we handle them? Just lowering the similarity may not be a good idea, depending upon the downstream application. For example, for translation, we might want to retain the discovered pair for pseudoword decoding, but we can append a unique id for each of the nodes, to indicate, a different part and a common part. E.g. n1, n2 = operation, variation. The common part is r\\*ation. we can then decode these nodes as following: UNIQ_1 r\\*ation, and UNIQ_2 r\\*ation. Need to think about this.\n",
    "\n",
    "## Notes - 22-Jul-2016\n",
    "\n",
    "- Generated wav files and vad files after fixing the multiple utterances into one bug\n",
    "- Transcription errors, where words are not missing in the transcriptions is also causing evaluation problems. E.g. from speech file: \"sp_0731\", vad id: 156, \"527.34 535.78 A: entonces t ve y pdele al prroco de San Antonio todos los datos que se requieran, todo lo que se necesita para este\". There is an extra \"para\" spoken after \"Antonio\". This is caught by ZRT, but evaluation fails, as no alignment\n",
    "\n",
    "## Notes - 18-jul-2016\n",
    "\n",
    "- Pair 72, example where ES transcriptions are incorrect. The transcripts includes words at times when there is only silence. This creates a problem as we are unable to filter out the pair just using\n",
    "- Pair 0, example where we have a good DTW match, but different words. Here label spreading should help us filter out the pairs\n",
    "- Pair 13, example with high DTW, but 0 EN sim. Label Spreading should lower the DTW value\n",
    "- Pair 18973, example with low(ish) DTW: 0.859, and high EN sim. Label spreading should increase the DTW value\n",
    "- Very few examples, where low DTW and high EN sim. In these examples as well, a majority are actually poor matches, corresponding to \"sil\", but with overlapping words in EN translations. ** Hyperparameter to control increasing DTW score should be low **\n",
    "\n",
    "\n",
    "## Analysis:\n",
    "\n",
    "1. Show pairs with missing matches, but no silent - done\n",
    "2. Show pairs with NO/ZERO match on the English side. Can we filter these out?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do:\n",
    "\n",
    "1. Add fnction to print:\n",
    " - ES transcript\n",
    " - EN transcript\n",
    " - File info\n",
    " - Return the wav file as IPython display audio object\n",
    " - Code snippet\n",
    " \n",
    "    ```python\n",
    "    print(ch.es_words_dict['001.202'])\n",
    "    print(ch.es_words_dict['006.116'])\n",
    "    print(ch.file_info_dict['001.202'])\n",
    "    print(ch.file_info_dict['006.116'])\n",
    "    ch.node_dict[71].es_words\n",
    "    ```\n",
    "\n",
    "2. Check ES transcripts for 'SIL' in the middle. Can see at the start and end. 'SP' occurs in the middle.\n",
    "3. Expand tail end of DTW threshold. Right now, only looking at pairs with DTW of 0.80 and above. Maybe there are many poor pairs below this threshold? In the current set, the majority of pairs have DTW of 1.0\n",
    "4. Plot cross speaker, and cross file matches\n",
    "5. **FIX** bug in channel id parsing for transcripts. There are multiple speakers, therefore, only checking for \"A:\" or \"B:\" will be wrong, as \"A1:\" and other similar ids exist\n",
    "6. Change the selected files to start from file id 41 onwards, as these are training files.\n",
    "7. **FIX** when merging multiple transcript lines, into 1 sentence, compute the start and end time carefully\n",
    "8. Compute the total duration of audio. Needs to be two parts, one from the vad regions used to split the wav files into utterances, and the second, for the actual non ['sil', 'sp'] parts in the transcriptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
