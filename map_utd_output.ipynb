{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import cPickle as pickle\n",
    "import json\n",
    "import subprocess\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "import bisect\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map ZRTools output to transcripts\n",
    "\n",
    "- Create modified .nodes file\n",
    "- Create mapping between es words, and nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"config.json\") as json_data_file:\n",
    "    config = json.load(json_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodes_fname = config[\"es\"]['nodes_fname']\n",
    "seg_nodes_fname = config[\"es\"]['seg_nodes_fname']\n",
    "nodes_dict_fname = config[\"es\"]['nodes_dict_fname']\n",
    "\n",
    "edges_utd_fname = config[\"es\"]['edges_utd_fname']\n",
    "edges_olap_fname = config[\"es\"]['edges_olap_fname']\n",
    "edges_all_fname = config[\"es\"]['edges_all_fname']\n",
    "edges_score_fname = config[\"es\"]['edges_score_fname']\n",
    "\n",
    "clusters_utd_fname = config['es']['clusters_utd_fname']\n",
    "clusters_fname = config['es']['clusters_fname']\n",
    "clusters_stats_fname = config['es']['clusters_stats_fname']\n",
    "\n",
    "pairs_fname = config['es']['score_pairs_fname']\n",
    "eval_fname = config['es']['eval_pairs_fname']\n",
    "\n",
    "feats_fname = config['es']['feats_fname']\n",
    "feats_dict_fname = config['es']['feats_dict_fname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Align = namedtuple('Align', ['word', 'start', 'end'])\n",
    "Node = namedtuple('Node', ['file', 'seg', 'start', 'end', 'es', 'es_cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segment_map = pickle.load(open(config['es']['segment_dict_fname'], \"rb\"))\n",
    "align_dict = pickle.load(open(config['es']['align_dict_fname'], \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes - identify the segment to which the node belongs\n",
    "\n",
    "Lookout for:\n",
    "1. Patterns that go across segment boundaries\n",
    "2. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_segid(node_start, node_end, file_id, segment_map):\n",
    "    seg_id_list, start_time_list = zip(*sorted(segment_map[file_id].items(), key=lambda t:t[0]))\n",
    "\n",
    "    # Binary search to find segment where the node starts and ends in\n",
    "    # we subtract 1 as bisect returns the index where we can insert a value keeping\n",
    "    # the sort order. We do not expect it to be 0, as the node will always have a 0 or positive start\n",
    "    # time\n",
    "    seg_id_start = bisect.bisect(start_time_list, node_start)-1\n",
    "    s1 = seg_id_list[seg_id_start]\n",
    "    seg_id_end = bisect.bisect(start_time_list, node_end)-1\n",
    "    s2 = seg_id_list[seg_id_end]\n",
    "    \n",
    "    if seg_id_start == seg_id_end:\n",
    "        start = node_start - segment_map[file_id][s1]\n",
    "        end = node_end - segment_map[file_id][s1]\n",
    "        return s1, start, end, 0\n",
    "    else:\n",
    "        # Calculate which segment overlaps more\n",
    "        #print (file_id, node_start, node_end, seg_id, seg_id_start, seg_id_end, seg_id_list[seg_id_start-1], seg_id_list[seg_id_end-1])\n",
    "        if (segment_map[file_id][s2] - node_start) >= (node_end - segment_map[file_id][s2]):\n",
    "            shift_value = node_end - segment_map[file_id][s2]\n",
    "            start = node_start - segment_map[file_id][s1] - shift_value\n",
    "            end = segment_map[file_id][s2] - segment_map[file_id][s1]\n",
    "            print(\"More in s1\", start, end, shift_value)\n",
    "            return s1, start, end, 1\n",
    "        else:\n",
    "            shift_value = segment_map[file_id][s2] - node_start\n",
    "            start = 0\n",
    "            end = node_end - segment_map[file_id][s2] + shift_value\n",
    "            print(\"More in s2\", start, end, shift_value)\n",
    "            return s2, start, end, 1\n",
    "    print (file_id, node_start, node_end, seg_id_start, seg_id_end)\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More in s1 429 561 8\n",
      "('042.079', 429, 561, 1)\n",
      "('038.001', 0, 51, 0)\n"
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "print(search_segid(20509, 20641, \"042\", segment_map))\n",
    "print(search_segid(0, 51, \"038\", segment_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes - create a master_graph.segnodes file replacing nodes with their segment ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_segmented_nodes(nodes_fname, segment_map, seg_nodes_fname):\n",
    "    total_errors = 0\n",
    "    with open(nodes_fname, \"r\") as nodes_f, open(seg_nodes_fname, \"w\") as segnodes_f:\n",
    "        for i, line in enumerate(nodes_f):\n",
    "            line_items = line.strip().split(None, 3)\n",
    "            file_id = line_items[0]\n",
    "            node_start, node_end = map(int, line_items[1:3])\n",
    "            try:\n",
    "                seg_id, seg_node_start, seg_node_end, e = search_segid(node_start, node_end, file_id, segment_map)\n",
    "                total_errors += e\n",
    "                outline = \"%s\\t%d\\t%d\\t%s\\n\" % (seg_id, seg_node_start, seg_node_end, line_items[3])\n",
    "                segnodes_f.write(outline)\n",
    "            except ValueError:\n",
    "                print(\"Incorrect line format at line: %d\\n%s\" % (i, line))\n",
    "                \n",
    "    print(\"Total nodes: %d\" % (i+1))\n",
    "    print(\"Total errors: %d\" % total_errors)\n",
    "    print(\"completed\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More in s2 0 52 4\n",
      "More in s2 0 121 9\n",
      "More in s2 0 57 1\n",
      "More in s2 0 56 9\n",
      "More in s1 221 273 5\n",
      "More in s2 0 55 3\n",
      "More in s2 0 62 3\n",
      "More in s2 0 59 3\n",
      "More in s2 0 61 4\n",
      "More in s2 0 63 1\n",
      "More in s1 254 321 15\n",
      "More in s1 265 321 19\n",
      "More in s2 0 56 1\n",
      "More in s1 258 321 6\n",
      "More in s1 264 321 9\n",
      "More in s1 231 292 0\n",
      "More in s2 0 55 9\n",
      "More in s1 264 321 1\n",
      "More in s2 0 53 2\n",
      "More in s1 260 321 4\n",
      "More in s1 253 321 12\n",
      "More in s1 348 401 19\n",
      "More in s1 92 148 2\n",
      "More in s2 0 55 1\n",
      "More in s2 0 61 6\n",
      "More in s1 3 59 1\n",
      "More in s1 290 346 1\n",
      "More in s2 0 52 1\n",
      "More in s2 0 58 2\n",
      "More in s2 0 52 1\n",
      "More in s1 153 211 2\n",
      "More in s2 0 59 1\n",
      "More in s2 0 65 4\n",
      "More in s2 0 65 3\n",
      "More in s1 262 355 0\n",
      "More in s2 0 82 2\n",
      "More in s2 0 106 5\n",
      "More in s1 468 519 2\n",
      "More in s1 449 609 9\n",
      "More in s1 601 751 4\n",
      "More in s2 0 59 4\n",
      "More in s1 375 451 5\n",
      "More in s1 32 88 2\n",
      "More in s1 392 448 0\n",
      "More in s1 466 517 1\n",
      "More in s1 249 303 2\n",
      "More in s1 73 124 1\n",
      "More in s2 0 51 1\n",
      "More in s2 0 52 1\n",
      "More in s1 245 296 1\n",
      "More in s1 250 303 1\n",
      "More in s1 3 59 1\n",
      "More in s1 91 158 5\n",
      "More in s1 252 321 18\n",
      "More in s2 0 63 1\n",
      "More in s1 32 88 2\n",
      "More in s1 265 321 19\n",
      "More in s2 0 58 1\n",
      "More in s1 94 158 2\n",
      "More in s1 901 953 1\n",
      "More in s2 0 57 1\n",
      "More in s2 0 58 22\n",
      "More in s1 238 292 12\n",
      "More in s2 0 53 13\n",
      "More in s2 0 56 1\n",
      "More in s2 0 56 9\n",
      "More in s2 0 58 2\n",
      "More in s2 0 52 1\n",
      "More in s1 7 59 2\n",
      "More in s1 16 73 1\n",
      "More in s1 -3 56 6\n",
      "More in s1 -5 52 5\n",
      "More in s1 239 292 8\n",
      "More in s1 242 295 5\n",
      "More in s1 66 126 1\n",
      "Total nodes: 29694\n",
      "Total errors: 75\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "create_segmented_nodes(nodes_fname, segment_map, seg_nodes_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes - find transcript words corresponding to node start and end times\n",
    "\n",
    "Create node dictionary:\n",
    "    - node id\n",
    "    - file id\n",
    "    - seg id\n",
    "    - start time\n",
    "    - end time\n",
    "    - es words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_align_words_for_node(align_words_list, start, end):\n",
    "    #display(align_words_list, start, end)\n",
    "    words, start_times, end_times = zip(*(align_words_list))\n",
    "    start_i = bisect.bisect(end_times, start)\n",
    "    # end index will be 1 beyond the actual end\n",
    "    end_i = bisect.bisect(start_times, end)\n",
    "    return words[start_i:end_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('VAMOS', 'A', 'VER')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('ESTA', 'MECHITA')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(find_align_words_for_node(align_dict[\"001\"][\"001.224\"][\"es\"], 191, 246))\n",
    "display(find_align_words_for_node(align_dict[\"001\"][\"001.274\"][\"es\"], 45, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_nodes_align(seg_nodes_fname, segment_map, align_dict):\n",
    "    total_errors = 0\n",
    "    nodes_dict = {}\n",
    "    with open(seg_nodes_fname, \"r\") as seg_nodes_f:\n",
    "        for nid, line in enumerate(seg_nodes_f, start=1):\n",
    "            line_items = line.strip().split()\n",
    "            file_id = line_items[0].split('.')[0]\n",
    "            seg_id = line_items[0]\n",
    "            start_t, end_t = map(int, line_items[1:3])\n",
    "            es_w = find_align_words_for_node(align_dict[file_id][seg_id]['es'], start_t, end_t)\n",
    "            if len(align_dict[file_id][seg_id]['es_cnt']) > 0:\n",
    "                es_cnt_w = find_align_words_for_node(align_dict[file_id][seg_id]['es_cnt'], start_t, end_t)\n",
    "            else:\n",
    "                es_cnt_w = tuple()\n",
    "                total_errors += 1\n",
    "            nodes_dict[nid] = Node(file_id, seg_id, start_t, end_t, es_w, es_cnt_w)\n",
    "            if nid % 100000 == 0:\n",
    "                print('reading node %d' % nid)\n",
    "    print(\"finished reading %d nodes\" % nid)\n",
    "    print('No content words found for %d nodes' % total_errors)\n",
    "    return nodes_dict\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished reading 29694 nodes\n",
      "No content words found for 156 nodes\n"
     ]
    }
   ],
   "source": [
    "nodes_dict = map_nodes_align(seg_nodes_fname, segment_map, align_dict)\n",
    "pickle.dump(nodes_dict, open(nodes_dict_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edges - create a valid edges file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_edges():\n",
    "    olap_dict = {}\n",
    "    pairs_list = []\n",
    "    # process clusters file\n",
    "    with open(config['es']['edges_olap_fname'], \"r\") as in_f:\n",
    "        for i, line in enumerate(in_f):\n",
    "            line_items = map(int, line.strip().split())\n",
    "            olap_dict[line_items[0]] = line_items[0]\n",
    "            if len(line_items) > 1:\n",
    "                for j in line_items[1:]:\n",
    "                    olap_dict[j] = line_items[0]\n",
    "\n",
    "    # Read edges dict\n",
    "    with open(config['es']['edges_utd_fname'], \"r\") as in_f:\n",
    "        for i, line in enumerate(in_f):\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Processing line: %d\" % (i+1))\n",
    "            line_items = line.strip().split()\n",
    "            node_1 = int(line_items[0])\n",
    "            node_2 = int(line_items[1])\n",
    "            if node_1 not in olap_dict:\n",
    "                olap_dict[node_1] = node_1\n",
    "            if node_2 not in olap_dict:\n",
    "                olap_dict[node_2] = node_2\n",
    "            dtw_val = float(line_items[2]) / 1000.0\n",
    "\n",
    "            node_1 = olap_dict[node_1]\n",
    "            node_2 = olap_dict[node_2]\n",
    "\n",
    "            # Add to pairs list as a tuple\n",
    "            pairs_list.append((min(node_1, node_2), max(node_1, node_2), dtw_val))\n",
    "\n",
    "\n",
    "    print(\"Finished - reading edges ...\")\n",
    "    print(\"Removing duplicates in pairs list\")\n",
    "    set_pairs = list(set(pairs_list))\n",
    "    print(\"Set length: %d and List length: %d\" %(len(set_pairs), len(pairs_list)))\n",
    "    pairs_list = sorted(list(set_pairs))\n",
    "    with open(config['es']['edges_score_fname'], \"w\") as out_f:\n",
    "        for (n1, n2, dtw) in set_pairs:\n",
    "            out_line = \"%d\\t%d\\t%.3f\\n\" % (n1, n2, dtw)\n",
    "            out_f.write(out_line)\n",
    "    pickle.dump(set_pairs, open(config['es']['score_pairs_fname'], \"wb\"))\n",
    "    print(\"finished writing edges\")\n",
    "    \n",
    "    # validity check for duplicates\n",
    "    set_nodes_only = [(n1,n2) for n1, n2, dtw in set_pairs]\n",
    "    if len(set_pairs) != len(set_nodes_only):\n",
    "        raise IOError\n",
    "    return pairs_list   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing line: 1\n",
      "Processing line: 1001\n",
      "Processing line: 2001\n",
      "Processing line: 3001\n",
      "Processing line: 4001\n",
      "Processing line: 5001\n",
      "Processing line: 6001\n",
      "Processing line: 7001\n",
      "Processing line: 8001\n",
      "Processing line: 9001\n",
      "Processing line: 10001\n",
      "Processing line: 11001\n",
      "Processing line: 12001\n",
      "Processing line: 13001\n",
      "Processing line: 14001\n",
      "Finished - reading edges ...\n",
      "Removing duplicates in pairs list\n",
      "Set length: 13370 and List length: 14847\n",
      "finished writing edges\n"
     ]
    }
   ],
   "source": [
    "valid_pairs = read_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Clusters\n",
    "\n",
    "- Save list of clusters\n",
    "- Generate bag of cluster ids for each segment\n",
    "    - use nodes per segment as replace with cluster id\n",
    "    - if no node found, use cluster id -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17394\n"
     ]
    }
   ],
   "source": [
    "segids = []\n",
    "for fid in segment_map:\n",
    "    segids.extend(segment_map[fid].keys())\n",
    "print(len(segids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_clusters(clusters_fname):\n",
    "    clusters = []\n",
    "    with open(clusters_fname, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            nodes = map(int, line.strip().split())\n",
    "            clusters.append(nodes)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = load_clusters(clusters_utd_fname)\n",
    "pickle.dump(clusters, open(clusters_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_pseudowords_for_segments(segment_map, nodes_dict, clusters, feats_fname, feats_dict_fname):\n",
    "    feats_dict = {}\n",
    "    total_errors = 0\n",
    "    display_den = len(clusters) // 10\n",
    "    \n",
    "    for clid, nodes in enumerate(clusters, start=1):\n",
    "        if clid % display_den == 0:\n",
    "            print('processing cluster %d out of %d' % (clid, len(clusters)))\n",
    "        for nid in nodes:\n",
    "            node = nodes_dict[nid]\n",
    "            if node.seg not in feats_dict:\n",
    "                feats_dict[node.seg] = []\n",
    "            feats_dict[node.seg].append(clid)\n",
    "    \n",
    "    pickle.dump(feats_dict, open(feats_dict_fname, \"wb\"))\n",
    "    print(\"Generation complete, now writing to file: %s\" % os.path.basename(feats_dict_fname))\n",
    "    \n",
    "    # Get complete list of segment ids\n",
    "    segids = []\n",
    "    for fid in segment_map:\n",
    "        segids.extend(segment_map[fid].keys())\n",
    "    \n",
    "    with open(feats_fname, \"w\") as out_f:\n",
    "        for seg_id in sorted(segids):\n",
    "            if seg_id not in feats_dict:\n",
    "                outline = \"-1\\n\"\n",
    "                total_errors += 1\n",
    "            else:\n",
    "                outline = \" \".join(map(str,sorted(feats_dict[seg_id])))\n",
    "                outline = outline.strip() + \"\\n\"\n",
    "            out_f.write(outline)\n",
    "    print(\"Finished writing features file: %s\" % os.path.basename(feats_fname))\n",
    "    print(\"Psuedowords not found for: %d segments, out of total: %d segments\" % (total_errors, len(segids)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing cluster 914 out of 9141\n",
      "processing cluster 1828 out of 9141\n",
      "processing cluster 2742 out of 9141\n",
      "processing cluster 3656 out of 9141\n",
      "processing cluster 4570 out of 9141\n",
      "processing cluster 5484 out of 9141\n",
      "processing cluster 6398 out of 9141\n",
      "processing cluster 7312 out of 9141\n",
      "processing cluster 8226 out of 9141\n",
      "processing cluster 9140 out of 9141\n",
      "Generation complete, now writing to file: pseudowords.dict\n",
      "Finished writing features file: pseudowords.feats\n",
      "Psuedowords not found for: 10273 segments, out of total: 17394 segments\n"
     ]
    }
   ],
   "source": [
    "generate_pseudowords_for_segments(segment_map, nodes_dict, clusters, feats_fname, feats_dict_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
