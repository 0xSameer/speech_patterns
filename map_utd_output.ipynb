{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import cPickle as pickle\n",
    "import json\n",
    "import subprocess\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "import bisect\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map ZRTools output to transcripts\n",
    "\n",
    "- Create modified .nodes file\n",
    "- Create mapping between es words, and nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"config.json\") as json_data_file:\n",
    "    config = json.load(json_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodes_fname = config[\"es\"]['nodes_fname']\n",
    "seg_nodes_fname = config[\"es\"]['seg_nodes_fname']\n",
    "nodes_dict_fname = config[\"es\"]['nodes_dict_fname']\n",
    "\n",
    "edges_utd_fname = config[\"es\"]['edges_utd_fname']\n",
    "edges_olap_fname = config[\"es\"]['edges_olap_fname']\n",
    "edges_all_fname = config[\"es\"]['edges_all_fname']\n",
    "edges_score_fname = config[\"es\"]['edges_score_fname']\n",
    "\n",
    "clusters_utd_fname = config['es']['clusters_utd_fname']\n",
    "clusters_fname = config['es']['clusters_fname']\n",
    "clusters_stats_fname = config['es']['clusters_stats_fname']\n",
    "\n",
    "pairs_fname = config['es']['score_pairs_fname']\n",
    "eval_fname = config['es']['eval_pairs_fname']\n",
    "\n",
    "feats_fname = config['es']['feats_fname']\n",
    "feats_dict_fname = config['es']['feats_dict_fname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Align = namedtuple('Align', ['word', 'start', 'end'])\n",
    "Node = namedtuple('Node', ['file', 'seg', 'start', 'end', 'es', 'es_cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segment_map = pickle.load(open(config['es']['segment_dict_fname'], \"rb\"))\n",
    "align_dict = pickle.load(open(config['es']['align_dict_fname'], \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes - identify the segment to which the node belongs\n",
    "\n",
    "Lookout for:\n",
    "1. Patterns that go across segment boundaries\n",
    "2. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_segid(node_start, node_end, file_id, segment_map):\n",
    "    seg_id_list, start_time_list = zip(*sorted(segment_map[file_id].items(), key=lambda t:t[0]))\n",
    "\n",
    "    # Binary search to find segment where the node starts and ends in\n",
    "    # we subtract 1 as bisect returns the index where we can insert a value keeping\n",
    "    # the sort order. We do not expect it to be 0, as the node will always have a 0 or positive start\n",
    "    # time\n",
    "    seg_id_start = bisect.bisect(start_time_list, node_start)-1\n",
    "    s1 = seg_id_list[seg_id_start]\n",
    "    seg_id_end = bisect.bisect(start_time_list, node_end)-1\n",
    "    s2 = seg_id_list[seg_id_end]\n",
    "    \n",
    "    if seg_id_start == seg_id_end:\n",
    "        start = node_start - segment_map[file_id][s1]\n",
    "        end = node_end - segment_map[file_id][s1]\n",
    "        return s1, start, end, 0\n",
    "    else:\n",
    "        # Calculate which segment overlaps more\n",
    "        #print (file_id, node_start, node_end, seg_id, seg_id_start, seg_id_end, seg_id_list[seg_id_start-1], seg_id_list[seg_id_end-1])\n",
    "        if (segment_map[file_id][s2] - node_start) >= (node_end - segment_map[file_id][s2]):\n",
    "            shift_value = node_end - segment_map[file_id][s2]\n",
    "            start = node_start - segment_map[file_id][s1] - shift_value\n",
    "            end = segment_map[file_id][s2] - segment_map[file_id][s1]\n",
    "            print(\"More in s1\", start, end, shift_value)\n",
    "            return s1, start, end, 1\n",
    "        else:\n",
    "            shift_value = segment_map[file_id][s2] - node_start\n",
    "            start = 0\n",
    "            end = node_end - segment_map[file_id][s2] + shift_value\n",
    "            print(\"More in s2\", start, end, shift_value)\n",
    "            return s2, start, end, 1\n",
    "    print (file_id, node_start, node_end, seg_id_start, seg_id_end)\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More in s1 429 561 8\n",
      "('042.079', 429, 561, 1)\n",
      "('038.001', 0, 51, 0)\n"
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "print(search_segid(20509, 20641, \"042\", segment_map))\n",
    "print(search_segid(0, 51, \"038\", segment_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes - create a master_graph.segnodes file replacing nodes with their segment ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_segmented_nodes(nodes_fname, segment_map, seg_nodes_fname):\n",
    "    total_errors = 0\n",
    "    with open(nodes_fname, \"r\") as nodes_f, open(seg_nodes_fname, \"w\") as segnodes_f:\n",
    "        for i, line in enumerate(nodes_f):\n",
    "            line_items = line.strip().split(None, 3)\n",
    "            file_id = line_items[0]\n",
    "            node_start, node_end = map(int, line_items[1:3])\n",
    "            try:\n",
    "                seg_id, seg_node_start, seg_node_end, e = search_segid(node_start, node_end, file_id, segment_map)\n",
    "                total_errors += e\n",
    "                outline = \"%s\\t%d\\t%d\\t%s\\n\" % (seg_id, seg_node_start, seg_node_end, line_items[3])\n",
    "                segnodes_f.write(outline)\n",
    "            except ValueError:\n",
    "                print(\"Incorrect line format at line: %d\\n%s\" % (i, line))\n",
    "                \n",
    "    print(\"Total nodes: %d\" % (i+1))\n",
    "    print(\"Total errors: %d\" % total_errors)\n",
    "    print(\"completed\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More in s2 0 52 4\n",
      "More in s1 0 56 0\n",
      "More in s2 0 47 1\n",
      "More in s1 43 91 2\n",
      "More in s1 801 851 3\n",
      "More in s2 0 121 9\n",
      "More in s2 0 51 6\n",
      "More in s1 227 276 24\n",
      "More in s2 0 53 4\n",
      "More in s1 454 502 2\n",
      "More in s1 8 54 0\n",
      "More in s2 0 48 3\n",
      "More in s2 0 59 2\n",
      "More in s1 74 125 2\n",
      "More in s1 108 155 4\n",
      "More in s2 0 48 1\n",
      "More in s2 0 46 16\n",
      "More in s2 0 46 4\n",
      "More in s2 0 49 9\n",
      "More in s1 9 58 0\n",
      "More in s1 107 154 12\n",
      "More in s1 221 273 5\n",
      "More in s2 0 55 3\n",
      "More in s2 0 62 3\n",
      "More in s2 0 59 3\n",
      "More in s2 0 61 4\n",
      "More in s2 0 46 10\n",
      "More in s1 312 369 0\n",
      "More in s2 0 50 18\n",
      "More in s1 5 54 21\n",
      "More in s1 113 159 14\n",
      "More in s2 0 53 1\n",
      "More in s2 0 56 6\n",
      "More in s1 86 134 2\n",
      "More in s2 0 49 4\n",
      "More in s1 207 257 1\n",
      "More in s2 0 47 8\n",
      "More in s2 0 63 1\n",
      "More in s2 0 50 4\n",
      "More in s1 254 321 15\n",
      "More in s2 0 53 10\n",
      "More in s1 265 321 19\n",
      "More in s1 264 321 9\n",
      "More in s1 231 292 0\n",
      "More in s2 0 55 9\n",
      "More in s1 264 321 1\n",
      "More in s1 272 321 4\n",
      "More in s2 0 53 2\n",
      "More in s2 0 50 1\n",
      "More in s1 269 321 10\n",
      "More in s1 260 321 4\n",
      "More in s2 0 46 1\n",
      "More in s1 253 321 12\n",
      "More in s2 0 46 11\n",
      "More in s1 73 126 15\n",
      "More in s2 0 56 6\n",
      "More in s2 0 49 12\n",
      "More in s2 0 57 1\n",
      "More in s2 0 50 13\n",
      "More in s2 0 55 1\n",
      "More in s1 55 102 7\n",
      "More in s2 0 61 6\n",
      "More in s1 257 307 11\n",
      "More in s2 0 47 10\n",
      "More in s1 109 156 18\n",
      "More in s1 491 537 3\n",
      "More in s2 0 47 9\n",
      "More in s1 373 420 2\n",
      "More in s1 256 304 2\n",
      "More in s2 0 52 1\n",
      "More in s2 0 58 2\n",
      "More in s2 0 49 10\n",
      "More in s2 0 51 23\n",
      "More in s2 0 52 1\n",
      "More in s1 8 54 0\n",
      "More in s1 5 54 0\n",
      "More in s1 153 211 2\n",
      "More in s2 0 59 1\n",
      "More in s1 21 69 3\n",
      "More in s2 0 49 7\n",
      "More in s2 0 65 4\n",
      "More in s2 0 47 14\n",
      "More in s2 0 50 1\n",
      "More in s2 0 51 18\n",
      "More in s2 0 50 3\n",
      "More in s2 0 48 19\n",
      "More in s2 0 50 5\n",
      "More in s1 19 65 5\n",
      "More in s2 0 65 3\n",
      "More in s1 262 355 0\n",
      "More in s2 0 82 2\n",
      "More in s2 0 106 5\n",
      "More in s1 468 519 2\n",
      "More in s1 449 609 9\n",
      "More in s1 601 751 4\n",
      "More in s2 0 59 4\n",
      "More in s1 375 451 5\n",
      "More in s1 824 871 0\n",
      "More in s1 117 163 17\n",
      "More in s2 0 47 2\n",
      "More in s2 0 47 18\n",
      "More in s2 0 47 4\n",
      "More in s2 0 48 13\n",
      "More in s1 86 134 2\n",
      "More in s1 392 448 0\n",
      "More in s1 142 196 16\n",
      "More in s1 -4 43 18\n",
      "More in s1 824 871 0\n",
      "More in s2 0 50 1\n",
      "More in s1 459 505 0\n",
      "More in s2 0 50 5\n",
      "More in s1 21 69 3\n",
      "More in s1 386 435 0\n",
      "More in s1 55 102 7\n",
      "More in s2 0 46 17\n",
      "More in s2 0 48 1\n",
      "More in s2 0 53 8\n",
      "More in s1 45 91 16\n",
      "More in s1 466 517 1\n",
      "More in s1 249 303 2\n",
      "More in s2 0 47 18\n",
      "More in s2 0 46 1\n",
      "More in s1 271 323 2\n",
      "More in s2 0 47 3\n",
      "More in s1 26 75 5\n",
      "More in s2 0 47 4\n",
      "More in s1 73 124 1\n",
      "More in s2 0 51 1\n",
      "More in s2 0 49 22\n",
      "More in s2 0 52 1\n",
      "More in s2 0 49 7\n",
      "More in s1 254 303 2\n",
      "More in s1 245 296 1\n",
      "More in s1 250 303 1\n",
      "More in s2 0 48 1\n",
      "More in s2 0 48 3\n",
      "More in s2 0 46 3\n",
      "More in s2 0 47 10\n",
      "More in s1 110 159 15\n",
      "More in s2 0 46 9\n",
      "More in s1 187 237 0\n",
      "More in s2 0 46 17\n",
      "More in s1 91 158 5\n",
      "More in s2 0 48 4\n",
      "More in s1 252 321 18\n",
      "More in s2 0 50 4\n",
      "More in s2 0 63 1\n",
      "More in s2 0 47 8\n",
      "More in s1 265 321 19\n",
      "More in s2 0 58 1\n",
      "More in s1 94 158 2\n",
      "More in s2 0 50 1\n",
      "More in s2 0 47 1\n",
      "More in s2 0 51 23\n",
      "More in s2 0 48 1\n",
      "More in s1 257 307 11\n",
      "More in s2 0 51 13\n",
      "More in s2 0 51 3\n",
      "More in s2 0 46 3\n",
      "More in s2 0 46 3\n",
      "More in s1 -13 36 20\n",
      "More in s2 0 57 1\n",
      "More in s2 0 51 13\n",
      "More in s2 0 51 3\n",
      "More in s1 58 109 0\n",
      "More in s1 106 155 2\n",
      "More in s1 238 292 12\n",
      "More in s2 0 53 11\n",
      "More in s2 0 49 2\n",
      "More in s2 0 47 14\n",
      "More in s1 236 282 9\n",
      "More in s1 386 435 0\n",
      "More in s2 0 56 1\n",
      "More in s2 0 50 1\n",
      "More in s2 0 57 2\n",
      "More in s2 0 50 2\n",
      "More in s2 0 50 1\n",
      "More in s2 0 49 12\n",
      "More in s2 0 49 20\n",
      "More in s2 0 48 17\n",
      "More in s2 0 58 2\n",
      "More in s2 0 48 1\n",
      "More in s2 0 49 1\n",
      "More in s1 26 73 0\n",
      "More in s2 0 52 1\n",
      "More in s2 0 50 1\n",
      "More in s2 0 46 1\n",
      "More in s1 7 59 2\n",
      "More in s1 16 73 1\n",
      "More in s1 -3 56 6\n",
      "More in s1 -5 52 5\n",
      "More in s2 0 46 17\n",
      "More in s2 0 48 4\n",
      "More in s1 242 295 5\n",
      "More in s1 66 126 1\n",
      "Total nodes: 104844\n",
      "Total errors: 195\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "create_segmented_nodes(nodes_fname, segment_map, seg_nodes_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes - find transcript words corresponding to node start and end times\n",
    "\n",
    "Create node dictionary:\n",
    "    - node id\n",
    "    - file id\n",
    "    - seg id\n",
    "    - start time\n",
    "    - end time\n",
    "    - es words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_align_words_for_node(align_words_list, start, end):\n",
    "    #display(align_words_list, start, end)\n",
    "    words, start_times, end_times = zip(*(align_words_list))\n",
    "    start_i = bisect.bisect(end_times, start)\n",
    "    # end index will be 1 beyond the actual end\n",
    "    end_i = bisect.bisect(start_times, end)\n",
    "    return words[start_i:end_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('VAMOS', 'A', 'VER')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('ESTA', 'MECHITA')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(find_align_words_for_node(align_dict[\"001\"][\"001.224\"][\"es\"], 191, 246))\n",
    "display(find_align_words_for_node(align_dict[\"001\"][\"001.274\"][\"es\"], 45, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_nodes_align(seg_nodes_fname, segment_map, align_dict):\n",
    "    total_errors = 0\n",
    "    nodes_dict = {}\n",
    "    with open(seg_nodes_fname, \"r\") as seg_nodes_f:\n",
    "        for nid, line in enumerate(seg_nodes_f, start=1):\n",
    "            line_items = line.strip().split()\n",
    "            file_id = line_items[0].split('.')[0]\n",
    "            seg_id = line_items[0]\n",
    "            start_t, end_t = map(int, line_items[1:3])\n",
    "            es_w = find_align_words_for_node(align_dict[file_id][seg_id]['es'], start_t, end_t)\n",
    "            if len(align_dict[file_id][seg_id]['es_cnt']) > 0:\n",
    "                es_cnt_w = find_align_words_for_node(align_dict[file_id][seg_id]['es_cnt'], start_t, end_t)\n",
    "            else:\n",
    "                es_cnt_w = tuple()\n",
    "                total_errors += 1\n",
    "            nodes_dict[nid] = Node(file_id, seg_id, start_t, end_t, es_w, es_cnt_w)\n",
    "            if nid % 100000 == 0:\n",
    "                print('reading node %d' % nid)\n",
    "    print(\"finished reading %d nodes\" % nid)\n",
    "    print('No content words found for %d nodes' % total_errors)\n",
    "    return nodes_dict\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading node 100000\n",
      "finished reading 104844 nodes\n",
      "No content words found for 298 nodes\n"
     ]
    }
   ],
   "source": [
    "nodes_dict = map_nodes_align(seg_nodes_fname, segment_map, align_dict)\n",
    "pickle.dump(nodes_dict, open(nodes_dict_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edges - create a valid edges file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_edges():\n",
    "    olap_dict = {}\n",
    "    pairs_list = []\n",
    "    # process clusters file\n",
    "    with open(config['es']['edges_olap_fname'], \"r\") as in_f:\n",
    "        for i, line in enumerate(in_f):\n",
    "            line_items = map(int, line.strip().split())\n",
    "            olap_dict[line_items[0]] = line_items[0]\n",
    "            if len(line_items) > 1:\n",
    "                for j in line_items[1:]:\n",
    "                    olap_dict[j] = line_items[0]\n",
    "\n",
    "    # Read edges dict\n",
    "    with open(config['es']['edges_utd_fname'], \"r\") as in_f:\n",
    "        for i, line in enumerate(in_f):\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Processing line: %d\" % (i+1))\n",
    "            line_items = line.strip().split()\n",
    "            node_1 = int(line_items[0])\n",
    "            node_2 = int(line_items[1])\n",
    "            if node_1 not in olap_dict:\n",
    "                olap_dict[node_1] = node_1\n",
    "            if node_2 not in olap_dict:\n",
    "                olap_dict[node_2] = node_2\n",
    "            dtw_val = float(line_items[2]) / 1000.0\n",
    "\n",
    "            node_1 = olap_dict[node_1]\n",
    "            node_2 = olap_dict[node_2]\n",
    "\n",
    "            # Add to pairs list as a tuple\n",
    "            pairs_list.append((min(node_1, node_2), max(node_1, node_2), dtw_val))\n",
    "\n",
    "\n",
    "    print(\"Finished - reading edges ...\")\n",
    "    print(\"Removing duplicates in pairs list\")\n",
    "    set_pairs = list(set(pairs_list))\n",
    "    print(\"Set length: %d and List length: %d\" %(len(set_pairs), len(pairs_list)))\n",
    "    pairs_list = sorted(list(set_pairs))\n",
    "    with open(config['es']['edges_score_fname'], \"w\") as out_f:\n",
    "        for (n1, n2, dtw) in set_pairs:\n",
    "            out_line = \"%d\\t%d\\t%.3f\\n\" % (n1, n2, dtw)\n",
    "            out_f.write(out_line)\n",
    "    pickle.dump(set_pairs, open(config['es']['score_pairs_fname'], \"wb\"))\n",
    "    print(\"finished writing edges\")\n",
    "    \n",
    "    # validity check for duplicates\n",
    "    set_nodes_only = [(n1,n2) for n1, n2, dtw in set_pairs]\n",
    "    if len(set_pairs) != len(set_nodes_only):\n",
    "        raise IOError\n",
    "    return pairs_list   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing line: 1\n",
      "Processing line: 1001\n",
      "Processing line: 2001\n",
      "Processing line: 3001\n",
      "Processing line: 4001\n",
      "Processing line: 5001\n",
      "Processing line: 6001\n",
      "Processing line: 7001\n",
      "Processing line: 8001\n",
      "Processing line: 9001\n",
      "Processing line: 10001\n",
      "Processing line: 11001\n",
      "Processing line: 12001\n",
      "Processing line: 13001\n",
      "Processing line: 14001\n",
      "Processing line: 15001\n",
      "Processing line: 16001\n",
      "Processing line: 17001\n",
      "Processing line: 18001\n",
      "Processing line: 19001\n",
      "Processing line: 20001\n",
      "Processing line: 21001\n",
      "Processing line: 22001\n",
      "Processing line: 23001\n",
      "Processing line: 24001\n",
      "Processing line: 25001\n",
      "Processing line: 26001\n",
      "Processing line: 27001\n",
      "Processing line: 28001\n",
      "Processing line: 29001\n",
      "Processing line: 30001\n",
      "Processing line: 31001\n",
      "Processing line: 32001\n",
      "Processing line: 33001\n",
      "Processing line: 34001\n",
      "Processing line: 35001\n",
      "Processing line: 36001\n",
      "Processing line: 37001\n",
      "Processing line: 38001\n",
      "Processing line: 39001\n",
      "Processing line: 40001\n",
      "Processing line: 41001\n",
      "Processing line: 42001\n",
      "Processing line: 43001\n",
      "Processing line: 44001\n",
      "Processing line: 45001\n",
      "Processing line: 46001\n",
      "Processing line: 47001\n",
      "Processing line: 48001\n",
      "Processing line: 49001\n",
      "Processing line: 50001\n",
      "Processing line: 51001\n",
      "Processing line: 52001\n",
      "Finished - reading edges ...\n",
      "Removing duplicates in pairs list\n",
      "Set length: 47364 and List length: 52422\n",
      "finished writing edges\n"
     ]
    }
   ],
   "source": [
    "valid_pairs = read_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Clusters\n",
    "\n",
    "- Save list of clusters\n",
    "- Generate bag of cluster ids for each segment\n",
    "    - use nodes per segment as replace with cluster id\n",
    "    - if no node found, use cluster id -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17394\n"
     ]
    }
   ],
   "source": [
    "segids = []\n",
    "for fid in segment_map:\n",
    "    segids.extend(segment_map[fid].keys())\n",
    "print(len(segids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_clusters(clusters_utd_fname):\n",
    "    clusters = []\n",
    "    with open(clusters_utd_fname, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            try:\n",
    "                nodes = map(int, line.strip().split())\n",
    "                clusters.append(nodes)\n",
    "            except:\n",
    "                print(line)                    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = load_clusters(clusters_utd_fname)\n",
    "pickle.dump(clusters, open(clusters_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_pseudowords_for_segments(segment_map, nodes_dict, clusters, feats_fname, feats_dict_fname):\n",
    "    feats_dict = {}\n",
    "    total_errors = 0\n",
    "    display_den = len(clusters) // 10\n",
    "    \n",
    "    for clid, nodes in enumerate(clusters, start=1):\n",
    "        if clid % display_den == 0:\n",
    "            print('processing cluster %d out of %d' % (clid, len(clusters)))\n",
    "        for nid in nodes:\n",
    "            node = nodes_dict[nid]\n",
    "            if node.seg not in feats_dict:\n",
    "                feats_dict[node.seg] = []\n",
    "            feats_dict[node.seg].append(clid)\n",
    "    \n",
    "    # Get complete list of segment ids\n",
    "    segids = []\n",
    "    for fid in segment_map:\n",
    "        segids.extend(segment_map[fid].keys())\n",
    "    \n",
    "    with open(feats_fname, \"w\") as out_f:\n",
    "        for seg_id in sorted(segids):\n",
    "            # adding -1 for missing pseudotext\n",
    "            if seg_id not in feats_dict:\n",
    "                #outline = \"-1\\n\"\n",
    "                total_errors += 1\n",
    "                feats_dict[seg_id] = [-1]\n",
    "            \n",
    "            outline = \" \".join(map(str,sorted(feats_dict[seg_id])))\n",
    "            outline = outline.strip() + \"\\n\"\n",
    "            out_f.write(outline)\n",
    "            \n",
    "    print(\"Finished writing features file: %s\" % os.path.basename(feats_fname))\n",
    "    print(\"Writing to file: %s\" % os.path.basename(feats_dict_fname))\n",
    "    pickle.dump(feats_dict, open(feats_dict_fname, \"wb\"))\n",
    "    print(\"Psuedowords not found for: %d segments, out of total: %d segments\" % (total_errors, len(segids)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing cluster 2281 out of 22810\n",
      "processing cluster 4562 out of 22810\n",
      "processing cluster 6843 out of 22810\n",
      "processing cluster 9124 out of 22810\n",
      "processing cluster 11405 out of 22810\n",
      "processing cluster 13686 out of 22810\n",
      "processing cluster 15967 out of 22810\n",
      "processing cluster 18248 out of 22810\n",
      "processing cluster 20529 out of 22810\n",
      "processing cluster 22810 out of 22810\n",
      "Finished writing features file: pseudowords.feats\n",
      "Writing to file: pseudowords.dict\n",
      "Psuedowords not found for: 7034 segments, out of total: 17394 segments\n"
     ]
    }
   ],
   "source": [
    "generate_pseudowords_for_segments(segment_map, nodes_dict, clusters, feats_fname, feats_dict_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
