{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import cPickle as pickle\n",
    "import json\n",
    "import subprocess\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "import bisect\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map ZRTools output to transcripts\n",
    "\n",
    "- Create modified .nodes file\n",
    "- Create mapping between es words, and nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"config.json\") as json_data_file:\n",
    "    config = json.load(json_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodes_fname = config[\"es\"]['nodes_fname']\n",
    "seg_nodes_fname = config[\"es\"]['seg_nodes_fname']\n",
    "nodes_dict_fname = config[\"es\"]['nodes_dict_fname']\n",
    "\n",
    "edges_utd_fname = config[\"es\"]['edges_utd_fname']\n",
    "edges_olap_fname = config[\"es\"]['edges_olap_fname']\n",
    "edges_all_fname = config[\"es\"]['edges_all_fname']\n",
    "edges_score_fname = config[\"es\"]['edges_score_fname']\n",
    "\n",
    "clusters_utd_fname = config['es']['clusters_utd_fname']\n",
    "clusters_fname = config['es']['clusters_fname']\n",
    "clusters_stats_fname = config['es']['clusters_stats_fname']\n",
    "\n",
    "pairs_fname = config['es']['score_pairs_fname']\n",
    "eval_fname = config['es']['eval_pairs_fname']\n",
    "\n",
    "feats_fname = config['es']['feats_fname']\n",
    "feats_dict_fname = config['es']['feats_dict_fname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Align = namedtuple('Align', ['word', 'start', 'end'])\n",
    "Node = namedtuple('Node', ['file', 'seg', 'start', 'end', 'es', 'es_cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segment_map = pickle.load(open(config['es']['segment_dict_fname'], \"rb\"))\n",
    "align_dict = pickle.load(open(config['es']['align_dict_fname'], \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes - identify the segment to which the node belongs\n",
    "\n",
    "Lookout for:\n",
    "1. Patterns that go across segment boundaries\n",
    "2. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def search_segid(node_start, node_end, file_id, segment_map):\n",
    "    seg_id_list, start_time_list = zip(*sorted(segment_map[file_id].items(), key=lambda t:t[0]))\n",
    "\n",
    "    # Binary search to find segment where the node starts and ends in\n",
    "    # we subtract 1 as bisect returns the index where we can insert a value keeping\n",
    "    # the sort order. We do not expect it to be 0, as the node will always have a 0 or positive start\n",
    "    # time\n",
    "    seg_id_start = bisect.bisect(start_time_list, node_start)-1\n",
    "    s1 = seg_id_list[seg_id_start]\n",
    "    seg_id_end = bisect.bisect(start_time_list, node_end)-1\n",
    "    s2 = seg_id_list[seg_id_end]\n",
    "    \n",
    "    if seg_id_start == seg_id_end:\n",
    "        start = node_start - segment_map[file_id][s1]\n",
    "        end = node_end - segment_map[file_id][s1]\n",
    "        return s1, start, end, 0\n",
    "    else:\n",
    "        # Calculate which segment overlaps more\n",
    "        #print (file_id, node_start, node_end, seg_id, seg_id_start, seg_id_end, seg_id_list[seg_id_start-1], seg_id_list[seg_id_end-1])\n",
    "        if (segment_map[file_id][s2] - node_start) >= (node_end - segment_map[file_id][s2]):\n",
    "            shift_value = node_end - segment_map[file_id][s2]\n",
    "            start = node_start - segment_map[file_id][s1] - shift_value\n",
    "            end = segment_map[file_id][s2] - segment_map[file_id][s1]\n",
    "            #print(\"More in s1\", start, end, shift_value)\n",
    "            return s1, start, end, 1\n",
    "        else:\n",
    "            shift_value = segment_map[file_id][s2] - node_start\n",
    "            start = 0\n",
    "            end = node_end - segment_map[file_id][s2] + shift_value\n",
    "            #print(\"More in s2\", start, end, shift_value)\n",
    "            return s2, start, end, 1\n",
    "    print (file_id, node_start, node_end, seg_id_start, seg_id_end)\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('042.079', 429, 561, 1)\n",
      "('038.001', 0, 51, 0)\n"
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "print(search_segid(20509, 20641, \"042\", segment_map))\n",
    "print(search_segid(0, 51, \"038\", segment_map))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes - create a master_graph.segnodes file replacing nodes with their segment ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_segmented_nodes(nodes_fname, segment_map, seg_nodes_fname):\n",
    "    total_errors = 0\n",
    "    with open(nodes_fname, \"r\") as nodes_f, open(seg_nodes_fname, \"w\") as segnodes_f:\n",
    "        for i, line in enumerate(nodes_f):\n",
    "            line_items = line.strip().split(None, 3)\n",
    "            file_id = line_items[0]\n",
    "            node_start, node_end = map(int, line_items[1:3])\n",
    "            try:\n",
    "                seg_id, seg_node_start, seg_node_end, e = search_segid(node_start, node_end, file_id, segment_map)\n",
    "                total_errors += e\n",
    "                outline = \"%s\\t%d\\t%d\\t%s\\n\" % (seg_id, seg_node_start, seg_node_end, line_items[3])\n",
    "                segnodes_f.write(outline)\n",
    "            except ValueError:\n",
    "                print(\"Incorrect line format at line: %d\\n%s\" % (i, line))\n",
    "                \n",
    "    print(\"Total nodes: %d\" % (i+1))\n",
    "    print(\"Total errors: %d\" % total_errors)\n",
    "    print(\"completed\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes: 566094\n",
      "Total errors: 6384\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "create_segmented_nodes(nodes_fname, segment_map, seg_nodes_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes - find transcript words corresponding to node start and end times\n",
    "\n",
    "Create node dictionary:\n",
    "    - node id\n",
    "    - file id\n",
    "    - seg id\n",
    "    - start time\n",
    "    - end time\n",
    "    - es words    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_align_words_for_node(align_words_list, start, end):\n",
    "    #display(align_words_list, start, end)\n",
    "    words, start_times, end_times = zip(*(align_words_list))\n",
    "    start_i = bisect.bisect(end_times, start)\n",
    "    # end index will be 1 beyond the actual end\n",
    "    end_i = bisect.bisect(start_times, end)\n",
    "    return words[start_i:end_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('VAMOS', 'A', 'VER')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('ESTA', 'MECHITA')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(find_align_words_for_node(align_dict[\"001\"][\"001.224\"][\"es\"], 191, 246))\n",
    "display(find_align_words_for_node(align_dict[\"001\"][\"001.274\"][\"es\"], 45, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_nodes_align(seg_nodes_fname, segment_map, align_dict):\n",
    "    total_errors = 0\n",
    "    nodes_dict = {}\n",
    "    with open(seg_nodes_fname, \"r\") as seg_nodes_f:\n",
    "        for nid, line in enumerate(seg_nodes_f, start=1):\n",
    "            line_items = line.strip().split()\n",
    "            file_id = line_items[0].split('.')[0]\n",
    "            seg_id = line_items[0]\n",
    "            start_t, end_t = map(int, line_items[1:3])\n",
    "            if seg_id in align_dict[file_id]:\n",
    "                es_w = find_align_words_for_node(align_dict[file_id][seg_id]['es'], start_t, end_t)\n",
    "                if len(align_dict[file_id][seg_id]['es_cnt']) > 0:\n",
    "                    es_cnt_w = find_align_words_for_node(align_dict[file_id][seg_id]['es_cnt'], start_t, end_t)\n",
    "                else:\n",
    "                    es_cnt_w = tuple()\n",
    "                    total_errors += 1\n",
    "            else:\n",
    "                es_w = tuple()\n",
    "                total_errors += 1\n",
    "            \n",
    "            nodes_dict[nid] = Node(file_id, seg_id, start_t, end_t, es_w, es_cnt_w)\n",
    "            if nid % 100000 == 0:\n",
    "                print('reading node %d' % nid)\n",
    "    print(\"finished reading %d nodes\" % nid)\n",
    "    print('No content words found for %d nodes' % total_errors)\n",
    "    return nodes_dict\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading node 100000\n",
      "reading node 200000\n",
      "reading node 300000\n",
      "reading node 400000\n",
      "reading node 500000\n",
      "finished reading 566094 nodes\n",
      "No content words found for 1224 nodes\n"
     ]
    }
   ],
   "source": [
    "nodes_dict = map_nodes_align(seg_nodes_fname, segment_map, align_dict)\n",
    "pickle.dump(nodes_dict, open(nodes_dict_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edges - create a valid edges file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_edges():\n",
    "    olap_dict = {}\n",
    "    pairs_list = []\n",
    "    # process clusters file\n",
    "    with open(config['es']['edges_olap_fname'], \"r\") as in_f:\n",
    "        for i, line in enumerate(in_f):\n",
    "            line_items = map(int, line.strip().split())\n",
    "            olap_dict[line_items[0]] = line_items[0]\n",
    "            if len(line_items) > 1:\n",
    "                for j in line_items[1:]:\n",
    "                    olap_dict[j] = line_items[0]\n",
    "\n",
    "    # Read edges dict\n",
    "    with open(config['es']['edges_utd_fname'], \"r\") as in_f:\n",
    "        for i, line in enumerate(in_f):\n",
    "            if i % 1000 == 0:\n",
    "                print(\"Processing line: %d\" % (i+1))\n",
    "            line_items = line.strip().split()\n",
    "            node_1 = int(line_items[0])\n",
    "            node_2 = int(line_items[1])\n",
    "            if node_1 not in olap_dict:\n",
    "                olap_dict[node_1] = node_1\n",
    "            if node_2 not in olap_dict:\n",
    "                olap_dict[node_2] = node_2\n",
    "            dtw_val = float(line_items[2]) / 1000.0\n",
    "\n",
    "            node_1 = olap_dict[node_1]\n",
    "            node_2 = olap_dict[node_2]\n",
    "\n",
    "            # Add to pairs list as a tuple\n",
    "            pairs_list.append((min(node_1, node_2), max(node_1, node_2), dtw_val))\n",
    "\n",
    "\n",
    "    print(\"Finished - reading edges ...\")\n",
    "    print(\"Removing duplicates in pairs list\")\n",
    "    set_pairs = list(set(pairs_list))\n",
    "    print(\"Set length: %d and List length: %d\" %(len(set_pairs), len(pairs_list)))\n",
    "    pairs_list = sorted(list(set_pairs))\n",
    "    with open(config['es']['edges_score_fname'], \"w\") as out_f:\n",
    "        for (n1, n2, dtw) in set_pairs:\n",
    "            out_line = \"%d\\t%d\\t%.3f\\n\" % (n1, n2, dtw)\n",
    "            out_f.write(out_line)\n",
    "    pickle.dump(set_pairs, open(config['es']['score_pairs_fname'], \"wb\"))\n",
    "    print(\"finished writing edges\")\n",
    "    \n",
    "    # validity check for duplicates\n",
    "    set_nodes_only = [(n1,n2) for n1, n2, dtw in set_pairs]\n",
    "    if len(set_pairs) != len(set_nodes_only):\n",
    "        raise IOError\n",
    "    return pairs_list   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing line: 1\n",
      "Processing line: 1001\n",
      "Processing line: 2001\n",
      "Processing line: 3001\n",
      "Processing line: 4001\n",
      "Processing line: 5001\n",
      "Processing line: 6001\n",
      "Processing line: 7001\n",
      "Processing line: 8001\n",
      "Processing line: 9001\n",
      "Processing line: 10001\n",
      "Processing line: 11001\n",
      "Processing line: 12001\n",
      "Processing line: 13001\n",
      "Processing line: 14001\n",
      "Processing line: 15001\n",
      "Processing line: 16001\n",
      "Processing line: 17001\n",
      "Processing line: 18001\n",
      "Processing line: 19001\n",
      "Processing line: 20001\n",
      "Processing line: 21001\n",
      "Processing line: 22001\n",
      "Processing line: 23001\n",
      "Processing line: 24001\n",
      "Processing line: 25001\n",
      "Processing line: 26001\n",
      "Processing line: 27001\n",
      "Processing line: 28001\n",
      "Processing line: 29001\n",
      "Processing line: 30001\n",
      "Processing line: 31001\n",
      "Processing line: 32001\n",
      "Processing line: 33001\n",
      "Processing line: 34001\n",
      "Processing line: 35001\n",
      "Processing line: 36001\n",
      "Processing line: 37001\n",
      "Processing line: 38001\n",
      "Processing line: 39001\n",
      "Processing line: 40001\n",
      "Processing line: 41001\n",
      "Processing line: 42001\n",
      "Processing line: 43001\n",
      "Processing line: 44001\n",
      "Processing line: 45001\n",
      "Processing line: 46001\n",
      "Processing line: 47001\n",
      "Processing line: 48001\n",
      "Processing line: 49001\n",
      "Processing line: 50001\n",
      "Processing line: 51001\n",
      "Processing line: 52001\n",
      "Processing line: 53001\n",
      "Processing line: 54001\n",
      "Processing line: 55001\n",
      "Processing line: 56001\n",
      "Processing line: 57001\n",
      "Processing line: 58001\n",
      "Processing line: 59001\n",
      "Processing line: 60001\n",
      "Processing line: 61001\n",
      "Processing line: 62001\n",
      "Processing line: 63001\n",
      "Processing line: 64001\n",
      "Processing line: 65001\n",
      "Processing line: 66001\n",
      "Processing line: 67001\n",
      "Processing line: 68001\n",
      "Processing line: 69001\n",
      "Processing line: 70001\n",
      "Processing line: 71001\n",
      "Processing line: 72001\n",
      "Processing line: 73001\n",
      "Processing line: 74001\n",
      "Processing line: 75001\n",
      "Processing line: 76001\n",
      "Processing line: 77001\n",
      "Processing line: 78001\n",
      "Processing line: 79001\n",
      "Processing line: 80001\n",
      "Processing line: 81001\n",
      "Processing line: 82001\n",
      "Processing line: 83001\n",
      "Processing line: 84001\n",
      "Processing line: 85001\n",
      "Processing line: 86001\n",
      "Processing line: 87001\n",
      "Processing line: 88001\n",
      "Processing line: 89001\n",
      "Processing line: 90001\n",
      "Processing line: 91001\n",
      "Processing line: 92001\n",
      "Processing line: 93001\n",
      "Processing line: 94001\n",
      "Processing line: 95001\n",
      "Processing line: 96001\n",
      "Processing line: 97001\n",
      "Processing line: 98001\n",
      "Processing line: 99001\n",
      "Processing line: 100001\n",
      "Processing line: 101001\n",
      "Processing line: 102001\n",
      "Processing line: 103001\n",
      "Processing line: 104001\n",
      "Processing line: 105001\n",
      "Processing line: 106001\n",
      "Processing line: 107001\n",
      "Processing line: 108001\n",
      "Processing line: 109001\n",
      "Processing line: 110001\n",
      "Processing line: 111001\n",
      "Processing line: 112001\n",
      "Processing line: 113001\n",
      "Processing line: 114001\n",
      "Processing line: 115001\n",
      "Processing line: 116001\n",
      "Processing line: 117001\n",
      "Processing line: 118001\n",
      "Processing line: 119001\n",
      "Processing line: 120001\n",
      "Processing line: 121001\n",
      "Processing line: 122001\n",
      "Processing line: 123001\n",
      "Processing line: 124001\n",
      "Processing line: 125001\n",
      "Processing line: 126001\n",
      "Processing line: 127001\n",
      "Processing line: 128001\n",
      "Processing line: 129001\n",
      "Processing line: 130001\n",
      "Processing line: 131001\n",
      "Processing line: 132001\n",
      "Processing line: 133001\n",
      "Processing line: 134001\n",
      "Processing line: 135001\n",
      "Processing line: 136001\n",
      "Processing line: 137001\n",
      "Processing line: 138001\n",
      "Processing line: 139001\n",
      "Processing line: 140001\n",
      "Processing line: 141001\n",
      "Processing line: 142001\n",
      "Processing line: 143001\n",
      "Processing line: 144001\n",
      "Processing line: 145001\n",
      "Processing line: 146001\n",
      "Processing line: 147001\n",
      "Processing line: 148001\n",
      "Processing line: 149001\n",
      "Processing line: 150001\n",
      "Processing line: 151001\n",
      "Processing line: 152001\n",
      "Processing line: 153001\n",
      "Processing line: 154001\n",
      "Processing line: 155001\n",
      "Processing line: 156001\n",
      "Processing line: 157001\n",
      "Processing line: 158001\n",
      "Processing line: 159001\n",
      "Processing line: 160001\n",
      "Processing line: 161001\n",
      "Processing line: 162001\n",
      "Processing line: 163001\n",
      "Processing line: 164001\n",
      "Processing line: 165001\n",
      "Processing line: 166001\n",
      "Processing line: 167001\n",
      "Processing line: 168001\n",
      "Processing line: 169001\n",
      "Processing line: 170001\n",
      "Processing line: 171001\n",
      "Processing line: 172001\n",
      "Processing line: 173001\n",
      "Processing line: 174001\n",
      "Processing line: 175001\n",
      "Processing line: 176001\n",
      "Processing line: 177001\n",
      "Processing line: 178001\n",
      "Processing line: 179001\n",
      "Processing line: 180001\n",
      "Processing line: 181001\n",
      "Processing line: 182001\n",
      "Processing line: 183001\n",
      "Processing line: 184001\n",
      "Processing line: 185001\n",
      "Processing line: 186001\n",
      "Processing line: 187001\n",
      "Processing line: 188001\n",
      "Processing line: 189001\n",
      "Processing line: 190001\n",
      "Processing line: 191001\n",
      "Processing line: 192001\n",
      "Processing line: 193001\n",
      "Processing line: 194001\n",
      "Processing line: 195001\n",
      "Processing line: 196001\n",
      "Processing line: 197001\n",
      "Processing line: 198001\n",
      "Processing line: 199001\n",
      "Processing line: 200001\n",
      "Processing line: 201001\n",
      "Processing line: 202001\n",
      "Processing line: 203001\n",
      "Processing line: 204001\n",
      "Processing line: 205001\n",
      "Processing line: 206001\n",
      "Processing line: 207001\n",
      "Processing line: 208001\n",
      "Processing line: 209001\n",
      "Processing line: 210001\n",
      "Processing line: 211001\n",
      "Processing line: 212001\n",
      "Processing line: 213001\n",
      "Processing line: 214001\n",
      "Processing line: 215001\n",
      "Processing line: 216001\n",
      "Processing line: 217001\n",
      "Processing line: 218001\n",
      "Processing line: 219001\n",
      "Processing line: 220001\n",
      "Processing line: 221001\n",
      "Processing line: 222001\n",
      "Processing line: 223001\n",
      "Processing line: 224001\n",
      "Processing line: 225001\n",
      "Processing line: 226001\n",
      "Processing line: 227001\n",
      "Processing line: 228001\n",
      "Processing line: 229001\n",
      "Processing line: 230001\n",
      "Processing line: 231001\n",
      "Processing line: 232001\n",
      "Processing line: 233001\n",
      "Processing line: 234001\n",
      "Processing line: 235001\n",
      "Processing line: 236001\n",
      "Processing line: 237001\n",
      "Processing line: 238001\n",
      "Processing line: 239001\n",
      "Processing line: 240001\n",
      "Processing line: 241001\n",
      "Processing line: 242001\n",
      "Processing line: 243001\n",
      "Processing line: 244001\n",
      "Processing line: 245001\n",
      "Processing line: 246001\n",
      "Processing line: 247001\n",
      "Processing line: 248001\n",
      "Processing line: 249001\n",
      "Processing line: 250001\n",
      "Processing line: 251001\n",
      "Processing line: 252001\n",
      "Processing line: 253001\n",
      "Processing line: 254001\n",
      "Processing line: 255001\n",
      "Processing line: 256001\n",
      "Processing line: 257001\n",
      "Processing line: 258001\n",
      "Processing line: 259001\n",
      "Processing line: 260001\n",
      "Processing line: 261001\n",
      "Processing line: 262001\n",
      "Processing line: 263001\n",
      "Processing line: 264001\n",
      "Processing line: 265001\n",
      "Processing line: 266001\n",
      "Processing line: 267001\n",
      "Processing line: 268001\n",
      "Processing line: 269001\n",
      "Processing line: 270001\n",
      "Processing line: 271001\n",
      "Processing line: 272001\n",
      "Processing line: 273001\n",
      "Processing line: 274001\n",
      "Processing line: 275001\n",
      "Processing line: 276001\n",
      "Processing line: 277001\n",
      "Processing line: 278001\n",
      "Processing line: 279001\n",
      "Processing line: 280001\n",
      "Processing line: 281001\n",
      "Processing line: 282001\n",
      "Processing line: 283001\n",
      "Finished - reading edges ...\n",
      "Removing duplicates in pairs list\n",
      "Set length: 266997 and List length: 283047\n",
      "finished writing edges\n"
     ]
    }
   ],
   "source": [
    "valid_pairs = read_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Clusters\n",
    "\n",
    "- Save list of clusters\n",
    "- Generate bag of cluster ids for each segment\n",
    "    - use nodes per segment as replace with cluster id\n",
    "    - if no node found, use cluster id -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17394\n"
     ]
    }
   ],
   "source": [
    "segids = []\n",
    "for fid in segment_map:\n",
    "    segids.extend(segment_map[fid].keys())\n",
    "print(len(segids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_clusters(clusters_utd_fname):\n",
    "    clusters = []\n",
    "    with open(clusters_utd_fname, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            try:\n",
    "                nodes = map(int, line.strip().split())\n",
    "                clusters.append(nodes)\n",
    "            except:\n",
    "                print(line)                    \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = load_clusters(clusters_utd_fname)\n",
    "pickle.dump(clusters, open(clusters_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_pseudowords_for_segments(segment_map, nodes_dict, clusters, feats_fname, feats_dict_fname):\n",
    "    feats_dict = {}\n",
    "    total_errors = 0\n",
    "    display_den = len(clusters) // 10\n",
    "    \n",
    "    for clid, nodes in enumerate(clusters):\n",
    "        if clid % display_den == 0:\n",
    "            print('processing cluster %d out of %d' % (clid, len(clusters)))\n",
    "        for nid in nodes:\n",
    "            node = nodes_dict[nid]\n",
    "            if node.seg not in feats_dict:\n",
    "                feats_dict[node.seg] = []\n",
    "            feats_dict[node.seg].append(str(clid))\n",
    "    \n",
    "    print(\"total clusters: %d\" % clid)\n",
    "    # Get complete list of segment ids\n",
    "    segids = []\n",
    "    for fid in segment_map:\n",
    "        segids.extend(segment_map[fid].keys())\n",
    "    \n",
    "    with open(feats_fname, \"w\") as out_f:\n",
    "        for seg_id in sorted(segids):\n",
    "            # adding -1 for missing pseudotext\n",
    "            if seg_id not in feats_dict:\n",
    "                #outline = \"-1\\n\"\n",
    "                total_errors += 1\n",
    "                feats_dict[seg_id] = ['-1']\n",
    "            \n",
    "            outline = \" \".join(map(str,sorted(feats_dict[seg_id])))\n",
    "            outline = outline.strip() + \"\\n\"\n",
    "            out_f.write(outline)\n",
    "            \n",
    "    print(\"Finished writing features file: %s\" % os.path.basename(feats_fname))\n",
    "    print(\"Writing to file: %s\" % os.path.basename(feats_dict_fname))\n",
    "    pickle.dump(feats_dict, open(feats_dict_fname, \"wb\"))\n",
    "    print(\"Psuedowords not found for: %d segments, out of total: %d segments\" % (total_errors, len(segids)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing cluster 0 out of 47794\n",
      "processing cluster 4779 out of 47794\n",
      "processing cluster 9558 out of 47794\n",
      "processing cluster 14337 out of 47794\n",
      "processing cluster 19116 out of 47794\n",
      "processing cluster 23895 out of 47794\n",
      "processing cluster 28674 out of 47794\n",
      "processing cluster 33453 out of 47794\n",
      "processing cluster 38232 out of 47794\n",
      "processing cluster 43011 out of 47794\n",
      "processing cluster 47790 out of 47794\n",
      "total clusters: 47793\n",
      "Finished writing features file: pseudowords.feats\n",
      "Writing to file: pseudowords.dict\n",
      "Psuedowords not found for: 4666 segments, out of total: 17394 segments\n"
     ]
    }
   ],
   "source": [
    "generate_pseudowords_for_segments(segment_map, nodes_dict, clusters, feats_fname, feats_dict_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'../../../ZRTools/exp/callhome/matches/config0.87-0.90-0.70-40/pseudowords.dict'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats_dict_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_dur_pseudotext(segment_map, nodes_dict, clusters, feats_fname, feats_dict_fname):\n",
    "    dur_dict = {}\n",
    "    total_errors = 0\n",
    "    total_dur = 0\n",
    "    display_den = len(clusters) // 10\n",
    "    \n",
    "    for clid, nodes in enumerate(clusters):\n",
    "        if clid % display_den == 0:\n",
    "            print('processing cluster %d out of %d' % (clid, len(clusters)))\n",
    "        for nid in nodes:\n",
    "            node = nodes_dict[nid]\n",
    "            if node.seg not in dur_dict:\n",
    "                dur_dict[node.seg] = set()\n",
    "            dur_dict[node.seg] |= set(range(node.start, node.end+1))\n",
    "            \n",
    "    # Get complete list of segment ids\n",
    "    segids = []\n",
    "    for fid in segment_map:\n",
    "        segids.extend(segment_map[fid].keys())\n",
    "    \n",
    "    print(\"Total segments with pseudotext: %d\" % len(dur_dict))\n",
    "    print(\"Total calls with pseudotext: %d\" % len(set([sid.split(\".\")[0] for sid in dur_dict])))\n",
    "    print(\"Total segments: %d\" % len(segids))\n",
    "    total_dur = sum([len(dur_list) for dur_list in dur_dict.values()])\n",
    "    print(\"Total duration: %d secs, %.2f hours\" % (total_dur, total_dur / 100 / 3600))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing cluster 0 out of 47794\n",
      "processing cluster 4779 out of 47794\n",
      "processing cluster 9558 out of 47794\n",
      "processing cluster 14337 out of 47794\n",
      "processing cluster 19116 out of 47794\n",
      "processing cluster 23895 out of 47794\n",
      "processing cluster 28674 out of 47794\n",
      "processing cluster 33453 out of 47794\n",
      "processing cluster 38232 out of 47794\n",
      "processing cluster 43011 out of 47794\n",
      "processing cluster 47790 out of 47794\n",
      "Total segments with pseudotext: 12728\n",
      "Total calls with pseudotext: 104\n",
      "Total segments: 17394\n",
      "Total duration: 2520269 secs, 7.00 hours\n"
     ]
    }
   ],
   "source": [
    "check_dur_pseudotext(segment_map, nodes_dict, clusters, feats_fname, feats_dict_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Keyword spotting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def map_keywords_to_segments(matches_file, segment_matches_file):\n",
    "    total_errors = 0\n",
    "    with open(matches_file, \"r\") as call_f, open(segment_matches_file, \"w\") as seg_f:\n",
    "        for i, line in enumerate(call_f):\n",
    "            line_items = line.strip().split()\n",
    "            if len(line_items) == 2:\n",
    "                key_id = line_items[0]\n",
    "                call_id = line_items[1]\n",
    "            else:\n",
    "                key_details = \"{0:s},{1:s},{2:s}\".format(key_id,line_items[0], line_items[1])\n",
    "                call_start, call_end = map(int, line_items[2:4])\n",
    "                try:\n",
    "                    seg_id, seg_node_start, seg_node_end, e = (search_segid(call_start, \n",
    "                                                                            call_end, \n",
    "                                                                            call_id, segment_map))\n",
    "                except ValueError:\n",
    "                    print(\"Incorrect line format at line: %d\\n%s\" % (i, line))\n",
    "                total_errors += e\n",
    "                seg_details = \"{0:s},{1:d},{2:d}\".format(seg_id, seg_node_start, seg_node_end)\n",
    "                outline = \"{0:s},{1:},{2:s},{3:s}\\n\".format(key_details, seg_details, \n",
    "                                                 line_items[-2], line_items[-1])\n",
    "                seg_f.write(outline)\n",
    "    print(\"Total nodes: %d\" % (i+1))\n",
    "    print(\"Total errors: %d\" % total_errors)\n",
    "    print(\"completed\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_keywords_to_segments(config['proto']['matches'], config['proto']['seg_matches'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
