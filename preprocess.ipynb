{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import cPickle as pickle\n",
    "import json\n",
    "import subprocess\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "from collections import namedtuple\n",
    "from itertools import izip\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing CALLHOME for ZRTools\n",
    "\n",
    "\n",
    "- Created: 26-Oct-2016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mapping for start time for each segment\n",
    "\n",
    "Format: Dictionary  \n",
    "key: {key: value}  \n",
    "*file: {file.seg.wav: start time}*  \n",
    "Name: segment_start.dict, segment_start.txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_segments_file(seg_fname):\n",
    "    segment_map = {}\n",
    "    with open(seg_fname, \"r\") as seg_f:\n",
    "        for i, line in enumerate(seg_f):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            try:\n",
    "                line_items = line.strip().split()\n",
    "                seg_key = line_items[0]\n",
    "                file_id = line_items[1]\n",
    "                if file_id not in segment_map:\n",
    "                    segment_map[file_id] = {}\n",
    "                seg_start = int(float(line_items[6])*100)\n",
    "                segment_map[file_id][seg_key] = seg_start\n",
    "            except ValueError:\n",
    "                print(\"Incorrect line format at line: %d\" % i)\n",
    "    return segment_map\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read segment map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segment_map = read_segments_file('../segments.txt')\n",
    "pickle.dump(segment_map, open(\"../segments.dict\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VAD files for merged wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_merged_vad(vad_file_id, segment_map, seg_vad_path, merged_vad_path):\n",
    "    total_dur_10ms = 0\n",
    "    total_dur_10ms_ge500ms = 0\n",
    "    with open(os.path.join(merged_vad_path, vad_file_id+\".vad\"), \"w\") as vad_f:\n",
    "        print(\"creating vad %s ...\" % vad_file_id)\n",
    "        for i, (seg_id, seg_start) in enumerate(sorted(segment_map[vad_file_id].items(), key=lambda t:t[0])):\n",
    "            with open(os.path.join(seg_vad_path, seg_id+\".vad\"), \"r\") as seg_vad_f:\n",
    "                for line in seg_vad_f:\n",
    "                    line_items = map(int, line.strip().split())\n",
    "                    start = seg_start+line_items[0]\n",
    "                    end = seg_start+line_items[1]\n",
    "                    total_dur_10ms += (end-start)\n",
    "                    total_dur_10ms_ge500ms += ((end - start) if (end-start) >= 50 else 0)\n",
    "                    out_line = (\"%d %d\\n\" %(start, end))\n",
    "                    vad_f.write(out_line)\n",
    "                # end for\n",
    "            # end reading seg file\n",
    "        # end looping over all segments\n",
    "    # end writing vad file\n",
    "    return total_dur_10ms, total_dur_10ms_ge500ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new directory for merged vads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_vads_path = \"../mergedVads\"\n",
    "seg_vad_path = \"../vad\"\n",
    "if not os.path.exists(merged_vads_path):\n",
    "    os.makedirs(merged_vads_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create merged vad for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating vad 090 ...\n",
      "creating vad 091 ...\n",
      "creating vad 092 ...\n",
      "creating vad 093 ...\n",
      "creating vad 094 ...\n",
      "creating vad 095 ...\n",
      "creating vad 096 ...\n",
      "creating vad 097 ...\n",
      "creating vad 010 ...\n",
      "creating vad 011 ...\n",
      "creating vad 012 ...\n",
      "creating vad 013 ...\n",
      "creating vad 014 ...\n",
      "creating vad 015 ...\n",
      "creating vad 018 ...\n",
      "creating vad 025 ...\n",
      "creating vad 024 ...\n",
      "creating vad 027 ...\n",
      "creating vad 026 ...\n",
      "creating vad 021 ...\n",
      "creating vad 023 ...\n",
      "creating vad 022 ...\n",
      "creating vad 029 ...\n",
      "creating vad 028 ...\n",
      "creating vad 115 ...\n",
      "creating vad 114 ...\n",
      "creating vad 038 ...\n",
      "creating vad 039 ...\n",
      "creating vad 111 ...\n",
      "creating vad 110 ...\n",
      "creating vad 113 ...\n",
      "creating vad 112 ...\n",
      "creating vad 032 ...\n",
      "creating vad 033 ...\n",
      "creating vad 030 ...\n",
      "creating vad 031 ...\n",
      "creating vad 036 ...\n",
      "creating vad 037 ...\n",
      "creating vad 034 ...\n",
      "creating vad 035 ...\n",
      "creating vad 051 ...\n",
      "creating vad 108 ...\n",
      "creating vad 049 ...\n",
      "creating vad 048 ...\n",
      "creating vad 047 ...\n",
      "creating vad 046 ...\n",
      "creating vad 045 ...\n",
      "creating vad 044 ...\n",
      "creating vad 043 ...\n",
      "creating vad 042 ...\n",
      "creating vad 041 ...\n",
      "creating vad 040 ...\n",
      "creating vad 058 ...\n",
      "creating vad 059 ...\n",
      "creating vad 103 ...\n",
      "creating vad 054 ...\n",
      "creating vad 056 ...\n",
      "creating vad 057 ...\n",
      "creating vad 050 ...\n",
      "creating vad 100 ...\n",
      "creating vad 052 ...\n",
      "creating vad 053 ...\n",
      "creating vad 101 ...\n",
      "creating vad 106 ...\n",
      "creating vad 107 ...\n",
      "creating vad 104 ...\n",
      "creating vad 105 ...\n",
      "creating vad 061 ...\n",
      "creating vad 060 ...\n",
      "creating vad 063 ...\n",
      "creating vad 062 ...\n",
      "creating vad 065 ...\n",
      "creating vad 064 ...\n",
      "creating vad 067 ...\n",
      "creating vad 066 ...\n",
      "creating vad 117 ...\n",
      "creating vad 116 ...\n",
      "creating vad 076 ...\n",
      "creating vad 077 ...\n",
      "creating vad 075 ...\n",
      "creating vad 072 ...\n",
      "creating vad 073 ...\n",
      "creating vad 070 ...\n",
      "creating vad 071 ...\n",
      "creating vad 078 ...\n",
      "creating vad 079 ...\n",
      "creating vad 119 ...\n",
      "creating vad 118 ...\n",
      "creating vad 089 ...\n",
      "creating vad 088 ...\n",
      "creating vad 083 ...\n",
      "creating vad 082 ...\n",
      "creating vad 081 ...\n",
      "creating vad 087 ...\n",
      "creating vad 086 ...\n",
      "creating vad 085 ...\n",
      "creating vad 084 ...\n",
      "creating vad 002 ...\n",
      "creating vad 001 ...\n",
      "creating vad 007 ...\n",
      "creating vad 006 ...\n",
      "creating vad 005 ...\n",
      "creating vad 009 ...\n",
      "creating vad 120 ...\n"
     ]
    }
   ],
   "source": [
    "total_dur_10ms, total_dur_10ms_ge500ms = 0, 0\n",
    "for vad_file_id in segment_map:\n",
    "    t1, t2 = create_merged_vad(vad_file_id, segment_map, seg_vad_path, merged_vads_path)\n",
    "    total_dur_10ms += t1\n",
    "    total_dur_10ms_ge500ms += t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4724341 3091219\n",
      "['13.123', '8.587']\n"
     ]
    }
   ],
   "source": [
    "print(total_dur_10ms, total_dur_10ms_ge500ms)\n",
    "print(map(lambda t: \"{0:.3f}\".format((t / 100.0 / 3600)), [total_dur_10ms, total_dur_10ms_ge500ms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_wavs_path = \"../mergeWavs\"\n",
    "plp_path = \"../plp\"\n",
    "plp_norm_path = \"../std_plp\"\n",
    "if not os.path.exists(plp_path):\n",
    "    os.makedirs(plp_path)\n",
    "if not os.path.exists(plp_norm_path):\n",
    "    os.makedirs(plp_norm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"config.json\") as json_data_file:\n",
    "    config = json.load(json_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_file_lst(file_lst_fname):\n",
    "    prefix = \"../corpora/callhome/mergeWavs\"\n",
    "    wav_file_list = [os.path.join(prefix, wav_file) for \\\n",
    "                     wav_file in os.listdir(merged_wavs_path) if wav_file.endswith(\".wav\")]\n",
    "    wav_file_list_string = \"\\n\".join(wav_file_list)\n",
    "    with open(file_lst_fname, \"w\") as out_f:\n",
    "        out_f.write(wav_file_list_string)\n",
    "    print(\"Finished writing files.lst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing files.lst\n"
     ]
    }
   ],
   "source": [
    "create_file_lst(config[\"es\"][\"lst_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_plp(wav_fname, plp_fname):\n",
    "    FEACALC = config['base'][\"feacalc\"]\n",
    "    subprocess.call([FEACALC,\"-plp\", \\\n",
    "                    \"12\", \"-cep\", \"13\", \"-dom\", \"cep\", \"-deltaorder\", \\\n",
    "                    \"2\", \"-dither\", \"-frqaxis\", \"bark\", \"-samplerate\", \\\n",
    "                    \"8000\", \"-win\", \"25\", \"-step\", \"10\", \"-ip\", \\\n",
    "                    \"MSWAVE\", \"-rasta\", \"false\", \"-compress\", \\\n",
    "                    \"true\", \"-op\", \"swappedraw\", \"-o\", plp_fname, wav_fname])\n",
    "\n",
    "    \n",
    "def normalize_plp(plp_fname, vad_fname, plp_norm_fname):\n",
    "    STANDFEAT = config['base'][\"standfeat\"]\n",
    "    # Standardize binary file, for VAD regions only\n",
    "    subprocess.call([STANDFEAT, \"-D\", \"39\", \"-infile\", \\\n",
    "                    plp_fname, \"-outfile\", plp_norm_fname, \\\n",
    "                    \"-vadfile\", vad_fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plp for file 090 \n",
      "normalizing plp 090\n",
      "plp for file 023 \n",
      "normalizing plp 023\n",
      "plp for file 051 \n",
      "normalizing plp 051\n",
      "plp for file 052 \n",
      "normalizing plp 052\n",
      "plp for file 072 \n",
      "normalizing plp 072\n",
      "plp for file 006 \n",
      "normalizing plp 006\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "for i, file_id in enumerate(segment_map):\n",
    "    wav_fname = os.path.join(merged_wavs_path, file_id+\".wav\")\n",
    "    vad_fname = os.path.join(merged_vads_path, file_id+\".vad\")\n",
    "    plp_fname = os.path.join(plp_path, file_id+\".binary\")\n",
    "    plp_norm_fname = os.path.join(plp_norm_path, file_id+\".std.binary\")\n",
    "    \n",
    "    #print(file_id, wav_fname, vad_fname, plp_fname, plp_norm_fname)\n",
    "    \n",
    "    # create PLP\n",
    "    if i % 20 == 0:\n",
    "        print(\"plp for file %s \" % file_id)\n",
    "    \n",
    "    if not os.path.exists(plp_fname):\n",
    "        create_plp(wav_fname, plp_fname)\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(\"normalizing plp %s\" % file_id)\n",
    "    \n",
    "    if not os.path.exists(plp_norm_fname):\n",
    "        normalize_plp(plp_fname, vad_fname, plp_norm_fname)\n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LSH files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_wavs_path = \"../mergeWavs\"\n",
    "plp_path = \"../plp\"\n",
    "plp_norm_path = \"../std_plp\"\n",
    "lsh_path = \"../lsh\"\n",
    "if not os.path.exists(lsh_path):\n",
    "    os.makedirs(lsh_path)\n",
    "lsh_proj_fname = os.path.join(lsh_path, \"proj_S64xD39_seed1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lsh_proj_file(lsh_proj_fname):\n",
    "    subprocess.call([config['base'][\"lsh_genproj\"], \\\n",
    "                     \"-D\",\"39\",\"-S\",\"64\",\"-seed\", \\\n",
    "                     \"1\",\"-projfile\", lsh_proj_fname])\n",
    "\n",
    "def create_lsh_file(plp_norm_fname, vad_fname, lsh_proj_fname, lsh_fname):\n",
    "    LSH = config['base'][\"lsh\"]\n",
    "    subprocess.call([LSH, \"-D\", \"39\", \"-S\", \"64\", \\\n",
    "                    \"-projfile\", lsh_proj_fname, \\\n",
    "                    \"-featfile\", plp_norm_fname, \"-sigfile\", \\\n",
    "                    lsh_fname, \"-vadfile\", vad_fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_lsh_proj_file(lsh_proj_fname)\n",
    "os.path.exists(lsh_proj_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsh for file 090 \n",
      "lsh for file 023 \n",
      "lsh for file 051 \n",
      "lsh for file 052 \n",
      "lsh for file 072 \n",
      "lsh for file 006 \n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "for i, file_id in enumerate(segment_map):\n",
    "    wav_fname = os.path.join(merged_wavs_path, file_id+\".wav\")\n",
    "    vad_fname = os.path.join(merged_vads_path, file_id+\".vad\")\n",
    "    plp_norm_fname = os.path.join(plp_norm_path, file_id+\".std.binary\")\n",
    "    lsh_fname = os.path.join(lsh_path, file_id+\".std.lsh64\")\n",
    "    \n",
    "    #print(file_id, wav_fname, vad_fname, plp_fname, plp_norm_fname)\n",
    "    \n",
    "    # create LSH\n",
    "    if i % 20 == 0:\n",
    "        print(\"lsh for file %s \" % file_id)\n",
    "    \n",
    "    if not os.path.exists(lsh_fname):\n",
    "        create_lsh_file(plp_norm_fname, vad_fname, lsh_proj_fname, lsh_fname)\n",
    "        \n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ZRTools discovery command files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_path = '../exp'\n",
    "if not os.path.exists(exp_path):\n",
    "    os.makedirs(exp_path)\n",
    "\n",
    "# List of wav files\n",
    "wav_file_list = sorted(segment_map.keys())\n",
    "exp_name = 'callhome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated files.base\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(exp_path, 'files.base'), \"w\") as out_f:\n",
    "    for wav_file in wav_file_list:\n",
    "        out_f.write(wav_file+'\\n')\n",
    "print(\"Generated files.base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_discovery_cmd_scripts(exp_path, wav_file_list, exp_name, num_splits=1):\n",
    "    disc_file_split_base = \"disc_{0:d}.cmd\"\n",
    "    disc_file_split = os.path.join(exp_path, disc_file_split_base)\n",
    "    disc_split_file = os.path.join(exp_path, \"disc_split.txt\")\n",
    "    num_files = len(wav_file_list)\n",
    "    exp_local_path = os.path.join(\"exp\", exp_name)\n",
    "    cmd_string = \"scripts/plebdisc_filepair \\\"{0:s}\\\" \\\"{1:s}\\\" {2:s} 39\\n\"\n",
    "\n",
    "    total_lines = num_files * num_files\n",
    "    lines_per_file = total_lines // num_splits\n",
    "    smallfile = None\n",
    "    curr_line = 0\n",
    "    curr_file_num = 0\n",
    "\n",
    "    for i in xrange(num_files) :\n",
    "        if i % 20 == 0:\n",
    "            print(\"Progress: {0:d} out of: {1:d}\".format(curr_line+1, total_lines))\n",
    "        for j in xrange(num_files):\n",
    "            out_line = cmd_string.format(wav_file_list[i], \\\n",
    "                                              wav_file_list[j], \\\n",
    "                                              exp_local_path)\n",
    "            if curr_line % lines_per_file == 0:\n",
    "                if smallfile:\n",
    "                    smallfile.close()\n",
    "                small_filename = disc_file_split.format(curr_file_num)\n",
    "                smallfile = open(small_filename, \"w\")\n",
    "                curr_file_num += 1\n",
    "            smallfile.write(out_line)\n",
    "            curr_line += 1\n",
    "    if smallfile:\n",
    "        smallfile.close()\n",
    "\n",
    "    # Making a list of commands to execute the split disc list\n",
    "    full_split_cmd_string = \"nice sh {0:s} 1> {1:s} 2>{2:s} &\\n\"\n",
    "    split_cmd = os.path.join(exp_local_path, \"matches\",\"{0:s}.{1:d}\")\n",
    "    with open(disc_split_file, \"w\") as out_f:\n",
    "        for i in xrange(curr_file_num):\n",
    "            curr_split_file = os.path.join(exp_local_path, disc_file_split_base.format(i))\n",
    "            split_cmd_out = split_cmd.format(\"out\", i)\n",
    "            #split_cmd_err = split_cmd.format(\"err\", i)\n",
    "            split_cmd_err = \"/dev/null\"\n",
    "\n",
    "            out_line = \"nice sh \"\n",
    "            out_f.write(full_split_cmd_string.format(curr_split_file, \\\n",
    "                                                    split_cmd_out, \\\n",
    "                                                    split_cmd_err))\n",
    "\n",
    "    print(\"Completed - disc.cmd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1 out of: 10816\n",
      "Progress: 2081 out of: 10816\n",
      "Progress: 4161 out of: 10816\n",
      "Progress: 6241 out of: 10816\n",
      "Progress: 8321 out of: 10816\n",
      "Progress: 10401 out of: 10816\n",
      "Completed - disc.cmd\n"
     ]
    }
   ],
   "source": [
    "create_discovery_cmd_scripts(exp_path=exp_path, wav_file_list=wav_file_list, exp_name=exp_name, num_splits=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read transcripts, and translations into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Align = namedtuple('Align', ['word', 'start', 'end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_alignment_file(align_fname, stopwords_corpus=None):\n",
    "    align_list = []\n",
    "    with open(align_fname, \"r\") as align_f:\n",
    "        for line in align_f:\n",
    "            line_items = line.strip().split()\n",
    "            if len(line_items) != 3:\n",
    "                raise ValueError\n",
    "            start, end = map(lambda v: int(float(v)*100), line_items[1:3])\n",
    "            if (not stopwords_corpus) or \\\n",
    "            (stopwords_corpus and line_items[0].lower().decode(\"utf-8\") not in stopwords_corpus):\n",
    "                align_list.append(Align(*[line_items[0], start, end]))\n",
    "    if sorted(align_list, key=lambda t: t.start) != align_list and not align_fname.endswith(\"en\"):\n",
    "        raise IOError    \n",
    "            \n",
    "    return align_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_words_path = '../wav2es-words/'\n",
    "en_words_path = '../wav2eng-words/'\n",
    "align_dict_fname = '../align_dict.p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Align(word='MECHITA', start=12, end=50),\n",
       " Align(word='QU\\xc3\\xa9', start=50, end=73),\n",
       " Align(word='LAS', start=109, end=126),\n",
       " Align(word='HA', start=126, end=129),\n",
       " Align(word='MANDADO', start=129, end=169),\n",
       " Align(word='A', start=169, end=176),\n",
       " Align(word='QUI\\xc3\\xa9N', start=176, end=192),\n",
       " Align(word='A', start=192, end=198),\n",
       " Align(word='POCHO', start=198, end=225)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Align(word='MECHITA', start=12, end=50),\n",
       " Align(word='MANDADO', start=129, end=169),\n",
       " Align(word='QUI\\xc3\\xa9N', start=176, end=192),\n",
       " Align(word='POCHO', start=198, end=225)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Align(word='MECHITA', start=12, end=50),\n",
       " Align(word='WHAT', start=50, end=73),\n",
       " Align(word='SENT', start=129, end=169),\n",
       " Align(word='IT', start=126, end=129),\n",
       " Align(word='TO', start=169, end=176),\n",
       " Align(word='WHOM', start=176, end=192),\n",
       " Align(word='TO', start=192, end=198),\n",
       " Align(word='POCHO', start=198, end=225)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[Align(word='MECHITA', start=12, end=50),\n",
       " Align(word='SENT', start=129, end=169),\n",
       " Align(word='POCHO', start=198, end=225)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test code\n",
    "stopwords_es = set(stopwords.words('spanish'))\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "display(read_alignment_file('../wav2es-words/001.001.es'))\n",
    "display(read_alignment_file('../wav2es-words/001.001.es', stopwords_corpus=stopwords_es))\n",
    "display(read_alignment_file('../wav2eng-words/001.001.en'))\n",
    "display(read_alignment_file('../wav2eng-words/001.001.en', stopwords_corpus=stopwords_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('.')[0].endswith('haa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_list(file_path, file_ext):\n",
    "    return [os.path.splitext(f)[0] for f in os.listdir(file_path) if f.endswith(file_ext)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "set(['009.025'])\n"
     ]
    }
   ],
   "source": [
    "es_file_list = get_file_list(es_words_path, 'es')\n",
    "en_file_list = get_file_list(en_words_path, 'en')\n",
    "\n",
    "print(sorted(es_file_list) == sorted(en_file_list))\n",
    "print(set(es_file_list)-set(en_file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_alignment_dict():\n",
    "    align_dict = {}\n",
    "    stopwords_es = set(stopwords.words('spanish'))\n",
    "    stopwords_en = set(stopwords.words('english'))\n",
    "    for file_id in segment_map:\n",
    "        #print(\"Processing file: %s\" % file_id)\n",
    "        align_dict[file_id] = {}\n",
    "        for seg_id in segment_map[file_id]:\n",
    "            align_dict[file_id][seg_id] = {}\n",
    "            es_fname = os.path.join(es_words_path, seg_id+\".es\")\n",
    "            en_fname = os.path.join(en_words_path, seg_id+\".en\")\n",
    "            align_dict[file_id][seg_id][\"es\"] = read_alignment_file(es_fname)\n",
    "            align_dict[file_id][seg_id][\"en\"] = read_alignment_file(en_fname)\n",
    "            align_dict[file_id][seg_id][\"es_cnt\"] = read_alignment_file(es_fname, stopwords_corpus=stopwords_es)\n",
    "            align_dict[file_id][seg_id][\"en_cnt\"] = read_alignment_file(en_fname, stopwords_corpus=stopwords_en)\n",
    "    return align_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "align_dict = create_alignment_dict()\n",
    "pickle.dump(align_dict, open(align_dict_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_gold_feats(align_dict, gold_feats_dict_fname, es_key=\"es\"):\n",
    "    gold_feats_dict = {}\n",
    "    for fid in align_dict:\n",
    "        for sid in align_dict[fid]:\n",
    "            gold_feats_dict[sid] = {}\n",
    "            if align_dict[fid][sid][es_key] == []:\n",
    "                # Only es_cnt can be empty, in which case include stop words\n",
    "                gold_feats_dict[sid] = [w.word for w in align_dict[fid][sid]['es']]\n",
    "            else:\n",
    "                gold_feats_dict[sid] = [w.word for w in align_dict[fid][sid][es_key]]\n",
    "    print(\"Saving gold features using key: %s\" % es_key)\n",
    "    pickle.dump(gold_feats_dict, open(gold_feats_dict_fname, \"wb\"))\n",
    "    print(\"finished ...\")\n",
    "    return gold_feats_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving gold features using key: es_cnt\n",
      "finished ...\n"
     ]
    }
   ],
   "source": [
    "align_dict = pickle.load(open(align_dict_fname, \"rb\"))\n",
    "gold_feats_dict_fname = config['es']['gold_feats']\n",
    "gold_feats_dict = create_gold_feats(align_dict, gold_feats_dict_fname, es_key=\"es_cnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MECHITA', 'MANDADO', 'QUI\\xc3\\xa9N', 'POCHO']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# align_dict['001']['001.001']['es_cnt']\n",
    "gold_feats_dict['001.001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['090', '091', '092', '093', '094']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'en': [Align(es_word='MECHITA', start=12, end=50),\n",
       "  Align(es_word='WHAT', start=50, end=73),\n",
       "  Align(es_word='SENT', start=129, end=169),\n",
       "  Align(es_word='IT', start=126, end=129),\n",
       "  Align(es_word='TO', start=169, end=176),\n",
       "  Align(es_word='WHOM', start=176, end=192),\n",
       "  Align(es_word='TO', start=192, end=198),\n",
       "  Align(es_word='POCHO', start=198, end=225)],\n",
       " 'es': [Align(es_word='MECHITA', start=12, end=50),\n",
       "  Align(es_word='QU\\xc3\\xa9', start=50, end=73),\n",
       "  Align(es_word='LAS', start=109, end=126),\n",
       "  Align(es_word='HA', start=126, end=129),\n",
       "  Align(es_word='MANDADO', start=129, end=169),\n",
       "  Align(es_word='A', start=169, end=176),\n",
       "  Align(es_word='QUI\\xc3\\xa9N', start=176, end=192),\n",
       "  Align(es_word='A', start=192, end=198),\n",
       "  Align(es_word='POCHO', start=198, end=225)]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(segment_map.keys()[:5])\n",
    "display(align_dict[\"001\"][\"001.001\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['090', '091', '092', '093', '094', '095', '096', '097', '010', '011', '012', '013', '014', '015', '018', '025', '024', '027', '026', '021', '023', '022', '029', '028', '115', '114', '038', '039', '111', '110', '113', '112', '032', '033', '030', '031', '036', '037', '034', '035', '051', '108', '049', '048', '047', '046', '045', '044', '043', '042', '041', '040', '058', '059', '103', '054', '056', '057', '050', '100', '052', '053', '101', '106', '107', '104', '105', '061', '060', '063', '062', '065', '064', '067', '066', '117', '116', '076', '077', '075', '072', '073', '070', '071', '078', '079', '119', '118', '089', '088', '083', '082', '081', '087', '086', '085', '084', '002', '001', '007', '006', '005', '009', '120']\n"
     ]
    }
   ],
   "source": [
    "print(segment_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['001.001',\n",
       " '001.002',\n",
       " '001.003',\n",
       " '001.004',\n",
       " '001.005',\n",
       " '001.006',\n",
       " '001.007',\n",
       " '001.008',\n",
       " '001.009',\n",
       " '001.010',\n",
       " '001.011',\n",
       " '001.012',\n",
       " '001.013',\n",
       " '001.014',\n",
       " '001.016',\n",
       " '001.017',\n",
       " '001.018',\n",
       " '001.019',\n",
       " '001.020',\n",
       " '001.021',\n",
       " '001.022',\n",
       " '001.023',\n",
       " '001.026',\n",
       " '001.027',\n",
       " '001.028',\n",
       " '001.029',\n",
       " '001.030',\n",
       " '001.031',\n",
       " '001.032',\n",
       " '001.033',\n",
       " '001.037',\n",
       " '001.038',\n",
       " '001.039',\n",
       " '001.041',\n",
       " '001.045',\n",
       " '001.046',\n",
       " '001.047',\n",
       " '001.048',\n",
       " '001.049',\n",
       " '001.050',\n",
       " '001.051',\n",
       " '001.052',\n",
       " '001.053',\n",
       " '001.054',\n",
       " '001.055',\n",
       " '001.056',\n",
       " '001.058',\n",
       " '001.059',\n",
       " '001.062',\n",
       " '001.066',\n",
       " '001.068',\n",
       " '001.069',\n",
       " '001.071',\n",
       " '001.072',\n",
       " '001.075',\n",
       " '001.076',\n",
       " '001.077',\n",
       " '001.078',\n",
       " '001.080',\n",
       " '001.081',\n",
       " '001.082',\n",
       " '001.083',\n",
       " '001.084',\n",
       " '001.085',\n",
       " '001.086',\n",
       " '001.087',\n",
       " '001.088',\n",
       " '001.089',\n",
       " '001.090',\n",
       " '001.092',\n",
       " '001.094',\n",
       " '001.096',\n",
       " '001.099',\n",
       " '001.100',\n",
       " '001.102',\n",
       " '001.103',\n",
       " '001.105',\n",
       " '001.107',\n",
       " '001.108',\n",
       " '001.109',\n",
       " '001.112',\n",
       " '001.113',\n",
       " '001.114',\n",
       " '001.115',\n",
       " '001.116',\n",
       " '001.117',\n",
       " '001.118',\n",
       " '001.119',\n",
       " '001.120',\n",
       " '001.121',\n",
       " '001.122',\n",
       " '001.124',\n",
       " '001.126',\n",
       " '001.127',\n",
       " '001.128',\n",
       " '001.130',\n",
       " '001.131',\n",
       " '001.132',\n",
       " '001.134',\n",
       " '001.135',\n",
       " '001.136',\n",
       " '001.137',\n",
       " '001.138',\n",
       " '001.139',\n",
       " '001.140',\n",
       " '001.141',\n",
       " '001.142',\n",
       " '001.143',\n",
       " '001.145',\n",
       " '001.146',\n",
       " '001.147',\n",
       " '001.148',\n",
       " '001.149',\n",
       " '001.150',\n",
       " '001.151',\n",
       " '001.152',\n",
       " '001.153',\n",
       " '001.154',\n",
       " '001.155',\n",
       " '001.156',\n",
       " '001.157',\n",
       " '001.159',\n",
       " '001.161',\n",
       " '001.162',\n",
       " '001.164',\n",
       " '001.165',\n",
       " '001.167',\n",
       " '001.168',\n",
       " '001.169',\n",
       " '001.170',\n",
       " '001.171',\n",
       " '001.172',\n",
       " '001.173',\n",
       " '001.174',\n",
       " '001.176',\n",
       " '001.177',\n",
       " '001.178',\n",
       " '001.180',\n",
       " '001.181',\n",
       " '001.182',\n",
       " '001.184',\n",
       " '001.185',\n",
       " '001.186',\n",
       " '001.187',\n",
       " '001.188',\n",
       " '001.189',\n",
       " '001.190',\n",
       " '001.192',\n",
       " '001.193',\n",
       " '001.194',\n",
       " '001.195',\n",
       " '001.196',\n",
       " '001.197',\n",
       " '001.198',\n",
       " '001.199',\n",
       " '001.200',\n",
       " '001.201',\n",
       " '001.202',\n",
       " '001.203',\n",
       " '001.204',\n",
       " '001.205',\n",
       " '001.206',\n",
       " '001.208',\n",
       " '001.209',\n",
       " '001.210',\n",
       " '001.211',\n",
       " '001.212',\n",
       " '001.213',\n",
       " '001.214',\n",
       " '001.215',\n",
       " '001.216',\n",
       " '001.217',\n",
       " '001.219',\n",
       " '001.220',\n",
       " '001.221',\n",
       " '001.222',\n",
       " '001.223',\n",
       " '001.224',\n",
       " '001.226',\n",
       " '001.228',\n",
       " '001.229',\n",
       " '001.230',\n",
       " '001.231',\n",
       " '001.232',\n",
       " '001.233',\n",
       " '001.234',\n",
       " '001.235',\n",
       " '001.236',\n",
       " '001.237',\n",
       " '001.239',\n",
       " '001.240',\n",
       " '001.241',\n",
       " '001.244',\n",
       " '001.246',\n",
       " '001.247',\n",
       " '001.248',\n",
       " '001.249',\n",
       " '001.250',\n",
       " '001.251',\n",
       " '001.252',\n",
       " '001.253',\n",
       " '001.254',\n",
       " '001.255',\n",
       " '001.256',\n",
       " '001.257',\n",
       " '001.258',\n",
       " '001.259',\n",
       " '001.260',\n",
       " '001.261',\n",
       " '001.263',\n",
       " '001.265',\n",
       " '001.266',\n",
       " '001.267',\n",
       " '001.269',\n",
       " '001.270',\n",
       " '001.271',\n",
       " '001.272',\n",
       " '001.273',\n",
       " '001.274',\n",
       " '001.278',\n",
       " '001.279',\n",
       " '002.001',\n",
       " '002.002',\n",
       " '002.003',\n",
       " '002.004',\n",
       " '002.005',\n",
       " '002.006',\n",
       " '002.007',\n",
       " '002.008',\n",
       " '002.009',\n",
       " '002.010',\n",
       " '002.011',\n",
       " '002.012',\n",
       " '002.013',\n",
       " '002.014',\n",
       " '002.015',\n",
       " '002.016',\n",
       " '002.017',\n",
       " '002.018',\n",
       " '002.019',\n",
       " '002.020',\n",
       " '002.021',\n",
       " '002.022',\n",
       " '002.023',\n",
       " '002.024',\n",
       " '002.025',\n",
       " '002.026',\n",
       " '002.027',\n",
       " '002.028',\n",
       " '002.029',\n",
       " '002.030',\n",
       " '002.031',\n",
       " '002.032',\n",
       " '002.033',\n",
       " '002.034',\n",
       " '002.035',\n",
       " '002.036',\n",
       " '002.037',\n",
       " '002.039',\n",
       " '002.040',\n",
       " '002.041',\n",
       " '002.042',\n",
       " '002.043',\n",
       " '002.044',\n",
       " '002.045',\n",
       " '002.046',\n",
       " '002.047',\n",
       " '002.048',\n",
       " '002.049',\n",
       " '002.050',\n",
       " '002.051',\n",
       " '002.052',\n",
       " '002.053',\n",
       " '002.054',\n",
       " '002.055',\n",
       " '002.056',\n",
       " '002.057',\n",
       " '002.058',\n",
       " '002.059',\n",
       " '002.060',\n",
       " '002.061',\n",
       " '002.062',\n",
       " '002.063',\n",
       " '002.064',\n",
       " '002.065',\n",
       " '002.066',\n",
       " '002.067',\n",
       " '002.068',\n",
       " '002.069',\n",
       " '002.070',\n",
       " '002.071',\n",
       " '002.072',\n",
       " '002.073',\n",
       " '002.074',\n",
       " '002.075',\n",
       " '002.076',\n",
       " '002.077',\n",
       " '002.078',\n",
       " '002.079',\n",
       " '002.080',\n",
       " '002.081',\n",
       " '002.082',\n",
       " '002.083',\n",
       " '002.084',\n",
       " '002.085',\n",
       " '002.086',\n",
       " '002.087',\n",
       " '002.088',\n",
       " '002.089',\n",
       " '002.090',\n",
       " '002.091',\n",
       " '002.092',\n",
       " '002.093',\n",
       " '002.094',\n",
       " '002.095',\n",
       " '002.096',\n",
       " '002.097',\n",
       " '002.098',\n",
       " '002.099',\n",
       " '002.100',\n",
       " '002.101',\n",
       " '002.102',\n",
       " '002.103',\n",
       " '002.104',\n",
       " '002.105',\n",
       " '002.106',\n",
       " '002.109',\n",
       " '002.110',\n",
       " '002.111',\n",
       " '002.112',\n",
       " '002.114',\n",
       " '002.115',\n",
       " '002.116',\n",
       " '002.117',\n",
       " '002.118',\n",
       " '002.119',\n",
       " '002.120',\n",
       " '002.121',\n",
       " '002.122',\n",
       " '002.123',\n",
       " '002.125',\n",
       " '002.126',\n",
       " '002.127',\n",
       " '002.128',\n",
       " '002.129',\n",
       " '002.130',\n",
       " '002.131',\n",
       " '002.132',\n",
       " '002.133',\n",
       " '002.134',\n",
       " '002.135',\n",
       " '002.136',\n",
       " '002.137',\n",
       " '002.138',\n",
       " '002.139',\n",
       " '002.140',\n",
       " '002.141',\n",
       " '002.142',\n",
       " '002.143',\n",
       " '002.144',\n",
       " '002.145',\n",
       " '002.146',\n",
       " '002.147',\n",
       " '002.149',\n",
       " '002.150',\n",
       " '002.151',\n",
       " '002.152',\n",
       " '002.153',\n",
       " '002.154',\n",
       " '002.156',\n",
       " '002.157',\n",
       " '002.159',\n",
       " '002.161',\n",
       " '002.162',\n",
       " '002.163',\n",
       " '002.164',\n",
       " '002.165',\n",
       " '002.166',\n",
       " '002.167',\n",
       " '002.168',\n",
       " '002.169',\n",
       " '002.170',\n",
       " '002.171',\n",
       " '002.172',\n",
       " '002.173',\n",
       " '002.174',\n",
       " '002.175',\n",
       " '002.176',\n",
       " '002.177',\n",
       " '002.178',\n",
       " '002.179',\n",
       " '002.180',\n",
       " '002.181',\n",
       " '002.182',\n",
       " '002.183',\n",
       " '002.184',\n",
       " '002.185',\n",
       " '002.186',\n",
       " '002.187',\n",
       " '002.188',\n",
       " '002.189',\n",
       " '002.190',\n",
       " '002.191',\n",
       " '002.192',\n",
       " '002.193',\n",
       " '002.194',\n",
       " '002.195',\n",
       " '002.196',\n",
       " '002.197',\n",
       " '002.198',\n",
       " '002.199',\n",
       " '002.200',\n",
       " '002.201',\n",
       " '002.203',\n",
       " '002.204',\n",
       " '002.205',\n",
       " '002.206',\n",
       " '002.207',\n",
       " '002.208',\n",
       " '002.209',\n",
       " '002.210',\n",
       " '002.211',\n",
       " '002.212',\n",
       " '002.213',\n",
       " '002.214',\n",
       " '002.216',\n",
       " '002.217',\n",
       " '002.218',\n",
       " '002.220']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sorted(segment_map['001'].keys()+segment_map['002'].keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
