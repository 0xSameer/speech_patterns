{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import cPickle as pickle\n",
    "import json\n",
    "import subprocess\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "from collections import namedtuple\n",
    "from itertools import izip\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"config.json\") as json_data_file:\n",
    "    config = json.load(json_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing CALLHOME for ZRTools\n",
    "\n",
    "\n",
    "- Created: 26-Oct-2016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mapping for start time for each segment\n",
    "\n",
    "Format: Dictionary  \n",
    "key: {key: value}  \n",
    "*file: {file.seg.wav: start time}*  \n",
    "Name: segment_start.dict, segment_start.txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_segments_file(seg_fname):\n",
    "    segment_map = {}\n",
    "    with open(seg_fname, \"r\") as seg_f:\n",
    "        for i, line in enumerate(seg_f):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            try:\n",
    "                line_items = line.strip().split()\n",
    "                seg_key = line_items[0]\n",
    "                file_id = line_items[1]\n",
    "                if file_id not in segment_map:\n",
    "                    segment_map[file_id] = {}\n",
    "                seg_start = int(float(line_items[6])*100)\n",
    "                segment_map[file_id][seg_key] = seg_start\n",
    "            except ValueError:\n",
    "                print(\"Incorrect line format at line: %d\" % i)\n",
    "    return segment_map\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read segment map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segment_map = read_segments_file('../segments.txt')\n",
    "pickle.dump(segment_map, open(config['es']['segment_dict_fname'], \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VAD files for merged wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_merged_vad_from_ed(vad_file_id, segment_map, seg_vad_path, merged_vad_path):\n",
    "    total_dur_10ms = 0\n",
    "    total_dur_10ms_ge500ms = 0\n",
    "    with open(os.path.join(merged_vad_path, vad_file_id+\".vad\"), \"w\") as vad_f:\n",
    "        print(\"creating vad %s ...\" % vad_file_id)\n",
    "        for i, (seg_id, seg_start) in enumerate(sorted(segment_map[vad_file_id].items(), key=lambda t:t[0])):\n",
    "            with open(os.path.join(seg_vad_path, seg_id+\".vad\"), \"r\") as seg_vad_f:\n",
    "                for line in seg_vad_f:\n",
    "                    line_items = map(int, line.strip().split())\n",
    "                    start = seg_start+line_items[0]\n",
    "                    end = seg_start+line_items[1]\n",
    "                    total_dur_10ms += (end-start)\n",
    "                    total_dur_10ms_ge500ms += ((end - start) if (end-start) >= 50 else 0)\n",
    "                    out_line = (\"%d %d\\n\" %(start, end))\n",
    "                    vad_f.write(out_line)\n",
    "                # end for\n",
    "            # end reading seg file\n",
    "        # end looping over all segments\n",
    "    # end writing vad file\n",
    "    return total_dur_10ms, total_dur_10ms_ge500ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new directory for merged vads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merged_ed_vads_path = \"../mergedVads\"\n",
    "# seg_vad_path = \"../vad\"\n",
    "# if not os.path.exists(merged_ed_vads_path):\n",
    "#     os.makedirs(merged_ed_vads_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create merged vad for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# total_dur_10ms, total_dur_10ms_ge500ms = 0, 0\n",
    "# for vad_file_id in segment_map:\n",
    "#     t1, t2 = create_merged_vad(vad_file_id, segment_map, seg_vad_path, merged_vads_path)\n",
    "#     total_dur_10ms += t1\n",
    "#     total_dur_10ms_ge500ms += t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(total_dur_10ms, total_dur_10ms_ge500ms)\n",
    "# print(map(lambda t: \"{0:.3f}\".format((t / 100.0 / 3600)), [total_dur_10ms, total_dur_10ms_ge500ms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_wavs_path = \"../mergeWavs\"\n",
    "plp_path = \"../plp\"\n",
    "plp_norm_path = \"../std_plp\"\n",
    "if not os.path.exists(plp_path):\n",
    "    os.makedirs(plp_path)\n",
    "if not os.path.exists(plp_norm_path):\n",
    "    os.makedirs(plp_norm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"config.json\") as json_data_file:\n",
    "    config = json.load(json_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_file_lst(file_lst_fname):\n",
    "    prefix = \"../corpora/callhome/mergeWavs\"\n",
    "    wav_file_list = [os.path.join(prefix, wav_file) for \\\n",
    "                     wav_file in os.listdir(merged_wavs_path) if wav_file.endswith(\".wav\")]\n",
    "    wav_file_list_string = \"\\n\".join(wav_file_list)\n",
    "    with open(file_lst_fname, \"w\") as out_f:\n",
    "        out_f.write(wav_file_list_string)\n",
    "    print(\"Finished writing files.lst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing files.lst\n"
     ]
    }
   ],
   "source": [
    "create_file_lst(config[\"es\"][\"lst_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_plp(wav_fname, plp_fname):\n",
    "    FEACALC = config['base'][\"feacalc\"]\n",
    "    subprocess.call([FEACALC,\"-plp\", \\\n",
    "                    \"12\", \"-cep\", \"13\", \"-dom\", \"cep\", \"-deltaorder\", \\\n",
    "                    \"2\", \"-dither\", \"-frqaxis\", \"bark\", \"-samplerate\", \\\n",
    "                    \"8000\", \"-win\", \"25\", \"-step\", \"10\", \"-ip\", \\\n",
    "                    \"MSWAVE\", \"-rasta\", \"false\", \"-compress\", \\\n",
    "                    \"true\", \"-op\", \"swappedraw\", \"-o\", plp_fname, wav_fname])\n",
    "\n",
    "    \n",
    "def normalize_plp(plp_fname, vad_fname, plp_norm_fname):\n",
    "    STANDFEAT = config['base'][\"standfeat\"]\n",
    "    # Standardize binary file, for VAD regions only\n",
    "    subprocess.call([STANDFEAT, \"-D\", \"39\", \"-infile\", \\\n",
    "                    plp_fname, \"-outfile\", plp_norm_fname, \\\n",
    "                    \"-vadfile\", vad_fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plp for file 090 \n",
      "normalizing plp 090\n",
      "plp for file 023 \n",
      "normalizing plp 023\n",
      "plp for file 051 \n",
      "normalizing plp 051\n",
      "plp for file 052 \n",
      "normalizing plp 052\n",
      "plp for file 072 \n",
      "normalizing plp 072\n",
      "plp for file 006 \n",
      "normalizing plp 006\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "for i, file_id in enumerate(segment_map):\n",
    "    wav_fname = os.path.join(merged_wavs_path, file_id+\".wav\")\n",
    "    vad_fname = os.path.join(merged_fa_vads_path, file_id+\".vad\")\n",
    "    plp_fname = os.path.join(plp_path, file_id+\".binary\")\n",
    "    plp_norm_fname = os.path.join(plp_norm_path, file_id+\".std.binary\")\n",
    "    \n",
    "    #print(file_id, wav_fname, vad_fname, plp_fname, plp_norm_fname)\n",
    "    \n",
    "    # create PLP\n",
    "    if i % 20 == 0:\n",
    "        print(\"plp for file %s \" % file_id)\n",
    "    \n",
    "    #if not os.path.exists(plp_fname):\n",
    "    create_plp(wav_fname, plp_fname)\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(\"normalizing plp %s\" % file_id)\n",
    "    \n",
    "    #if not os.path.exists(plp_norm_fname):\n",
    "    normalize_plp(plp_fname, vad_fname, plp_norm_fname)\n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LSH files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_wavs_path = \"../mergeWavs\"\n",
    "plp_path = \"../plp\"\n",
    "plp_norm_path = \"../std_plp\"\n",
    "lsh_path = \"../lsh\"\n",
    "if not os.path.exists(lsh_path):\n",
    "    os.makedirs(lsh_path)\n",
    "lsh_proj_fname = os.path.join(lsh_path, \"proj_S64xD39_seed1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lsh_proj_file(lsh_proj_fname):\n",
    "    subprocess.call([config['base'][\"lsh_genproj\"], \\\n",
    "                     \"-D\",\"39\",\"-S\",\"64\",\"-seed\", \\\n",
    "                     \"1\",\"-projfile\", lsh_proj_fname])\n",
    "\n",
    "def create_lsh_file(plp_norm_fname, vad_fname, lsh_proj_fname, lsh_fname):\n",
    "    LSH = config['base'][\"lsh\"]\n",
    "    subprocess.call([LSH, \"-D\", \"39\", \"-S\", \"64\", \\\n",
    "                    \"-projfile\", lsh_proj_fname, \\\n",
    "                    \"-featfile\", plp_norm_fname, \"-sigfile\", \\\n",
    "                    lsh_fname, \"-vadfile\", vad_fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_lsh_proj_file(lsh_proj_fname)\n",
    "os.path.exists(lsh_proj_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsh for file 090 \n",
      "lsh for file 023 \n",
      "lsh for file 051 \n",
      "lsh for file 052 \n",
      "lsh for file 072 \n",
      "lsh for file 006 \n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "for i, file_id in enumerate(segment_map):\n",
    "    wav_fname = os.path.join(merged_wavs_path, file_id+\".wav\")\n",
    "    vad_fname = os.path.join(merged_fa_vads_path, file_id+\".vad\")\n",
    "    plp_norm_fname = os.path.join(plp_norm_path, file_id+\".std.binary\")\n",
    "    lsh_fname = os.path.join(lsh_path, file_id+\".std.lsh64\")\n",
    "    \n",
    "    #print(file_id, wav_fname, vad_fname, plp_fname, plp_norm_fname)\n",
    "    \n",
    "    # create LSH\n",
    "    if i % 20 == 0:\n",
    "        print(\"lsh for file %s \" % file_id)\n",
    "    \n",
    "    #if not os.path.exists(lsh_fname):\n",
    "    create_lsh_file(plp_norm_fname, vad_fname, lsh_proj_fname, lsh_fname)\n",
    "        \n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ZRTools discovery command files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_path = '../exp'\n",
    "if not os.path.exists(exp_path):\n",
    "    os.makedirs(exp_path)\n",
    "\n",
    "# List of wav files\n",
    "wav_file_list = sorted(segment_map.keys())\n",
    "exp_name = 'callhome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated files.base\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(exp_path, 'files.base'), \"w\") as out_f:\n",
    "    for wav_file in wav_file_list:\n",
    "        out_f.write(wav_file+'\\n')\n",
    "print(\"Generated files.base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_discovery_cmd_scripts(exp_path, wav_file_list, exp_name, num_splits=1):\n",
    "    disc_file_split_base = \"disc_{0:d}.cmd\"\n",
    "    disc_file_split = os.path.join(exp_path, disc_file_split_base)\n",
    "    disc_split_file = os.path.join(exp_path, \"disc_split.txt\")\n",
    "    num_files = len(wav_file_list)\n",
    "    exp_local_path = os.path.join(\"exp\", exp_name)\n",
    "    cmd_string = \"scripts/plebdisc_filepair \\\"{0:s}\\\" \\\"{1:s}\\\" {2:s} 39\\n\"\n",
    "\n",
    "    total_lines = num_files * num_files\n",
    "    lines_per_file = total_lines // num_splits\n",
    "    smallfile = None\n",
    "    curr_line = 0\n",
    "    curr_file_num = 0\n",
    "\n",
    "    for i in xrange(num_files) :\n",
    "        if i % 20 == 0:\n",
    "            print(\"Progress: {0:d} out of: {1:d}\".format(curr_line+1, total_lines))\n",
    "        for j in xrange(num_files):\n",
    "            out_line = cmd_string.format(wav_file_list[i], \\\n",
    "                                              wav_file_list[j], \\\n",
    "                                              exp_local_path)\n",
    "            if curr_line % lines_per_file == 0:\n",
    "                if smallfile:\n",
    "                    smallfile.close()\n",
    "                small_filename = disc_file_split.format(curr_file_num)\n",
    "                smallfile = open(small_filename, \"w\")\n",
    "                curr_file_num += 1\n",
    "            smallfile.write(out_line)\n",
    "            curr_line += 1\n",
    "    if smallfile:\n",
    "        smallfile.close()\n",
    "\n",
    "    # Making a list of commands to execute the split disc list\n",
    "    full_split_cmd_string = \"nice sh {0:s} 1> {1:s} 2>{2:s} &\\n\"\n",
    "    split_cmd = os.path.join(exp_local_path, \"matches\",\"{0:s}.{1:d}\")\n",
    "    with open(disc_split_file, \"w\") as out_f:\n",
    "        for i in xrange(curr_file_num):\n",
    "            curr_split_file = os.path.join(exp_local_path, disc_file_split_base.format(i))\n",
    "            split_cmd_out = split_cmd.format(\"out\", i)\n",
    "            #split_cmd_err = split_cmd.format(\"err\", i)\n",
    "            split_cmd_err = \"/dev/null\"\n",
    "\n",
    "            out_line = \"nice sh \"\n",
    "            out_f.write(full_split_cmd_string.format(curr_split_file, \\\n",
    "                                                    split_cmd_out, \\\n",
    "                                                    split_cmd_err))\n",
    "\n",
    "    print(\"Completed - disc.cmd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1 out of: 10816\n",
      "Progress: 2081 out of: 10816\n",
      "Progress: 4161 out of: 10816\n",
      "Progress: 6241 out of: 10816\n",
      "Progress: 8321 out of: 10816\n",
      "Progress: 10401 out of: 10816\n",
      "Completed - disc.cmd\n"
     ]
    }
   ],
   "source": [
    "create_discovery_cmd_scripts(exp_path=exp_path, wav_file_list=wav_file_list, exp_name=exp_name, num_splits=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read transcripts, and translations into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Align = namedtuple('Align', ['word', 'start', 'end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_alignment_file(align_fname, stopwords_corpus=None):\n",
    "    align_list = []\n",
    "    with open(align_fname, \"r\") as align_f:\n",
    "        for line in align_f:\n",
    "            line_items = line.strip().split()\n",
    "            if len(line_items) != 3:\n",
    "                raise ValueError\n",
    "            start, end = map(lambda v: int(float(v)*100), line_items[1:3])\n",
    "            if (not stopwords_corpus) or \\\n",
    "            (stopwords_corpus and line_items[0].lower().decode(\"utf-8\") not in stopwords_corpus):\n",
    "                align_list.append(Align(*[line_items[0], start, end]))\n",
    "    if sorted(align_list, key=lambda t: t.start) != align_list and not align_fname.endswith(\"en\"):\n",
    "        raise IOError    \n",
    "            \n",
    "    return align_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_words_path = '../wav2es-words/'\n",
    "en_words_path = '../wav2eng-words/'\n",
    "align_dict_fname = config['es']['align_dict_fname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test code\n",
    "stopwords_es = set(stopwords.words('spanish'))\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "display(read_alignment_file('../wav2es-words/001.001.es'))\n",
    "display(read_alignment_file('../wav2es-words/001.001.es', stopwords_corpus=stopwords_es))\n",
    "display(read_alignment_file('../wav2eng-words/001.001.en'))\n",
    "display(read_alignment_file('../wav2eng-words/001.001.en', stopwords_corpus=stopwords_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.listdir('.')[0].endswith('haa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_list(file_path, file_ext):\n",
    "    return [os.path.splitext(f)[0] for f in os.listdir(file_path) if f.endswith(file_ext)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_file_list = get_file_list(es_words_path, 'es')\n",
    "en_file_list = get_file_list(en_words_path, 'en')\n",
    "\n",
    "print(sorted(es_file_list) == sorted(en_file_list))\n",
    "print(set(es_file_list)-set(en_file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_alignment_dict():\n",
    "    align_dict = {}\n",
    "    stopwords_es = set(stopwords.words('spanish'))\n",
    "    stopwords_en = set(stopwords.words('english'))\n",
    "    for file_id in segment_map:\n",
    "        #print(\"Processing file: %s\" % file_id)\n",
    "        align_dict[file_id] = {}\n",
    "        for seg_id in segment_map[file_id]:\n",
    "            align_dict[file_id][seg_id] = {}\n",
    "            es_fname = os.path.join(es_words_path, seg_id+\".es\")\n",
    "            en_fname = os.path.join(en_words_path, seg_id+\".en\")\n",
    "            align_dict[file_id][seg_id][\"es\"] = read_alignment_file(es_fname)\n",
    "            align_dict[file_id][seg_id][\"en\"] = read_alignment_file(en_fname)\n",
    "            align_dict[file_id][seg_id][\"es_cnt\"] = read_alignment_file(es_fname, stopwords_corpus=stopwords_es)\n",
    "            align_dict[file_id][seg_id][\"en_cnt\"] = read_alignment_file(en_fname, stopwords_corpus=stopwords_en)\n",
    "    return align_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "align_dict = create_alignment_dict()\n",
    "pickle.dump(align_dict, open(align_dict_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VAD from alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "align_dict = pickle.load(open(align_dict_fname, \"rb\"))\n",
    "segment_map = pickle.load(open(config['es']['segment_dict_fname'], \"rb\"))\n",
    "has_500ms_fa_vad_dict_fname = config['es']['has_500ms_fa_vad_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Align(word='MECHITA', start=12, end=50),\n",
       "  Align(word='QU\\xc3\\xa9', start=50, end=73),\n",
       "  Align(word='LAS', start=109, end=126),\n",
       "  Align(word='HA', start=126, end=129),\n",
       "  Align(word='MANDADO', start=129, end=169),\n",
       "  Align(word='A', start=169, end=176),\n",
       "  Align(word='QUI\\xc3\\xa9N', start=176, end=192),\n",
       "  Align(word='A', start=192, end=198),\n",
       "  Align(word='POCHO', start=198, end=225)],\n",
       " [Align(word='LAS', start=25, end=48),\n",
       "  Align(word='HA', start=48, end=56),\n",
       "  Align(word='MANDADO', start=56, end=113),\n",
       "  Align(word='AL', start=113, end=135),\n",
       "  Align(word='AH', start=181, end=211)])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_dict['001']['001.001']['es'], align_dict['001']['001.002']['es']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 258)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_map['001']['001.001'], segment_map['001']['001.002']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_uttr_vad_from_alignment(align_dict, vad_path):\n",
    "    has_500ms_dur = {}\n",
    "    total_dur_10ms = 0\n",
    "    total_dur_10ms_ge500ms = 0\n",
    "    for i, vad_file_id in enumerate(align_dict):\n",
    "        if i % 20 == 0:\n",
    "            print(\"Created vad for %d files id\" % i)\n",
    "        for seg_id in align_dict[vad_file_id]:\n",
    "            with open(os.path.join(vad_path, seg_id+\".vad\"), \"w\") as vad_f:\n",
    "                dur_10ms = 0\n",
    "                dur_10ms_ge500ms = 0\n",
    "                vad_list = []\n",
    "                # start index\n",
    "                s = 0\n",
    "                # create a local list of alignment values\n",
    "                align_list = align_dict[vad_file_id][seg_id]['es']\n",
    "                for j in xrange(len(align_list)):\n",
    "                    # if 1st or last element, add to vad_list\n",
    "                    if ((j+1) == len(align_list)) or (align_list[j].end != align_list[j+1].start):\n",
    "                        vad_list.append(((align_list[s].start), (align_list[j].end)))\n",
    "                        s=j+1\n",
    "                # write vad list to file        \n",
    "                for vad_tup in vad_list:\n",
    "                    start = vad_tup[0]\n",
    "                    end = vad_tup[1]\n",
    "                    dur_10ms += (end-start)\n",
    "                    dur_10ms_ge500ms += ((end - start) if (end-start) >= 50 else 0)\n",
    "                    out_line = (\"%d %d\\n\" %(start, end))\n",
    "                    vad_f.write(out_line)\n",
    "                \n",
    "                # set whether atleast one vad region of 500 ms\n",
    "                has_500ms_dur[seg_id] = (dur_10ms_ge500ms > 0)\n",
    "                # compute total durations\n",
    "                total_dur_10ms += dur_10ms\n",
    "                total_dur_10ms_ge500ms += dur_10ms_ge500ms \n",
    "                    \n",
    "            # end for\n",
    "        # end looping over all segments\n",
    "    # end writing vad file\n",
    "    return total_dur_10ms, total_dur_10ms_ge500ms, has_500ms_dur\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_merged_vad_from_alignment(vad_file_id, align_dict, segment_map, vad_path):\n",
    "    total_dur_10ms = 0\n",
    "    total_dur_10ms_ge500ms = 0\n",
    "    with open(os.path.join(vad_path, vad_file_id+\".vad\"), \"w\") as vad_f:\n",
    "        print(\"creating vad %s ...\" % vad_file_id)\n",
    "        for i, (seg_id, seg_start) in enumerate(sorted(segment_map[vad_file_id].items(), key=lambda t:t[0])):\n",
    "            vad_list = []\n",
    "            # start index\n",
    "            s = 0\n",
    "            # create a local list of alignment values\n",
    "            align_list = align_dict[vad_file_id][seg_id]['es']\n",
    "            for j in xrange(len(align_list)):\n",
    "                # if 1st or last element, add to vad_list\n",
    "                if ((j+1) == len(align_list)) or (align_list[j].end != align_list[j+1].start):\n",
    "                    vad_list.append(((seg_start+align_list[s].start), (seg_start+align_list[j].end)))\n",
    "                    s=j+1\n",
    "            # write vad list to file        \n",
    "            for vad_tup in vad_list:\n",
    "                start = vad_tup[0]\n",
    "                end = vad_tup[1]\n",
    "                total_dur_10ms += (end-start)\n",
    "                total_dur_10ms_ge500ms += ((end - start) if (end-start) >= 50 else 0)\n",
    "                out_line = (\"%d %d\\n\" %(start, end))\n",
    "                vad_f.write(out_line)\n",
    "            # end for\n",
    "        # end looping over all segments\n",
    "    # end writing vad file\n",
    "    return total_dur_10ms, total_dur_10ms_ge500ms\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new directory for merged vads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uttr_fa_vads_path = config['es']['es_uttr_fa_vad']\n",
    "if not os.path.exists(uttr_fa_vads_path):\n",
    "    os.makedirs(uttr_fa_vads_path)\n",
    "\n",
    "merged_fa_vads_path = config['es']['es_merge_fa_vad']\n",
    "if not os.path.exists(merged_fa_vads_path):\n",
    "    os.makedirs(merged_fa_vads_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vad for 0 files id\n",
      "Created vad for 20 files id\n",
      "Created vad for 40 files id\n",
      "Created vad for 60 files id\n",
      "Created vad for 80 files id\n",
      "Created vad for 100 files id\n",
      "['12.704 hrs', '11.329 hrs']\n",
      "saving dict: ../has_500ms_fa_vad_dict.p\n"
     ]
    }
   ],
   "source": [
    "t1, t2, has_500ms_dur = create_uttr_vad_from_alignment(align_dict, uttr_fa_vads_path)\n",
    "# print(t1, t2)\n",
    "print(map(lambda t: \"{0:.3f} hrs\".format((t / 100.0 / 3600)), [t1, t2]))\n",
    "print(\"saving dict: %s\" % has_500ms_fa_vad_dict_fname)\n",
    "pickle.dump(has_500ms_dur, open(has_500ms_fa_vad_dict_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total utterances: 17394\n",
      "with 500ms: 14410\n"
     ]
    }
   ],
   "source": [
    "# check how many utterances have atleast 500ms VAD\n",
    "print(\"total utterances: %d\" % len(has_500ms_dur))\n",
    "uttrs_with_500ms = {k:v for k, v in has_500ms_dur.items() if v}\n",
    "print(\"with 500ms: %d\" % len(uttrs_with_500ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save dev file with only 500ms segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dev_500ms_fname = config['es']['mt_dev_500ms_files']\n",
    "dev_fname = config['es']['mt_dev_test_files']\n",
    "with open(dev_fname, \"r\") as dev_f, open(dev_500ms_fname, \"w\") as dev_500ms_f:\n",
    "    for line in dev_f:\n",
    "        if line.strip() in has_500ms_dur and has_500ms_dur[line.strip()]:\n",
    "            dev_500ms_f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2081  2081 16648 ../files-dev-500ms.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc $dev_500ms_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create merged vad for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_dur_10ms, total_dur_10ms_ge500ms = 0, 0\n",
    "for vad_file_id in segment_map:\n",
    "    t1, t2 = create_vad_from_alignment(vad_file_id, align_dict, segment_map, merged_fa_vads_path)\n",
    "    total_dur_10ms += t1\n",
    "    total_dur_10ms_ge500ms += t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(total_dur_10ms, total_dur_10ms_ge500ms)\n",
    "print(map(lambda t: \"{0:.3f}\".format((t / 100.0 / 3600)), [total_dur_10ms, total_dur_10ms_ge500ms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_gold_feats(align_dict, gold_feats_dict_fname, es_key=\"es\"):\n",
    "    gold_feats_dict = {}\n",
    "    for fid in align_dict:\n",
    "        for sid in align_dict[fid]:\n",
    "            gold_feats_dict[sid] = {}\n",
    "            if align_dict[fid][sid][es_key] == []:\n",
    "                # Only es_cnt can be empty, in which case include stop words\n",
    "                gold_feats_dict[sid] = [w.word for w in align_dict[fid][sid]['es']]\n",
    "            else:\n",
    "                gold_feats_dict[sid] = [w.word for w in align_dict[fid][sid][es_key]]\n",
    "    print(\"Saving gold features using key: %s\" % es_key)\n",
    "    pickle.dump(gold_feats_dict, open(gold_feats_dict_fname, \"wb\"))\n",
    "    print(\"finished ...\")\n",
    "    return gold_feats_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "align_dict = pickle.load(open(align_dict_fname, \"rb\"))\n",
    "gold_feats_dict_fname = config['es']['gold_feats']\n",
    "gold_feats_dict = create_gold_feats(align_dict, gold_feats_dict_fname, es_key=\"es_cnt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gold_feats_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-730d84beb1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# align_dict['001']['001.001']['es_cnt']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgold_feats_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'001.001'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gold_feats_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# align_dict['001']['001.001']['es_cnt']\n",
    "gold_feats_dict['001.001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "display(segment_map.keys()[:5])\n",
    "display(align_dict[\"001\"][\"001.001\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(segment_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(sorted(segment_map['001'].keys()+segment_map['002'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'../../../ZRTools/exp/callhome/files.lst'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config[\"es\"][\"lst_file\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check English translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Align(word='MECHITA', start=12, end=50),\n",
       " Align(word='WHAT', start=50, end=73),\n",
       " Align(word='SENT', start=129, end=169),\n",
       " Align(word='IT', start=126, end=129),\n",
       " Align(word='TO', start=169, end=176),\n",
       " Align(word='WHOM', start=176, end=192),\n",
       " Align(word='TO', start=192, end=198),\n",
       " Align(word='POCHO', start=198, end=225)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "align_dict['001']['001.001']['en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_words = [a.word for fid in align_dict for sid in align_dict[fid] for a in align_dict[fid][sid]['es']]\n",
    "es_cnt_words = [a.word for fid in align_dict for sid in align_dict[fid] for a in align_dict[fid][sid]['es_cnt']]\n",
    "en_words = [a.word for fid in align_dict for sid in align_dict[fid] for a in align_dict[fid][sid]['en']]\n",
    "en_cnt_words = [a.word for fid in align_dict for sid in align_dict[fid] for a in align_dict[fid][sid]['en_cnt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es_words_freq = Counter(es_words)\n",
    "es_cnt_words_freq = Counter(es_cnt_words)\n",
    "en_words_freq = Counter(en_words)\n",
    "en_cnt_words_freq = Counter(en_cnt_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('THE', 5178), ('AND', 4629), ('THAT', 4080), ('I', 3849), ('TO', 3359), ('YES', 3345), (\"'T\", 2265), ('YOU', 2256), ('NO', 2135), (\"'S\", 2030)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(en_words_freq.items(), reverse=True, key=lambda t:t[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('YES', 3345), (\"'T\", 2265), (\"'S\", 2030), ('WELL', 1829), ('AH', 1349), ('KNOW', 1100), ('OH', 1066), ('SEE', 934), ('YEAH', 904), ('LIKE', 889)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(en_cnt_words_freq.items(), reverse=True, key=lambda t:t[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('QUE', 7089), ('NO', 6110), ('Y', 5037), ('A', 4310), ('DE', 4009), ('S\\xc3\\xad', 3667), ('LA', 3425), ('YA', 2782), ('EL', 2680), ('ES', 2587)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(es_words_freq.items(), reverse=True, key=lambda t:t[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AH', 1831), ('PUES', 1236), ('BUENO', 1186), ('BIEN', 1183), ('SI', 1045), ('<LAUGH>', 987), ('MMM', 976), ('AS\\xc3\\xad', 781), ('ENTONCES', 775), ('CLARO', 729)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(es_cnt_words_freq.items(), reverse=True, key=lambda t:t[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"'T\", 2265), (\"'S\", 2030), (\"'R\", 1), (\"'D\", 34), (\"'M\", 627), (\"'TS\", 1), (\"'VE\", 184), (\"'\", 40), (\"'OEUVRES\", 1), (\"'RE\", 269), (\"'CLOCK\", 4), (\"'LL\", 540), (\"O'CLOCK\", 1), (\"'AM\", 8)]\n"
     ]
    }
   ],
   "source": [
    "print([(w,f) for w, f in en_cnt_words_freq.items() if \"'\" in w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<SNEEZE>', 4), ('<COUGH>', 11), ('<LAUGH>', 987), ('<BREATH>', 16), ('<NOISE>', 450), ('<BACKGROUND>', 107)]\n"
     ]
    }
   ],
   "source": [
    "print([(w,f) for w, f in es_words_freq.items() if \"<\" in w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<SNEEZE>', 4), ('<COUGH>', 11), ('<LAUGH>', 987), ('<BREATH>', 16), ('<NOISE>', 450), ('<BACKGROUND>', 107)]\n"
     ]
    }
   ],
   "source": [
    "print([(w,f) for w, f in es_cnt_words_freq.items() if \"<\" in w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
