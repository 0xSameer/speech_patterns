{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import cPickle as pickle\n",
    "import json\n",
    "import subprocess\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "from collections import namedtuple\n",
    "from itertools import izip\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"config.json\") as json_data_file:\n",
    "    config = json.load(json_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing CALLHOME for ZRTools\n",
    "\n",
    "\n",
    "- Created: 26-Oct-2016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mapping for start time for each segment\n",
    "\n",
    "Format: Dictionary  \n",
    "key: {key: value}  \n",
    "*file: {file.seg.wav: start time}*  \n",
    "Name: segment_start.dict, segment_start.txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_segments_file(seg_fname):\n",
    "    segment_map = {}\n",
    "    with open(seg_fname, \"r\") as seg_f:\n",
    "        for i, line in enumerate(seg_f):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            try:\n",
    "                line_items = line.strip().split()\n",
    "                seg_key = line_items[0]\n",
    "                file_id = line_items[1]\n",
    "                if file_id not in segment_map:\n",
    "                    segment_map[file_id] = {}\n",
    "                seg_start = int(float(line_items[6])*100)\n",
    "                segment_map[file_id][seg_key] = seg_start\n",
    "            except ValueError:\n",
    "                print(\"Incorrect line format at line: %d\" % i)\n",
    "    return segment_map\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read segment map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_segment_map():\n",
    "    segment_map = read_segments_file('../segments.txt')\n",
    "    pickle.dump(segment_map, open(config['es']['segment_dict_fname'], \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VAD files for merged wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_merged_vad_from_ed(vad_file_id, segment_map, seg_vad_path, merged_vad_path):\n",
    "    total_dur_10ms = 0\n",
    "    total_dur_10ms_ge500ms = 0\n",
    "    with open(os.path.join(merged_vad_path, vad_file_id+\".vad\"), \"w\") as vad_f:\n",
    "        print(\"creating vad %s ...\" % vad_file_id)\n",
    "        for i, (seg_id, seg_start) in enumerate(sorted(segment_map[vad_file_id].items(), key=lambda t:t[0])):\n",
    "            with open(os.path.join(seg_vad_path, seg_id+\".vad\"), \"r\") as seg_vad_f:\n",
    "                for line in seg_vad_f:\n",
    "                    line_items = map(int, line.strip().split())\n",
    "                    start = seg_start+line_items[0]\n",
    "                    end = seg_start+line_items[1]\n",
    "                    total_dur_10ms += (end-start)\n",
    "                    total_dur_10ms_ge500ms += ((end - start) if (end-start) >= 50 else 0)\n",
    "                    out_line = (\"%d %d\\n\" %(start, end))\n",
    "                    vad_f.write(out_line)\n",
    "                # end for\n",
    "            # end reading seg file\n",
    "        # end looping over all segments\n",
    "    # end writing vad file\n",
    "    return total_dur_10ms, total_dur_10ms_ge500ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new directory for merged vads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merged_ed_vads_path = \"../mergedVads\"\n",
    "# seg_vad_path = \"../vad\"\n",
    "# if not os.path.exists(merged_ed_vads_path):\n",
    "#     os.makedirs(merged_ed_vads_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create merged vad for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# total_dur_10ms, total_dur_10ms_ge500ms = 0, 0\n",
    "# for vad_file_id in segment_map:\n",
    "#     t1, t2 = create_merged_vad(vad_file_id, segment_map, seg_vad_path, merged_vads_path)\n",
    "#     total_dur_10ms += t1\n",
    "#     total_dur_10ms_ge500ms += t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(total_dur_10ms, total_dur_10ms_ge500ms)\n",
    "# print(map(lambda t: \"{0:.3f}\".format((t / 100.0 / 3600)), [total_dur_10ms, total_dur_10ms_ge500ms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_wavs_path = \"../mergeWavs\"\n",
    "plp_path = \"../plp\"\n",
    "plp_norm_path = \"../std_plp\"\n",
    "if not os.path.exists(plp_path):\n",
    "    os.makedirs(plp_path)\n",
    "if not os.path.exists(plp_norm_path):\n",
    "    os.makedirs(plp_norm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_file_lst(file_lst_fname):\n",
    "    prefix = \"../corpora/callhome/mergeWavs\"\n",
    "    wav_file_list = [os.path.join(prefix, wav_file) for \\\n",
    "                     wav_file in os.listdir(merged_wavs_path) if wav_file.endswith(\".wav\")]\n",
    "    wav_file_list_string = \"\\n\".join(wav_file_list)\n",
    "    with open(file_lst_fname, \"w\") as out_f:\n",
    "        out_f.write(wav_file_list_string)\n",
    "    print(\"Finished writing files.lst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create_file_lst(config[\"es\"][\"lst_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_plp(wav_fname, plp_fname):\n",
    "    FEACALC = config['base'][\"feacalc\"]\n",
    "    subprocess.call([FEACALC,\"-plp\", \\\n",
    "                    \"12\", \"-cep\", \"13\", \"-dom\", \"cep\", \"-deltaorder\", \\\n",
    "                    \"2\", \"-dither\", \"-frqaxis\", \"bark\", \"-samplerate\", \\\n",
    "                    \"8000\", \"-win\", \"25\", \"-step\", \"10\", \"-ip\", \\\n",
    "                    \"MSWAVE\", \"-rasta\", \"false\", \"-compress\", \\\n",
    "                    \"true\", \"-op\", \"swappedraw\", \"-o\", plp_fname, wav_fname])\n",
    "\n",
    "    \n",
    "def normalize_plp(plp_fname, vad_fname, plp_norm_fname):\n",
    "    STANDFEAT = config['base'][\"standfeat\"]\n",
    "    # Standardize binary file, for VAD regions only\n",
    "    subprocess.call([STANDFEAT, \"-D\", \"39\", \"-infile\", \\\n",
    "                    plp_fname, \"-outfile\", plp_norm_fname, \\\n",
    "                    \"-vadfile\", vad_fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_and_normalize_plps():\n",
    "    for i, file_id in enumerate(segment_map):\n",
    "        wav_fname = os.path.join(merged_wavs_path, file_id+\".wav\")\n",
    "        vad_fname = os.path.join(merged_fa_vads_path, file_id+\".vad\")\n",
    "        plp_fname = os.path.join(plp_path, file_id+\".binary\")\n",
    "        plp_norm_fname = os.path.join(plp_norm_path, file_id+\".std.binary\")\n",
    "\n",
    "        #print(file_id, wav_fname, vad_fname, plp_fname, plp_norm_fname)\n",
    "\n",
    "        # create PLP\n",
    "        if i % 20 == 0:\n",
    "            print(\"plp for file %s \" % file_id)\n",
    "\n",
    "        #if not os.path.exists(plp_fname):\n",
    "        create_plp(wav_fname, plp_fname)\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(\"normalizing plp %s\" % file_id)\n",
    "\n",
    "        #if not os.path.exists(plp_norm_fname):\n",
    "        normalize_plp(plp_fname, vad_fname, plp_norm_fname)\n",
    "    print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LSH files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsh_path = \"../lsh\"\n",
    "if not os.path.exists(lsh_path):\n",
    "    os.makedirs(lsh_path)\n",
    "lsh_proj_fname = os.path.join(lsh_path, \"proj_S64xD39_seed1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lsh_proj_file(lsh_proj_fname):\n",
    "    subprocess.call([config['base'][\"lsh_genproj\"], \\\n",
    "                     \"-D\",\"39\",\"-S\",\"64\",\"-seed\", \\\n",
    "                     \"1\",\"-projfile\", lsh_proj_fname])\n",
    "\n",
    "def create_lsh_file(plp_norm_fname, vad_fname, lsh_proj_fname, lsh_fname):\n",
    "    LSH = config['base'][\"lsh\"]\n",
    "    subprocess.call([LSH, \"-D\", \"39\", \"-S\", \"64\", \\\n",
    "                    \"-projfile\", lsh_proj_fname, \\\n",
    "                    \"-featfile\", plp_norm_fname, \"-sigfile\", \\\n",
    "                    lsh_fname, \"-vadfile\", vad_fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(lsh_proj_fname):\n",
    "    create_lsh_proj_file(lsh_proj_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_lsh_files():\n",
    "    for i, file_id in enumerate(segment_map):\n",
    "        wav_fname = os.path.join(merged_wavs_path, file_id+\".wav\")\n",
    "        vad_fname = os.path.join(merged_fa_vads_path, file_id+\".vad\")\n",
    "        plp_norm_fname = os.path.join(plp_norm_path, file_id+\".std.binary\")\n",
    "        lsh_fname = os.path.join(lsh_path, file_id+\".std.lsh64\")\n",
    "\n",
    "        #print(file_id, wav_fname, vad_fname, plp_fname, plp_norm_fname)\n",
    "\n",
    "        # create LSH\n",
    "        if i % 20 == 0:\n",
    "            print(\"lsh for file %s \" % file_id)\n",
    "\n",
    "        #if not os.path.exists(lsh_fname):\n",
    "        create_lsh_file(plp_norm_fname, vad_fname, lsh_proj_fname, lsh_fname)\n",
    "\n",
    "    print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ZRTools discovery command files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_path = '../exp'\n",
    "if not os.path.exists(exp_path):\n",
    "    os.makedirs(exp_path)\n",
    "\n",
    "# List of wav files\n",
    "segment_map = pickle.load(open(config['es']['segment_dict_fname'], \"rb\"))\n",
    "wav_file_list = sorted(segment_map.keys())\n",
    "exp_name = 'callhome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_files_base():\n",
    "    with open(os.path.join(exp_path, 'files.base'), \"w\") as out_f:\n",
    "        for wav_file in wav_file_list:\n",
    "            out_f.write(wav_file+'\\n')\n",
    "    print(\"Generated files.base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_discovery_cmd_scripts(exp_path, wav_file_list, exp_name, num_splits=1):\n",
    "    disc_file_split_base = \"disc_{0:d}.cmd\"\n",
    "    disc_file_split = os.path.join(exp_path, disc_file_split_base)\n",
    "    disc_split_file = os.path.join(exp_path, \"disc_split.txt\")\n",
    "    num_files = len(wav_file_list)\n",
    "    exp_local_path = os.path.join(\"exp\", exp_name)\n",
    "    cmd_string = \"scripts/plebdisc_filepair \\\"{0:s}\\\" \\\"{1:s}\\\" {2:s} 39\\n\"\n",
    "\n",
    "    total_lines = num_files * num_files\n",
    "    lines_per_file = total_lines // num_splits\n",
    "    smallfile = None\n",
    "    curr_line = 0\n",
    "    curr_file_num = 0\n",
    "\n",
    "    for i in xrange(num_files) :\n",
    "        if i % 20 == 0:\n",
    "            print(\"Progress: {0:d} out of: {1:d}\".format(curr_line+1, total_lines))\n",
    "        for j in xrange(num_files):\n",
    "            out_line = cmd_string.format(wav_file_list[i], \\\n",
    "                                              wav_file_list[j], \\\n",
    "                                              exp_local_path)\n",
    "            if curr_line % lines_per_file == 0:\n",
    "                if smallfile:\n",
    "                    smallfile.close()\n",
    "                small_filename = disc_file_split.format(curr_file_num)\n",
    "                smallfile = open(small_filename, \"w\")\n",
    "                curr_file_num += 1\n",
    "            smallfile.write(out_line)\n",
    "            curr_line += 1\n",
    "    if smallfile:\n",
    "        smallfile.close()\n",
    "\n",
    "    # Making a list of commands to execute the split disc list\n",
    "    full_split_cmd_string = \"nice sh {0:s} 1> {1:s} 2>{2:s} &\\n\"\n",
    "    split_cmd = os.path.join(exp_local_path, \"matches\",\"{0:s}.{1:d}\")\n",
    "    with open(disc_split_file, \"w\") as out_f:\n",
    "        for i in xrange(curr_file_num):\n",
    "            curr_split_file = os.path.join(exp_local_path, disc_file_split_base.format(i))\n",
    "            split_cmd_out = split_cmd.format(\"out\", i)\n",
    "            #split_cmd_err = split_cmd.format(\"err\", i)\n",
    "            split_cmd_err = \"/dev/null\"\n",
    "\n",
    "            out_line = \"nice sh \"\n",
    "            out_f.write(full_split_cmd_string.format(curr_split_file, \\\n",
    "                                                    split_cmd_out, \\\n",
    "                                                    split_cmd_err))\n",
    "\n",
    "    print(\"Completed - disc.cmd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create_discovery_cmd_scripts(exp_path=exp_path, wav_file_list=wav_file_list, exp_name=exp_name, num_splits=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read transcripts, and translations into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Align = namedtuple('Align', ['word', 'start', 'end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_alignment_file(align_fname, stopwords_corpus=None):\n",
    "    align_list = []\n",
    "    with open(align_fname, \"r\") as align_f:\n",
    "        for line in align_f:\n",
    "            line_items = line.strip().split()\n",
    "            if len(line_items) != 3:\n",
    "                raise ValueError\n",
    "            start, end = map(lambda v: int(float(v)*100), line_items[1:3])\n",
    "            if (not stopwords_corpus) or \\\n",
    "            (stopwords_corpus and line_items[0].lower().decode(\"utf-8\") not in stopwords_corpus):\n",
    "                align_list.append(Align(*[line_items[0], start, end]))\n",
    "    if sorted(align_list, key=lambda t: t.start) != align_list and not align_fname.endswith(\"en\"):\n",
    "        raise IOError    \n",
    "            \n",
    "    return align_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_words_path = '../wav2es-words/'\n",
    "en_words_path = '../wav2eng-words/'\n",
    "align_dict_fname = config['es']['align_dict_fname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # test code\n",
    "# stopwords_es = set(stopwords.words('spanish'))\n",
    "# stopwords_en = set(stopwords.words('english'))\n",
    "# display(read_alignment_file('../wav2es-words/001.001.es'))\n",
    "# display(read_alignment_file('../wav2es-words/001.001.es', stopwords_corpus=stopwords_es))\n",
    "# display(read_alignment_file('../wav2eng-words/001.001.en'))\n",
    "# display(read_alignment_file('../wav2eng-words/001.001.en', stopwords_corpus=stopwords_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_list(file_path, file_ext):\n",
    "    return [os.path.splitext(f)[0] for f in os.listdir(file_path) if f.endswith(file_ext)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "set(['009.025'])\n"
     ]
    }
   ],
   "source": [
    "es_file_list = get_file_list(es_words_path, 'es')\n",
    "en_file_list = get_file_list(en_words_path, 'en')\n",
    "\n",
    "print(sorted(es_file_list) == sorted(en_file_list))\n",
    "print(set(es_file_list)-set(en_file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_alignment_dict():\n",
    "    align_dict = {}\n",
    "    stopwords_es = set(stopwords.words('spanish'))\n",
    "    stopwords_en = set(stopwords.words('english'))\n",
    "    for file_id in segment_map:\n",
    "        #print(\"Processing file: %s\" % file_id)\n",
    "        align_dict[file_id] = {}\n",
    "        for seg_id in segment_map[file_id]:\n",
    "            align_dict[file_id][seg_id] = {}\n",
    "            es_fname = os.path.join(es_words_path, seg_id+\".es\")\n",
    "            en_fname = os.path.join(en_words_path, seg_id+\".en\")\n",
    "            align_dict[file_id][seg_id][\"es\"] = read_alignment_file(es_fname)\n",
    "            align_dict[file_id][seg_id][\"en\"] = read_alignment_file(en_fname)\n",
    "            align_dict[file_id][seg_id][\"es_cnt\"] = read_alignment_file(es_fname, stopwords_corpus=stopwords_es)\n",
    "            align_dict[file_id][seg_id][\"en_cnt\"] = read_alignment_file(en_fname, stopwords_corpus=stopwords_en)\n",
    "    return align_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# align_dict = create_alignment_dict()\n",
    "# pickle.dump(align_dict, open(align_dict_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VAD from alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "align_dict = pickle.load(open(align_dict_fname, \"rb\"))\n",
    "segment_map = pickle.load(open(config['es']['segment_dict_fname'], \"rb\"))\n",
    "# has_500ms_fa_vad_dict_fname = config['es']['has_500ms_fa_vad_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_uttr_vad_from_alignment(align_dict, vad_path):\n",
    "    has_500ms_dur = {}\n",
    "    total_dur_10ms = 0\n",
    "    total_dur_10ms_ge500ms = 0\n",
    "    for i, vad_file_id in enumerate(align_dict):\n",
    "        if i % 20 == 0:\n",
    "            print(\"Created vad for %d files id\" % i)\n",
    "        for seg_id in align_dict[vad_file_id]:\n",
    "            with open(os.path.join(vad_path, seg_id+\".vad\"), \"w\") as vad_f:\n",
    "                dur_10ms = 0\n",
    "                dur_10ms_ge500ms = 0\n",
    "                vad_list = []\n",
    "                # start index\n",
    "                s = 0\n",
    "                # create a local list of alignment values\n",
    "                align_list = align_dict[vad_file_id][seg_id]['es']\n",
    "                for j in xrange(len(align_list)):\n",
    "                    # if 1st or last element, add to vad_list\n",
    "                    if ((j+1) == len(align_list)) or (align_list[j].end != align_list[j+1].start):\n",
    "                        vad_list.append(((align_list[s].start), (align_list[j].end)))\n",
    "                        s=j+1\n",
    "                # write vad list to file        \n",
    "                for vad_tup in vad_list:\n",
    "                    start = vad_tup[0]\n",
    "                    end = vad_tup[1]\n",
    "                    dur_10ms += (end-start)\n",
    "                    dur_10ms_ge500ms += ((end - start) if (end-start) >= 50 else 0)\n",
    "                    out_line = (\"%d %d\\n\" %(start, end))\n",
    "                    vad_f.write(out_line)\n",
    "                \n",
    "                # set whether atleast one vad region of 500 ms\n",
    "                has_500ms_dur[seg_id] = (dur_10ms_ge500ms > 0)\n",
    "                # compute total durations\n",
    "                total_dur_10ms += dur_10ms\n",
    "                total_dur_10ms_ge500ms += dur_10ms_ge500ms \n",
    "                    \n",
    "            # end for\n",
    "        # end looping over all segments\n",
    "    # end writing vad file\n",
    "    return total_dur_10ms, total_dur_10ms_ge500ms, has_500ms_dur\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_merged_vad_from_alignment(vad_file_id, align_dict, segment_map, vad_path):\n",
    "    total_dur_10ms = 0\n",
    "    total_dur_10ms_ge500ms = 0\n",
    "    with open(os.path.join(vad_path, vad_file_id+\".vad\"), \"w\") as vad_f:\n",
    "        print(\"creating vad %s ...\" % vad_file_id)\n",
    "        for i, (seg_id, seg_start) in enumerate(sorted(segment_map[vad_file_id].items(), key=lambda t:t[0])):\n",
    "            vad_list = []\n",
    "            # start index\n",
    "            s = 0\n",
    "            # create a local list of alignment values\n",
    "            align_list = align_dict[vad_file_id][seg_id]['es']\n",
    "            for j in xrange(len(align_list)):\n",
    "                # if 1st or last element, add to vad_list\n",
    "                if ((j+1) == len(align_list)) or (align_list[j].end != align_list[j+1].start):\n",
    "                    vad_list.append(((seg_start+align_list[s].start), (seg_start+align_list[j].end)))\n",
    "                    s=j+1\n",
    "            # write vad list to file        \n",
    "            for vad_tup in vad_list:\n",
    "                start = vad_tup[0]\n",
    "                end = vad_tup[1]\n",
    "                total_dur_10ms += (end-start)\n",
    "                total_dur_10ms_ge500ms += ((end - start) if (end-start) >= 50 else 0)\n",
    "                out_line = (\"%d %d\\n\" %(start, end))\n",
    "                vad_f.write(out_line)\n",
    "            # end for\n",
    "        # end looping over all segments\n",
    "    # end writing vad file\n",
    "    return total_dur_10ms, total_dur_10ms_ge500ms\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new directory for merged vads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uttr_fa_vads_path = config['es']['es_uttr_fa_vad']\n",
    "if not os.path.exists(uttr_fa_vads_path):\n",
    "    os.makedirs(uttr_fa_vads_path)\n",
    "\n",
    "merged_fa_vads_path = config['es']['es_merge_fa_vad']\n",
    "if not os.path.exists(merged_fa_vads_path):\n",
    "    os.makedirs(merged_fa_vads_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# t1, t2, has_500ms_dur = create_uttr_vad_from_alignment(align_dict, uttr_fa_vads_path)\n",
    "# # print(t1, t2)\n",
    "# print(map(lambda t: \"{0:.3f} hrs\".format((t / 100.0 / 3600)), [t1, t2]))\n",
    "# print(\"saving dict: %s\" % has_500ms_fa_vad_dict_fname)\n",
    "# pickle.dump(has_500ms_dur, open(has_500ms_fa_vad_dict_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # check how many utterances have atleast 500ms VAD\n",
    "# print(\"total utterances: %d\" % len(has_500ms_dur))\n",
    "# uttrs_with_500ms = {k:v for k, v in has_500ms_dur.items() if v}\n",
    "# print(\"with 500ms: %d\" % len(uttrs_with_500ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save dev file with only 500ms segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dev_500ms_fname = config['es']['mt_dev_500ms_files']\n",
    "# dev_fname = config['es']['mt_dev_test_files']\n",
    "# with open(dev_fname, \"r\") as dev_f, open(dev_500ms_fname, \"w\") as dev_500ms_f:\n",
    "#     for line in dev_f:\n",
    "#         if line.strip() in has_500ms_dur and has_500ms_dur[line.strip()]:\n",
    "#             dev_500ms_f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !wc $dev_500ms_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create merged vad for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_merged_vads():\n",
    "    total_dur_10ms, total_dur_10ms_ge500ms = 0, 0\n",
    "    for vad_file_id in segment_map:\n",
    "        t1, t2 = create_vad_from_alignment(vad_file_id, align_dict, segment_map, merged_fa_vads_path)\n",
    "        total_dur_10ms += t1\n",
    "        total_dur_10ms_ge500ms += t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(total_dur_10ms, total_dur_10ms_ge500ms)\n",
    "# print(map(lambda t: \"{0:.3f}\".format((t / 100.0 / 3600)), [total_dur_10ms, total_dur_10ms_ge500ms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_gold_feats(align_dict, gold_feats_dict_fname, es_key=\"es\"):\n",
    "    gold_feats_dict = {}\n",
    "    for fid in align_dict:\n",
    "        for sid in align_dict[fid]:\n",
    "            gold_feats_dict[sid] = {}\n",
    "            if align_dict[fid][sid][es_key] == []:\n",
    "                # Only es_cnt can be empty, in which case include stop words\n",
    "                gold_feats_dict[sid] = [w.word for w in align_dict[fid][sid]['es']]\n",
    "            else:\n",
    "                gold_feats_dict[sid] = [w.word for w in align_dict[fid][sid][es_key]]\n",
    "    print(\"Saving gold features using key: %s\" % es_key)\n",
    "    pickle.dump(gold_feats_dict, open(gold_feats_dict_fname, \"wb\"))\n",
    "    print(\"finished ...\")\n",
    "    return gold_feats_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gold_feats():\n",
    "    align_dict = pickle.load(open(align_dict_fname, \"rb\"))\n",
    "    gold_feats_dict_fname = config['es']['gold_feats']\n",
    "    gold_feats_dict = create_gold_feats(align_dict, gold_feats_dict_fname, es_key=\"es_cnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check English translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_words = [a.word for fid in align_dict for sid in align_dict[fid] for a in align_dict[fid][sid]['es']]\n",
    "es_cnt_words = [a.word for fid in align_dict for sid in align_dict[fid] for a in align_dict[fid][sid]['es_cnt']]\n",
    "en_words = [a.word for fid in align_dict for sid in align_dict[fid] for a in align_dict[fid][sid]['en']]\n",
    "en_cnt_words = [a.word for fid in align_dict for sid in align_dict[fid] for a in align_dict[fid][sid]['en_cnt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es_words_freq = Counter(es_words)\n",
    "es_cnt_words_freq = Counter(es_cnt_words)\n",
    "en_words_freq = Counter(en_words)\n",
    "en_cnt_words_freq = Counter(en_cnt_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('THE', 5178), ('AND', 4629), ('THAT', 4080), ('I', 3849), ('TO', 3359), ('YES', 3345), (\"'T\", 2265), ('YOU', 2256), ('NO', 2135), (\"'S\", 2030)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(en_words_freq.items(), reverse=True, key=lambda t:t[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('YES', 3345), (\"'T\", 2265), (\"'S\", 2030), ('WELL', 1829), ('AH', 1349), ('KNOW', 1100), ('OH', 1066), ('SEE', 934), ('YEAH', 904), ('LIKE', 889)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(en_cnt_words_freq.items(), reverse=True, key=lambda t:t[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('QUE', 7089), ('NO', 6110), ('Y', 5037), ('A', 4310), ('DE', 4009), ('S\\xc3\\xad', 3667), ('LA', 3425), ('YA', 2782), ('EL', 2680), ('ES', 2587)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(es_words_freq.items(), reverse=True, key=lambda t:t[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AH', 1831), ('PUES', 1236), ('BUENO', 1186), ('BIEN', 1183), ('SI', 1045), ('<LAUGH>', 987), ('MMM', 976), ('AS\\xc3\\xad', 781), ('ENTONCES', 775), ('CLARO', 729)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(es_cnt_words_freq.items(), reverse=True, key=lambda t:t[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"'T\", 2265), (\"'S\", 2030), (\"'R\", 1), (\"'D\", 34), (\"'M\", 627), (\"'TS\", 1), (\"'VE\", 184), (\"'\", 40), (\"'OEUVRES\", 1), (\"'RE\", 269), (\"'CLOCK\", 4), (\"'LL\", 540), (\"O'CLOCK\", 1), (\"'AM\", 8)]\n"
     ]
    }
   ],
   "source": [
    "print([(w,f) for w, f in en_cnt_words_freq.items() if \"'\" in w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<SNEEZE>', 4), ('<COUGH>', 11), ('<LAUGH>', 987), ('<BREATH>', 16), ('<NOISE>', 450), ('<BACKGROUND>', 107)]\n"
     ]
    }
   ],
   "source": [
    "print([(w,f) for w, f in es_words_freq.items() if \"<\" in w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<SNEEZE>', 4), ('<COUGH>', 11), ('<LAUGH>', 987), ('<BREATH>', 16), ('<NOISE>', 450), ('<BACKGROUND>', 107)]\n"
     ]
    }
   ],
   "source": [
    "print([(w,f) for w, f in es_cnt_words_freq.items() if \"<\" in w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Key-word spotting, prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_proto_list():\n",
    "    protos = []\n",
    "    with open(config['proto']['protos_list'], \"r\") as f:\n",
    "        for line in f:\n",
    "            protos.append(line.strip())\n",
    "    return protos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_vad_norm_plp_for_protos(protos):\n",
    "    if not os.path.exists(config['proto']['vad_path']):\n",
    "        os.makedirs(config['proto']['vad_path'])\n",
    "    if not os.path.exists(config['proto']['norm_plp_path']):\n",
    "        os.makedirs(config['proto']['norm_plp_path'])\n",
    "    if not os.path.exists(config['proto']['proto_float32_path']):\n",
    "        os.makedirs(config['proto']['proto_float32_path'])\n",
    "    if not os.path.exists(config['proto']['lsh_path']):\n",
    "        os.makedirs(config['proto']['lsh_path'])\n",
    "        \n",
    "    sys.stderr.flush()\n",
    "    with tqdm(total=len(protos)) as pbar:\n",
    "        for i, pid in enumerate(protos, start=1):\n",
    "            try:\n",
    "                pid_base = os.path.splitext(pid)[0]\n",
    "                plp_fname = os.path.join(config['proto']['proto_path'], pid)\n",
    "                plp_f32_fname = os.path.join(config['proto']['proto_float32_path'], pid)\n",
    "                plp_norm_fname = (os.path.join(config['proto']['norm_plp_path'], \n",
    "                                          \"{0:s}.std.binary\".format(pid_base)))\n",
    "                vad_fname = (os.path.join(config['proto']['vad_path'], \"{0:s}.vad\".format(pid_base)))\n",
    "                lsh_fname = (os.path.join(config['proto']['lsh_path'], \n",
    "                                          \"{0:s}.std.lsh64\".format(pid_base)))\n",
    "                # read npy file to get shape\n",
    "                x = np.load(plp_fname)\n",
    "                # ZRTools use float32\n",
    "                y = x.astype(np.float32)\n",
    "                y.tofile(plp_f32_fname)\n",
    "\n",
    "                # create vad file\n",
    "                with open(vad_fname, \"w\") as f:\n",
    "                    f.write(\"0\\t{0:d}\\n\".format(y.shape[0]))\n",
    "\n",
    "                # normalize plp\n",
    "                normalize_plp(plp_f32_fname, vad_fname, plp_norm_fname)\n",
    "                #print(os.path.getsize(plp_fname))\n",
    "\n",
    "                # create lsh\n",
    "                create_lsh_file(plp_norm_fname, vad_fname, lsh_proj_fname, lsh_fname)\n",
    "                \n",
    "                # update progress\n",
    "                pbar.set_description(\"processing proto {0:s}\\n\".format(pid_base))\n",
    "#                 pbar.update(i)\n",
    "                print(i)\n",
    "            except:\n",
    "                print(pid)\n",
    "    print(\"completed\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10TH.npy', '13.npy', '155.npy', '20TH.npy', '5TH.npy']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protos = read_proto_list()\n",
    "protos[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto 155\n",
      "processing proto ABEL\n",
      "processing proto ABSOLUTELY\n",
      "processing proto ABUSED\n",
      "processing proto ACCELERATING\n",
      "processing proto ACCESSIBLE\n",
      "processing proto ACCORDING\n",
      "processing proto ACCUSING\n",
      "processing proto ACTED\n",
      "processing proto ACTRESS\n",
      "processing proto ADDRESSES\n",
      "processing proto ADOBE\n",
      "processing proto ADRIAN\n",
      "processing proto ADVANTAGEMM\n",
      "processing proto ADVISER\n",
      "processing proto AFFECTED\n",
      "processing proto AFTERNOONS\n",
      "processing proto AGARRAPO\n",
      "processing proto AGENT\n",
      "processing proto AGREEMENT\n",
      "processing proto AHA\n",
      "processing proto AHH\n",
      "processing proto AIRES\n",
      "processing proto ALARM\n",
      "processing proto ALEXICO\n",
      "processing proto ALIKE\n",
      "processing proto ALLERGY\n",
      "processing proto AL\n",
      "processing proto ALSO\n",
      "processing proto ALWAYS\n",
      "processing proto AMAZES\n",
      "processing proto AMENDED\n",
      "processing proto AMINTITA\n",
      "processing proto AMUSES\n",
      "processing proto ANDALUCIA\n",
      "processing proto ANGELES\n",
      "processing proto ANISSA\n",
      "processing proto ANNULLEDH\n",
      "processing proto A\n",
      "processing proto ANTENOR\n",
      "processing proto ANTIDEPRESSANTS\n",
      "processing proto ANTO\n",
      "processing proto ANYONE\n",
      "processing proto APART\n",
      "processing proto APPEARS\n",
      "processing proto APPOINTMENT\n",
      "processing proto ARAOS\n",
      "processing proto AREAS\n",
      "processing proto ARGUE\n",
      "processing proto ARM\n",
      "processing proto ARRANGE\n",
      "processing proto ARTICLE\n",
      "processing proto ASKING\n",
      "processing proto ASSHOLES\n",
      "processing proto ASUNCI\n",
      "processing proto ATTACHED\n",
      "processing proto ATTENTIVE\n",
      "processing proto AUG\n",
      "processing proto AUSTRIA\n",
      "processing proto AVAILABLE\n",
      "processing proto AWARE\n",
      "processing proto AYLLAPU\n",
      "processing proto BACKED\n",
      "processing proto BAD\n",
      "processing proto BALL\n",
      "processing proto BANGS\n",
      "processing proto BARBIES\n",
      "processing proto BARRANCO\n",
      "processing proto BASKETS\n",
      "processing proto BATHROOM\n",
      "processing proto BEATING\n",
      "processing proto BED\n",
      "processing proto BEFORE\n",
      "processing proto BEGUN\n",
      "processing proto BELGIAN\n",
      "processing proto BELONG\n",
      "processing proto BENJAMIN\n",
      "processing proto BESIDES\n",
      "processing proto BETTY\n",
      "processing proto BICYCLE\n",
      "processing proto BILL\n",
      "processing proto BIRTHDAY\n",
      "processing proto BIT\n",
      "processing proto BLAH\n",
      "processing proto BLIND\n",
      "processing proto BLOUSE\n",
      "processing proto BODY\n",
      "processing proto BOLUDA\n",
      "processing proto BONUSES\n",
      "processing proto BORE\n",
      "processing proto BOTHERED\n",
      "processing proto BOTTOM\n",
      "processing proto BOY\n",
      "processing proto BRASIL\n",
      "processing proto BREAD\n",
      "processing proto BRED\n",
      "processing proto BRIEFLY\n",
      "processing proto BRIO\n",
      "processing proto BROOK\n",
      "processing proto BRUNELA\n",
      "processing proto BUDGET\n",
      "processing proto BULK\n",
      "processing proto BUNK\n",
      "processing proto BURNT\n",
      "processing proto BUSTOS\n",
      "processing proto BUY\n",
      "processing proto CACERES\n",
      "processing proto CALCULATIONS\n",
      "processing proto CALLED\n",
      "processing proto CALM\n",
      "processing proto CAMIRI\n",
      "processing proto CANCER\n",
      "processing proto CAN\n",
      "processing proto CAPITAL\n",
      "processing proto CARDIOLOGIST\n",
      "processing proto CARE\n",
      "processing proto CARLA\n",
      "processing proto CARMENCITA\n",
      "processing proto CAROLINA\n",
      "processing proto CARTS\n",
      "processing proto CASTILIAN\n",
      "processing proto CATARACTS\n",
      ": 325229it [00:13, 49145.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASTAñEDA.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto CATS\n",
      "processing proto CECI\n",
      "processing proto CELEBRATION\n",
      "processing proto CENTIMETER\n",
      "processing proto CEREAL\n",
      "processing proto CESAREAN\n",
      "processing proto CHAMA\n",
      "processing proto CHANGING\n",
      "processing proto CHARGED\n",
      "processing proto CHAT\n",
      "processing proto CHEAPER\n",
      "processing proto CHECHO\n",
      "processing proto CHEER\n",
      "processing proto CHERLA\n",
      "processing proto CHICKPEAS\n",
      "processing proto CHILEANS\n",
      "processing proto CHIN\n",
      "processing proto CHIVETO\n",
      "processing proto CHOITE\n",
      "processing proto CHOSEN\n",
      "processing proto CHUCHA\n",
      "processing proto CINCINATTI\n",
      "processing proto CIVIL\n",
      "processing proto CLASS\n",
      "processing proto CLEARER\n",
      "processing proto CLIMBED\n",
      "processing proto CLOCK\n",
      "processing proto CLOTHING\n",
      "processing proto COBS\n",
      "processing proto CODE\n",
      "processing proto COINCIDENTALLY\n",
      "processing proto COLLEGE\n",
      "processing proto COLONY\n",
      "processing proto COME\n",
      "processing proto COMMENT\n",
      "processing proto COMMITTEE\n",
      "processing proto COMMUNICATIVE\n",
      "processing proto COMPATIBLE\n",
      "processing proto COMPLEX\n",
      "processing proto COMPUTER\n",
      "processing proto CONCENTRATION\n",
      "processing proto CONCISION\n",
      "processing proto CONDITIONS\n",
      "processing proto CONFIDENT\n",
      "processing proto CONFRONTED\n",
      "processing proto CONGRESS\n",
      "processing proto CON\n",
      "processing proto CONSIDERATION\n",
      "processing proto CONSTANZA\n",
      "processing proto CONSUL\n",
      "processing proto CONTAMINATE\n",
      "processing proto CONTINUED\n",
      "processing proto CONTRADICTORY\n",
      "processing proto CONVERSION\n",
      "processing proto CONVULSION\n",
      "processing proto COOLER\n",
      "processing proto COPENHAGEN\n",
      "processing proto CORN\n",
      "processing proto COSTA\n",
      "processing proto COULD\n",
      "processing proto COUNTRY\n",
      "processing proto COURTS\n",
      "processing proto COW\n",
      "processing proto CRAZY\n",
      "processing proto CRIMES\n",
      "processing proto CRITTERS\n",
      "processing proto CROSS\n",
      "processing proto CUA\n",
      "processing proto CUCUTA\n",
      "processing proto CURE\n",
      "processing proto CURUES\n",
      "processing proto CUZCO\n",
      "processing proto DALLAS\n",
      "processing proto DANES\n",
      "processing proto DARKSKINNED\n",
      "processing proto DATING\n",
      "processing proto DAYS\n",
      "processing proto DEAR\n",
      "processing proto DECENT\n",
      "processing proto DECLARATION\n",
      "processing proto DEEP\n",
      "processing proto DEFINITELY\n",
      "processing proto DELAYED\n",
      "processing proto DELIGHTFUL\n",
      "processing proto DENISE\n",
      "processing proto DENTURES\n",
      "processing proto DEPOSIT\n",
      "processing proto DESERVED\n",
      "processing proto DESPAIR\n",
      "processing proto DETAIL\n",
      "processing proto DEVICES\n",
      "processing proto DIAGNOSED\n",
      "processing proto DIANI\n",
      "processing proto DICTIONARY\n",
      "processing proto DIFFERENCE\n",
      "processing proto DINE\n",
      "processing proto DIRT\n",
      "processing proto DISCHARGED\n",
      "processing proto DISCRIMINATION\n",
      "processing proto DISKS\n",
      "processing proto DISTRACT\n",
      "processing proto DISTRICT\n",
      "processing proto DIVINA\n",
      "processing proto D\n",
      "processing proto DOCTORS\n",
      "processing proto DOES\n",
      "processing proto DOLLS\n",
      "processing proto DONATED\n",
      "processing proto DOODLING\n",
      "processing proto DOUBLE\n",
      ": 1210569it [00:26, 95276.21it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOñIHUE.npy\n",
      "DOñINHUE.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto DOZEN\n",
      "processing proto DRAMATIC\n",
      "processing proto DREAMING\n",
      "processing proto DRESS\n",
      "processing proto DRIVERS\n",
      "processing proto DROPPED\n",
      "processing proto DRYING\n",
      "processing proto DUMB\n",
      "processing proto DUST\n",
      "processing proto EARNING\n",
      "processing proto EARTH\n",
      "processing proto EASY\n",
      "processing proto ECHO\n",
      "processing proto ECUADOR\n",
      "processing proto EEEEE\n",
      "processing proto EGGS\n",
      "processing proto EITHER\n",
      "processing proto ELECTRIC\n",
      "processing proto ELENITA\n",
      "processing proto ELISA\n",
      "processing proto EMBARRASSING\n",
      "processing proto EMPLOYMENT\n",
      "processing proto ENDED\n",
      "processing proto ENERGY\n",
      "processing proto ENIA\n",
      "processing proto E\n",
      "processing proto ENTER\n",
      "processing proto ENVELOP\n",
      "processing proto EQUIPPING\n",
      "processing proto ES\n",
      "processing proto ESTABLISHMENT\n",
      "processing proto ETCETERA\n",
      "processing proto EVEN\n",
      "processing proto EVERYONE\n",
      "processing proto EXACT\n",
      "processing proto EXCEED\n",
      "processing proto EXCUSE\n",
      "processing proto EXODUS\n",
      "processing proto EXPERIENCE\n",
      "processing proto EXPLAINING\n",
      "processing proto EXPRESIDENT\n",
      "processing proto EXTRADITED\n",
      "processing proto EYESIGHT\n",
      "processing proto FACE\n",
      "processing proto FAHRENHEIT\n",
      "processing proto FALKLANDS\n",
      "processing proto FAMILIES\n",
      "processing proto FANS\n",
      "processing proto FAR\n",
      "processing proto FATE\n",
      "processing proto FAVORING\n",
      "processing proto FEDERAEH\n",
      "processing proto FELIPE\n",
      "processing proto FERNAND\n",
      "processing proto FEW\n",
      ": 1859537it [00:32, 120939.07it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FERNáNDEZ.npy\n",
      "FERNáN.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto FIFTH\n",
      "processing proto FIGURES\n",
      "processing proto FINALLY\n",
      "processing proto FINCA\n",
      "processing proto FINISHES\n",
      "processing proto FIRM\n",
      "processing proto FITS\n",
      "processing proto FLAT\n",
      "processing proto FLOOR\n",
      "processing proto FLUENT\n",
      "processing proto FOCUSED\n",
      "processing proto FOOD\n",
      "processing proto FORCES\n",
      "processing proto FORGET\n",
      "processing proto FORMED\n",
      "processing proto FORTUNE\n",
      "processing proto FOUR\n",
      "processing proto FRANCINA\n",
      "processing proto FREAK\n",
      "processing proto FREEWAY\n",
      "processing proto FRIACHO\n",
      "processing proto FRIENDS\n",
      "processing proto FROM\n",
      "processing proto FUCKING\n",
      "processing proto FULFILLED\n",
      "processing proto FUNDAMENTAL\n",
      "processing proto FURROW\n",
      "processing proto GABRIEL\n",
      "processing proto GAME\n",
      "processing proto GAS\n",
      "processing proto GAY\n",
      "processing proto GENERATED\n",
      "processing proto GEORGE\n",
      "processing proto GERMANY\n",
      "processing proto GIFTS\n",
      "processing proto GIRLS\n",
      "processing proto GLASSES\n",
      "processing proto GLUTTON\n",
      "processing proto GOES\n",
      "processing proto GONG\n",
      "processing proto GORDA\n",
      "processing proto GOTTA\n",
      "processing proto GRABBED\n",
      "processing proto GRADUATED\n",
      "processing proto GRANDCHILDREN\n",
      "processing proto GRANDSON\n",
      "processing proto GREECE\n",
      "processing proto GREW\n",
      "processing proto GROUND\n",
      "processing proto GROWS\n",
      "processing proto GUARANTEE\n",
      "processing proto GUERRERO\n",
      "processing proto GUITO\n",
      "processing proto GYMNASTIC\n",
      "processing proto HAIR\n",
      "processing proto HANDLE\n",
      "processing proto HAPPENED\n",
      "processing proto HAPPY\n",
      "processing proto HARP\n",
      "processing proto HAVEN\n",
      "processing proto HEADBAND\n",
      "processing proto HEARING\n",
      "processing proto HEAVY\n",
      "processing proto HELPED\n",
      "processing proto HE\n",
      "processing proto HERSELF\n",
      "processing proto HIGHSCHOOL\n",
      ": 2861478it [00:40, 141788.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERNáNDEZ.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto HIPNOPEDIA\n",
      "processing proto HIS\n",
      "processing proto HMM\n",
      "processing proto HOLIDAY\n",
      "processing proto HONE\n",
      "processing proto HON\n",
      "processing proto HOPEFULLY\n",
      "processing proto HOSPITAL\n",
      "processing proto HOT\n",
      "processing proto HOUSEWIFE\n",
      "processing proto HT\n",
      "processing proto HUGE\n",
      "processing proto HUMANS\n",
      ": 3066323it [00:41, 148482.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUARáS.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto HURST\n",
      "processing proto HYPNOPAEDIA\n",
      "processing proto IDEAL\n",
      "processing proto IDIOTS\n",
      "processing proto IGUEY\n",
      "processing proto IL\n",
      ": 3171436it [00:42, 152179.82it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IGUAZú.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto IMMEDIATE\n",
      "processing proto IMPORTANT\n",
      "processing proto IMPROVE\n",
      "processing proto INCAE\n",
      "processing proto INCREASED\n",
      "processing proto INDEPENDENT\n",
      "processing proto INEVITABLE\n",
      "processing proto INFORMALLY\n",
      "processing proto INGRATE\n",
      "processing proto INLAWS\n",
      "processing proto INSIDE\n",
      "processing proto INSTALL\n",
      "processing proto INSURANCE\n",
      "processing proto INTERCHANGE\n",
      "processing proto INTERNAL\n",
      "processing proto INTERPRETATION\n",
      "processing proto INT\n",
      "processing proto INVENTED\n",
      "processing proto INVITATION\n",
      "processing proto INVOLVES\n",
      "processing proto IRIS\n",
      "processing proto ISLAND\n",
      "processing proto ITALIA\n",
      "processing proto ITS\n",
      "processing proto JACKASS\n",
      "processing proto JAIL\n",
      ": 3612367it [00:45, 151230.85it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IVáN.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto JAPAN\n",
      "processing proto JEDY\n",
      "processing proto JESSICA\n",
      "processing proto J\n",
      "processing proto JOIE\n",
      "processing proto JO\n",
      "processing proto JOYA\n",
      "processing proto JUDGE\n",
      ": 3756590it [00:46, 162249.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOSé.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto JUMPED\n",
      "processing proto JURISPRUDENCE\n",
      "processing proto KAREN\n",
      "processing proto KENNEDY\n",
      "processing proto KICKBACK\n",
      "processing proto KIDNAP\n",
      "processing proto KILLING\n",
      "processing proto KINDA\n",
      "processing proto KISS\n",
      "processing proto KNEW\n",
      "processing proto KNOWS\n",
      "processing proto LABORATORY\n",
      "processing proto LAID\n",
      "processing proto LAND\n",
      "processing proto LAS\n",
      "processing proto LATINA\n",
      ": 4053557it [00:48, 173792.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LARRAñAGA.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto LAUNDRY\n",
      "processing proto LAYING\n",
      "processing proto LEAFLETS\n",
      "processing proto LEASE\n",
      "processing proto LEGALLY\n",
      "processing proto LENCHA\n",
      "processing proto LESSEN\n",
      "processing proto LETTERS\n",
      "processing proto LIAR\n",
      "processing proto LIED\n",
      "processing proto LIGHTNING\n",
      "processing proto LIKEWISE\n",
      "processing proto LINEL\n",
      "processing proto LION\n",
      "processing proto LISTENING\n",
      "processing proto LITTLE\n",
      "processing proto LLA\n",
      "processing proto LOAN\n",
      "processing proto LOGICAL\n",
      "processing proto LOOKED\n",
      "processing proto LOOSES\n",
      "processing proto LOSS\n",
      "processing proto LOVELY\n",
      "processing proto LOWER\n",
      "processing proto LUCKILY\n",
      "processing proto LULITO\n",
      "processing proto MACAMAN\n",
      "processing proto MADHOUSE\n",
      "processing proto MAGAZINE\n",
      "processing proto MAIDS\n",
      "processing proto MAINTAINS\n",
      "processing proto MAKING\n",
      "processing proto MAMMY\n",
      "processing proto MANDATORY\n",
      "processing proto MANOLITO\n",
      "processing proto MANU\n",
      "processing proto MARCH\n",
      "processing proto MARIBEL\n",
      ": 4848510it [00:52, 189504.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MARAñON.npy\n",
      "MARíA.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto MARIO\n",
      "processing proto MARKS\n",
      "processing proto MARQUI\n",
      "processing proto MARTINA\n",
      "processing proto MASTERS\n",
      "processing proto MATES\n",
      "processing proto MATTER\n",
      "processing proto MAURO\n",
      "processing proto MAZATLAN\n",
      "processing proto MEANWHILE\n",
      "processing proto MEDELLIN\n",
      "processing proto MEETING\n",
      "processing proto MEMORIZED\n",
      "processing proto ME\n",
      ": 5159426it [00:54, 194190.16it/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MéLIDA.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto MERENGUE\n",
      "processing proto MESSAGES\n",
      "processing proto METEOROLOGY\n",
      "processing proto MEXICAN\n",
      "processing proto MIAS\n",
      "processing proto MICROSURGERY\n",
      "processing proto MIGUEL\n",
      "processing proto MILKMAN\n",
      "processing proto MINA\n",
      "processing proto MINISERIES\n",
      "processing proto MINUTES\n",
      "processing proto MISSES\n",
      "processing proto MISTRESS\n",
      ": 5436961it [00:55, 203092.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISIóN.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto M\n",
      "processing proto MODEST\n",
      "processing proto MOMENT\n",
      "processing proto MONDAYS\n",
      "processing proto MONTECINOS\n",
      "processing proto MOOD\n",
      ": 5573462it [00:56, 203674.05it/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MóNICA.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto MORMON\n",
      "processing proto MOTA\n",
      "processing proto MOVED\n",
      "processing proto MOVING\n",
      "processing proto MUNDIAL\n",
      "processing proto MURDERER\n",
      "processing proto MY\n",
      "processing proto NAMED\n",
      "processing proto NANNY\n",
      "processing proto NATALIA\n",
      "processing proto NATUSHA\n",
      "processing proto NEED\n",
      "processing proto NEGRURA\n",
      "processing proto NENA\n",
      "processing proto NEUROLOGIST\n",
      "processing proto NEWS\n",
      "processing proto NIECE\n",
      "processing proto NINA\n",
      ": 6000095it [00:58, 214340.98it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NICOLáS.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto NINTITA\n",
      "processing proto NONA\n",
      "processing proto NORA\n",
      "processing proto NOSAS\n",
      "processing proto NOTES\n",
      "processing proto NOTROPIL\n",
      "processing proto NOYES\n",
      "processing proto NUM\n",
      ": 6196374it [00:59, 218710.33it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "´.npy\n",
      "¨.npy\n",
      "¡.npy\n",
      "​.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto NYLON\n",
      "processing proto OBLIGATORY\n",
      "processing proto OBVIOUS\n",
      "processing proto OCULAR\n",
      ": 6295794it [00:59, 221957.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCAñA.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto OFFENDED\n",
      "processing proto OFFICES\n",
      "processing proto OHHH\n",
      "processing proto OLDER\n",
      "processing proto OMARIA\n",
      "processing proto ONES\n",
      "processing proto OPELUCE\n",
      "processing proto OPERABLE\n",
      "processing proto OPINIONS\n",
      "processing proto OPTIONS\n",
      "processing proto ORGANIZATION\n",
      "processing proto ORLANDO\n",
      "processing proto OSO\n",
      "processing proto OTTAWA\n",
      "processing proto OUTFIT\n",
      "processing proto OVERDID\n",
      "processing proto OVERWHELMS\n",
      "processing proto OYE\n",
      "processing proto PACK\n",
      "processing proto PAINS\n",
      "processing proto PALACE\n",
      "processing proto PAN\n",
      "processing proto PAPER\n",
      "processing proto PARAGRAPHS\n",
      "processing proto PARISH\n",
      "processing proto PARTICULAR\n",
      "processing proto PASADENA\n",
      "processing proto PASSPORT\n",
      "processing proto PATO\n",
      "processing proto PAULA\n",
      "processing proto PAYMENT\n",
      "processing proto PEACE\n",
      "processing proto PEDESTRIAN\n",
      "processing proto PEGUELE\n",
      "processing proto PENDING\n",
      "processing proto PEPE\n",
      "processing proto PERFUME\n",
      "processing proto PERMIT\n",
      "processing proto PERSON\n",
      "processing proto PERVERT\n",
      "processing proto PHARMACIES\n",
      "processing proto PHONE\n",
      "processing proto PHOTOS\n",
      "processing proto PIANIST\n",
      "processing proto PICTURE\n",
      "processing proto PILGRIMAGE\n",
      "processing proto PIMPLE\n",
      "processing proto PITY\n",
      "processing proto PLANNED\n",
      "processing proto PLATELETS\n",
      "processing proto PLAYS\n",
      "processing proto PLEASURE\n",
      "processing proto PODS\n",
      "processing proto POINT\n",
      "processing proto POLITICAL\n",
      "processing proto POOLS\n",
      "processing proto POPY\n",
      "processing proto PORTUGUESE\n",
      "processing proto POSSIBLY\n",
      "processing proto POST\n",
      "processing proto POUT\n",
      "processing proto PRACTICE\n",
      "processing proto PRECIOUS\n",
      "processing proto PREGNANT\n",
      "processing proto PREPARING\n",
      "processing proto PRESIDENTIAL\n",
      "processing proto PREVIOUSLY\n",
      "processing proto PRICKS\n",
      "processing proto PRINTING\n",
      "processing proto PROBABLY\n",
      "processing proto PRODUCES\n",
      "processing proto PROFESSORS\n",
      "processing proto PROGRESSIVE\n",
      "processing proto PROMONCADE\n",
      "processing proto PROOFS\n",
      "processing proto PROPOSE\n",
      "processing proto PROVERB\n",
      "processing proto PSYCHE\n",
      ": 8309595it [01:08, 249335.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protos.list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto PSYCHOSOMATIC\n",
      "processing proto PUCHAS\n",
      "processing proto PULL\n",
      "processing proto PURCHASES\n",
      "processing proto PUSHY\n",
      "processing proto QUALITY\n",
      "processing proto QUESTION\n",
      "processing proto QUIETER\n",
      "processing proto QUIT\n",
      "processing proto RADIO\n",
      "processing proto RAISE\n",
      "processing proto RAMON\n",
      "processing proto RARELY\n",
      "processing proto RAULITO\n",
      "processing proto READING\n",
      ": 8744826it [01:10, 262481.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAúL.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto REALLY\n",
      "processing proto RECEIVE\n",
      "processing proto RECIPE\n",
      "processing proto RECOMMENDATIONS\n",
      "processing proto RECORDING\n",
      "processing proto RECYCLE\n",
      "processing proto REFERENCE\n",
      "processing proto REFORMIST\n",
      "processing proto REGARDS\n",
      "processing proto REGULAR\n",
      "processing proto RELATIONSHIP\n",
      "processing proto RELIABLE\n",
      "processing proto REMAIN\n",
      "processing proto REMINDED\n",
      "processing proto RENDERING\n",
      "processing proto RENTING\n",
      "processing proto REPEATED\n",
      "processing proto REPLACE\n",
      "processing proto REPRESENTATIVE\n",
      "processing proto REQUEST\n",
      "processing proto RESENTMENT\n",
      "processing proto RESI\n",
      "processing proto RESPONSE\n",
      "processing proto RESTS\n",
      "processing proto RET\n",
      "processing proto REUNION\n",
      "processing proto REVOLUTION\n",
      "processing proto RICARDO\n",
      "processing proto RIDDEN\n",
      "processing proto RINGING\n",
      "processing proto R\n",
      "processing proto ROBERTO\n",
      "processing proto RODEO\n",
      "processing proto ROLLS\n",
      "processing proto ROOMMATE\n",
      "processing proto ROSARY\n",
      "processing proto ROUND\n",
      "processing proto RUINED\n",
      "processing proto RUSSIAN\n",
      "processing proto SADNESS\n",
      "processing proto SAINT\n",
      "processing proto SALMERON\n",
      "processing proto SALVADOR\n",
      "processing proto SAMY\n",
      "processing proto SA\n",
      "processing proto SARITA\n",
      "processing proto SATURDAY\n",
      "processing proto SAW\n",
      "processing proto SCARE\n",
      "processing proto SCENTS\n",
      "processing proto SCIATICA\n",
      "processing proto SCREEN\n",
      "processing proto SEARCH\n",
      "processing proto SECONDARY\n",
      "processing proto SEEING\n",
      "processing proto SELDOM\n",
      "processing proto SELLERS\n",
      "processing proto SENDING\n",
      "processing proto SEPARATED\n",
      "processing proto SERIES\n",
      "processing proto SERVICES\n",
      "processing proto SEVENTH\n",
      "processing proto SEXPERCIOS\n",
      "processing proto SHAME\n",
      "processing proto SHES\n",
      "processing proto SHO\n",
      "processing proto SHOPS\n",
      "processing proto SHOWED\n",
      "processing proto SHY\n",
      "processing proto SIGNAL\n",
      "processing proto SILICONE\n",
      "processing proto SIMON\n",
      "processing proto SINGING\n",
      "processing proto SIS\n",
      "processing proto SIX\n",
      "processing proto SIZE\n",
      "processing proto SKY\n",
      "processing proto SLOPES\n",
      "processing proto SMILE\n",
      "processing proto SNOWED\n",
      "processing proto SOCCER\n",
      "processing proto SOLARES\n",
      "processing proto SOLVE\n",
      "processing proto SOMEONE\n",
      "processing proto SOMEWHERE\n",
      "processing proto SONNY\n",
      "processing proto SORT\n",
      "processing proto SOUTH\n",
      "processing proto SPEAKING\n",
      "processing proto SPECIAL\n",
      "processing proto SPEEDS\n",
      "processing proto SPILLING\n",
      "processing proto SPIRITUAL\n",
      "processing proto SPONTANEOUS\n",
      "processing proto STABILIZED\n",
      "processing proto STARING\n",
      "processing proto STARVE\n",
      "processing proto STATUE\n",
      "processing proto STEP\n",
      "processing proto STIMULATE\n",
      "processing proto STITCHES\n",
      "processing proto STONEYBROOK\n",
      "processing proto STORES\n",
      "processing proto STRAIGHT\n",
      "processing proto STREETS\n",
      "processing proto STRING\n",
      "processing proto STRUCTURE\n",
      "processing proto STUDENTS\n",
      "processing proto STUFF\n",
      "processing proto SUBJECTS\n",
      "processing proto SUDDEN\n",
      "processing proto SUIPCO\n",
      "processing proto SUMMER\n",
      "processing proto SUPER\n",
      "processing proto SUPPOSED\n",
      "processing proto SURPRISED\n",
      "processing proto SUSPENSION\n",
      "processing proto SWEETHEART\n",
      "processing proto SWORD\n",
      "processing proto TABLECLOTHS\n",
      "processing proto TADPOLES\n",
      "processing proto TALKED\n",
      "processing proto TAMALE\n",
      "processing proto TAPE\n",
      "processing proto TAXES\n",
      "processing proto TEACH\n",
      "processing proto TECHNIQUE\n",
      "processing proto TELEN\n",
      "processing proto TELL\n",
      "processing proto TENCHITA\n",
      "processing proto TENSION\n",
      "processing proto TERMS\n",
      "processing proto TESA\n",
      "processing proto TEST\n",
      "processing proto THANKSGIVING\n",
      "processing proto THEIR\n",
      "processing proto THEREFORE\n",
      "processing proto THICK\n",
      "processing proto THINNER\n",
      "processing proto THIRTY\n",
      "processing proto THOUSAND\n",
      "processing proto THROW\n",
      "processing proto TIERINI\n",
      "processing proto TILL\n",
      "processing proto TINSO\n",
      "processing proto TITLES\n",
      "processing proto TOLD\n",
      "processing proto TOMORROW\n",
      "processing proto TOO\n",
      "processing proto TORTILLAS\n",
      "processing proto TOURISTIC\n",
      "processing proto TOWNS\n",
      "processing proto TRAINED\n",
      "processing proto TRANQUIL\n",
      "processing proto TRANSPORTATION\n",
      "processing proto TRAVELS\n",
      "processing proto TREATS\n",
      "processing proto TRICK\n",
      "processing proto TROUBLE\n",
      "processing proto TRUMPET\n",
      "processing proto TUESDAYS\n",
      "processing proto TURK\n",
      "processing proto TUTORIAL\n",
      "processing proto TWELFTH\n",
      "processing proto TWENTYFIVE\n",
      "processing proto TWISTED\n",
      "processing proto TYPICAL\n",
      "processing proto UHM\n",
      "processing proto UNBELIEVABLE\n",
      "processing proto UNDECIDED\n",
      "processing proto UNDESIRABLE\n",
      "processing proto UNFORTUNATELY\n",
      "processing proto UNIVERSIDAD\n",
      "processing proto UN\n",
      "processing proto UP\n",
      "processing proto URUGUAY\n",
      "processing proto USES\n",
      "processing proto VACANCIES\n",
      "processing proto VACCINE\n",
      "processing proto VALID\n",
      "processing proto VANDALS\n",
      "processing proto VAT\n",
      "processing proto VENEZUELAN\n",
      "processing proto VERACRUZ\n",
      "processing proto VERYM\n",
      "processing proto VICTORIAN\n",
      "processing proto VIGILANCE\n",
      "processing proto VIRTUE\n",
      "processing proto VISITED\n",
      "processing proto VLADI\n",
      "processing proto VOMIT\n",
      "processing proto WAITER\n",
      "processing proto WALK\n",
      "processing proto WANTING\n",
      "processing proto WASHING\n",
      "processing proto WASTE\n",
      "processing proto WATERS\n",
      "processing proto WEAR\n",
      "processing proto WEEKEND\n",
      "processing proto WEIRDEST\n",
      "processing proto WERE\n",
      "processing proto WHEN\n",
      "processing proto WHISTLES\n",
      "processing proto WHOSE\n",
      "processing proto WILLIAM\n",
      "processing proto WINS\n",
      "processing proto WITHDRAWING\n",
      "processing proto WIT\n",
      "processing proto WONDER\n",
      "processing proto WORKER\n",
      "processing proto WORLD\n",
      "processing proto WORST\n",
      "processing proto WRITE\n",
      "processing proto WROTE\n",
      "processing proto YANKEELAND\n",
      "processing proto YEAS\n",
      "processing proto YESTERDAY\n",
      "processing proto YIQUE\n",
      "processing proto YOUNGER\n",
      "processing proto YOVANI\n",
      "processing proto ZIP\n",
      "processing proto ZULEMA\n",
      ": 16196081it [01:35, 168723.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_vad_norm_plp_for_protos(protos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_keyword_spotting_cmd_scripts(exp_path, wav_file_list, protos, exp_name, num_splits=1):\n",
    "    disc_file_split_base = \"keyword_spot_{0:d}.cmd\"\n",
    "    disc_file_split = os.path.join(exp_path, disc_file_split_base)\n",
    "    disc_split_file = os.path.join(exp_path, \"keyword_spot_split.txt\")\n",
    "    \n",
    "    num_files = len(wav_file_list)\n",
    "    num_protos = len(protos)\n",
    "    \n",
    "    exp_local_path = os.path.join(\"exp\", exp_name)\n",
    "    cmd_string = \"scripts/plebdisc_filepair_keyword_spotting \\\"{0:s}\\\" \\\"{1:s}\\\" {2:s} 39\\n\"\n",
    "\n",
    "    total_lines = num_files * num_protos\n",
    "    lines_per_file = total_lines // num_splits\n",
    "    smallfile = None\n",
    "    curr_line = 0\n",
    "    curr_file_num = 0\n",
    "\n",
    "    for i in xrange(num_protos):\n",
    "        pid_base = os.path.splitext(protos[i])[0]\n",
    "        if i % 20 == 0:\n",
    "            print(\"Progress: {0:d} out of: {1:d}\".format(curr_line+1, total_lines))\n",
    "        for j in xrange(num_files):\n",
    "            out_line = cmd_string.format(pid_base, wav_file_list[j], exp_local_path)\n",
    "            if curr_line % lines_per_file == 0:\n",
    "                if smallfile:\n",
    "                    smallfile.close()\n",
    "                small_filename = disc_file_split.format(curr_file_num)\n",
    "                smallfile = open(small_filename, \"w\")\n",
    "                curr_file_num += 1\n",
    "            smallfile.write(out_line)\n",
    "            curr_line += 1\n",
    "    if smallfile:\n",
    "        smallfile.close()\n",
    "\n",
    "    # Making a list of commands to execute the split disc list\n",
    "    full_split_cmd_string = \"nice sh {0:s} 1> {1:s} 2>{2:s} &\\n\"\n",
    "    split_cmd = os.path.join(exp_local_path, \"matches\",\"{0:s}.{1:d}\")\n",
    "    with open(disc_split_file, \"w\") as out_f:\n",
    "        for i in xrange(curr_file_num):\n",
    "            curr_split_file = os.path.join(exp_local_path, disc_file_split_base.format(i))\n",
    "            split_cmd_out = split_cmd.format(\"keyword_spot_out\", i)\n",
    "            #split_cmd_err = split_cmd.format(\"err\", i)\n",
    "            split_cmd_err = \"/dev/null\"\n",
    "\n",
    "            out_line = \"nice sh \"\n",
    "            out_f.write(full_split_cmd_string.format(curr_split_file, \\\n",
    "                                                    split_cmd_out, \\\n",
    "                                                    split_cmd_err))\n",
    "\n",
    "    print(\"Completed - keyword_spot cmd script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 1 out of: 188199\n",
      "Progress: 661 out of: 188199\n",
      "Progress: 1321 out of: 188199\n",
      "Progress: 1981 out of: 188199\n",
      "Progress: 2641 out of: 188199\n",
      "Progress: 3301 out of: 188199\n",
      "Progress: 3961 out of: 188199\n",
      "Progress: 4621 out of: 188199\n",
      "Progress: 5281 out of: 188199\n",
      "Progress: 5941 out of: 188199\n",
      "Progress: 6601 out of: 188199\n",
      "Progress: 7261 out of: 188199\n",
      "Progress: 7921 out of: 188199\n",
      "Progress: 8581 out of: 188199\n",
      "Progress: 9241 out of: 188199\n",
      "Progress: 9901 out of: 188199\n",
      "Progress: 10561 out of: 188199\n",
      "Progress: 11221 out of: 188199\n",
      "Progress: 11881 out of: 188199\n",
      "Progress: 12541 out of: 188199\n",
      "Progress: 13201 out of: 188199\n",
      "Progress: 13861 out of: 188199\n",
      "Progress: 14521 out of: 188199\n",
      "Progress: 15181 out of: 188199\n",
      "Progress: 15841 out of: 188199\n",
      "Progress: 16501 out of: 188199\n",
      "Progress: 17161 out of: 188199\n",
      "Progress: 17821 out of: 188199\n",
      "Progress: 18481 out of: 188199\n",
      "Progress: 19141 out of: 188199\n",
      "Progress: 19801 out of: 188199\n",
      "Progress: 20461 out of: 188199\n",
      "Progress: 21121 out of: 188199\n",
      "Progress: 21781 out of: 188199\n",
      "Progress: 22441 out of: 188199\n",
      "Progress: 23101 out of: 188199\n",
      "Progress: 23761 out of: 188199\n",
      "Progress: 24421 out of: 188199\n",
      "Progress: 25081 out of: 188199\n",
      "Progress: 25741 out of: 188199\n",
      "Progress: 26401 out of: 188199\n",
      "Progress: 27061 out of: 188199\n",
      "Progress: 27721 out of: 188199\n",
      "Progress: 28381 out of: 188199\n",
      "Progress: 29041 out of: 188199\n",
      "Progress: 29701 out of: 188199\n",
      "Progress: 30361 out of: 188199\n",
      "Progress: 31021 out of: 188199\n",
      "Progress: 31681 out of: 188199\n",
      "Progress: 32341 out of: 188199\n",
      "Progress: 33001 out of: 188199\n",
      "Progress: 33661 out of: 188199\n",
      "Progress: 34321 out of: 188199\n",
      "Progress: 34981 out of: 188199\n",
      "Progress: 35641 out of: 188199\n",
      "Progress: 36301 out of: 188199\n",
      "Progress: 36961 out of: 188199\n",
      "Progress: 37621 out of: 188199\n",
      "Progress: 38281 out of: 188199\n",
      "Progress: 38941 out of: 188199\n",
      "Progress: 39601 out of: 188199\n",
      "Progress: 40261 out of: 188199\n",
      "Progress: 40921 out of: 188199\n",
      "Progress: 41581 out of: 188199\n",
      "Progress: 42241 out of: 188199\n",
      "Progress: 42901 out of: 188199\n",
      "Progress: 43561 out of: 188199\n",
      "Progress: 44221 out of: 188199\n",
      "Progress: 44881 out of: 188199\n",
      "Progress: 45541 out of: 188199\n",
      "Progress: 46201 out of: 188199\n",
      "Progress: 46861 out of: 188199\n",
      "Progress: 47521 out of: 188199\n",
      "Progress: 48181 out of: 188199\n",
      "Progress: 48841 out of: 188199\n",
      "Progress: 49501 out of: 188199\n",
      "Progress: 50161 out of: 188199\n",
      "Progress: 50821 out of: 188199\n",
      "Progress: 51481 out of: 188199\n",
      "Progress: 52141 out of: 188199\n",
      "Progress: 52801 out of: 188199\n",
      "Progress: 53461 out of: 188199\n",
      "Progress: 54121 out of: 188199\n",
      "Progress: 54781 out of: 188199\n",
      "Progress: 55441 out of: 188199\n",
      "Progress: 56101 out of: 188199\n",
      "Progress: 56761 out of: 188199\n",
      "Progress: 57421 out of: 188199\n",
      "Progress: 58081 out of: 188199\n",
      "Progress: 58741 out of: 188199\n",
      "Progress: 59401 out of: 188199\n",
      "Progress: 60061 out of: 188199\n",
      "Progress: 60721 out of: 188199\n",
      "Progress: 61381 out of: 188199\n",
      "Progress: 62041 out of: 188199\n",
      "Progress: 62701 out of: 188199\n",
      "Progress: 63361 out of: 188199\n",
      "Progress: 64021 out of: 188199\n",
      "Progress: 64681 out of: 188199\n",
      "Progress: 65341 out of: 188199\n",
      "Progress: 66001 out of: 188199\n",
      "Progress: 66661 out of: 188199\n",
      "Progress: 67321 out of: 188199\n",
      "Progress: 67981 out of: 188199\n",
      "Progress: 68641 out of: 188199\n",
      "Progress: 69301 out of: 188199\n",
      "Progress: 69961 out of: 188199\n",
      "Progress: 70621 out of: 188199\n",
      "Progress: 71281 out of: 188199\n",
      "Progress: 71941 out of: 188199\n",
      "Progress: 72601 out of: 188199\n",
      "Progress: 73261 out of: 188199\n",
      "Progress: 73921 out of: 188199\n",
      "Progress: 74581 out of: 188199\n",
      "Progress: 75241 out of: 188199\n",
      "Progress: 75901 out of: 188199\n",
      "Progress: 76561 out of: 188199\n",
      "Progress: 77221 out of: 188199\n",
      "Progress: 77881 out of: 188199\n",
      "Progress: 78541 out of: 188199\n",
      "Progress: 79201 out of: 188199\n",
      "Progress: 79861 out of: 188199\n",
      "Progress: 80521 out of: 188199\n",
      "Progress: 81181 out of: 188199\n",
      "Progress: 81841 out of: 188199\n",
      "Progress: 82501 out of: 188199\n",
      "Progress: 83161 out of: 188199\n",
      "Progress: 83821 out of: 188199\n",
      "Progress: 84481 out of: 188199\n",
      "Progress: 85141 out of: 188199\n",
      "Progress: 85801 out of: 188199\n",
      "Progress: 86461 out of: 188199\n",
      "Progress: 87121 out of: 188199\n",
      "Progress: 87781 out of: 188199\n",
      "Progress: 88441 out of: 188199\n",
      "Progress: 89101 out of: 188199\n",
      "Progress: 89761 out of: 188199\n",
      "Progress: 90421 out of: 188199\n",
      "Progress: 91081 out of: 188199\n",
      "Progress: 91741 out of: 188199\n",
      "Progress: 92401 out of: 188199\n",
      "Progress: 93061 out of: 188199\n",
      "Progress: 93721 out of: 188199\n",
      "Progress: 94381 out of: 188199\n",
      "Progress: 95041 out of: 188199\n",
      "Progress: 95701 out of: 188199\n",
      "Progress: 96361 out of: 188199\n",
      "Progress: 97021 out of: 188199\n",
      "Progress: 97681 out of: 188199\n",
      "Progress: 98341 out of: 188199\n",
      "Progress: 99001 out of: 188199\n",
      "Progress: 99661 out of: 188199\n",
      "Progress: 100321 out of: 188199\n",
      "Progress: 100981 out of: 188199\n",
      "Progress: 101641 out of: 188199\n",
      "Progress: 102301 out of: 188199\n",
      "Progress: 102961 out of: 188199\n",
      "Progress: 103621 out of: 188199\n",
      "Progress: 104281 out of: 188199\n",
      "Progress: 104941 out of: 188199\n",
      "Progress: 105601 out of: 188199\n",
      "Progress: 106261 out of: 188199\n",
      "Progress: 106921 out of: 188199\n",
      "Progress: 107581 out of: 188199\n",
      "Progress: 108241 out of: 188199\n",
      "Progress: 108901 out of: 188199\n",
      "Progress: 109561 out of: 188199\n",
      "Progress: 110221 out of: 188199\n",
      "Progress: 110881 out of: 188199\n",
      "Progress: 111541 out of: 188199\n",
      "Progress: 112201 out of: 188199\n",
      "Progress: 112861 out of: 188199\n",
      "Progress: 113521 out of: 188199\n",
      "Progress: 114181 out of: 188199\n",
      "Progress: 114841 out of: 188199\n",
      "Progress: 115501 out of: 188199\n",
      "Progress: 116161 out of: 188199\n",
      "Progress: 116821 out of: 188199\n",
      "Progress: 117481 out of: 188199\n",
      "Progress: 118141 out of: 188199\n",
      "Progress: 118801 out of: 188199\n",
      "Progress: 119461 out of: 188199\n",
      "Progress: 120121 out of: 188199\n",
      "Progress: 120781 out of: 188199\n",
      "Progress: 121441 out of: 188199\n",
      "Progress: 122101 out of: 188199\n",
      "Progress: 122761 out of: 188199\n",
      "Progress: 123421 out of: 188199\n",
      "Progress: 124081 out of: 188199\n",
      "Progress: 124741 out of: 188199\n",
      "Progress: 125401 out of: 188199\n",
      "Progress: 126061 out of: 188199\n",
      "Progress: 126721 out of: 188199\n",
      "Progress: 127381 out of: 188199\n",
      "Progress: 128041 out of: 188199\n",
      "Progress: 128701 out of: 188199\n",
      "Progress: 129361 out of: 188199\n",
      "Progress: 130021 out of: 188199\n",
      "Progress: 130681 out of: 188199\n",
      "Progress: 131341 out of: 188199\n",
      "Progress: 132001 out of: 188199\n",
      "Progress: 132661 out of: 188199\n",
      "Progress: 133321 out of: 188199\n",
      "Progress: 133981 out of: 188199\n",
      "Progress: 134641 out of: 188199\n",
      "Progress: 135301 out of: 188199\n",
      "Progress: 135961 out of: 188199\n",
      "Progress: 136621 out of: 188199\n",
      "Progress: 137281 out of: 188199\n",
      "Progress: 137941 out of: 188199\n",
      "Progress: 138601 out of: 188199\n",
      "Progress: 139261 out of: 188199\n",
      "Progress: 139921 out of: 188199\n",
      "Progress: 140581 out of: 188199\n",
      "Progress: 141241 out of: 188199\n",
      "Progress: 141901 out of: 188199\n",
      "Progress: 142561 out of: 188199\n",
      "Progress: 143221 out of: 188199\n",
      "Progress: 143881 out of: 188199\n",
      "Progress: 144541 out of: 188199\n",
      "Progress: 145201 out of: 188199\n",
      "Progress: 145861 out of: 188199\n",
      "Progress: 146521 out of: 188199\n",
      "Progress: 147181 out of: 188199\n",
      "Progress: 147841 out of: 188199\n",
      "Progress: 148501 out of: 188199\n",
      "Progress: 149161 out of: 188199\n",
      "Progress: 149821 out of: 188199\n",
      "Progress: 150481 out of: 188199\n",
      "Progress: 151141 out of: 188199\n",
      "Progress: 151801 out of: 188199\n",
      "Progress: 152461 out of: 188199\n",
      "Progress: 153121 out of: 188199\n",
      "Progress: 153781 out of: 188199\n",
      "Progress: 154441 out of: 188199\n",
      "Progress: 155101 out of: 188199\n",
      "Progress: 155761 out of: 188199\n",
      "Progress: 156421 out of: 188199\n",
      "Progress: 157081 out of: 188199\n",
      "Progress: 157741 out of: 188199\n",
      "Progress: 158401 out of: 188199\n",
      "Progress: 159061 out of: 188199\n",
      "Progress: 159721 out of: 188199\n",
      "Progress: 160381 out of: 188199\n",
      "Progress: 161041 out of: 188199\n",
      "Progress: 161701 out of: 188199\n",
      "Progress: 162361 out of: 188199\n",
      "Progress: 163021 out of: 188199\n",
      "Progress: 163681 out of: 188199\n",
      "Progress: 164341 out of: 188199\n",
      "Progress: 165001 out of: 188199\n",
      "Progress: 165661 out of: 188199\n",
      "Progress: 166321 out of: 188199\n",
      "Progress: 166981 out of: 188199\n",
      "Progress: 167641 out of: 188199\n",
      "Progress: 168301 out of: 188199\n",
      "Progress: 168961 out of: 188199\n",
      "Progress: 169621 out of: 188199\n",
      "Progress: 170281 out of: 188199\n",
      "Progress: 170941 out of: 188199\n",
      "Progress: 171601 out of: 188199\n",
      "Progress: 172261 out of: 188199\n",
      "Progress: 172921 out of: 188199\n",
      "Progress: 173581 out of: 188199\n",
      "Progress: 174241 out of: 188199\n",
      "Progress: 174901 out of: 188199\n",
      "Progress: 175561 out of: 188199\n",
      "Progress: 176221 out of: 188199\n",
      "Progress: 176881 out of: 188199\n",
      "Progress: 177541 out of: 188199\n",
      "Progress: 178201 out of: 188199\n",
      "Progress: 178861 out of: 188199\n",
      "Progress: 179521 out of: 188199\n",
      "Progress: 180181 out of: 188199\n",
      "Progress: 180841 out of: 188199\n",
      "Progress: 181501 out of: 188199\n",
      "Progress: 182161 out of: 188199\n",
      "Progress: 182821 out of: 188199\n",
      "Progress: 183481 out of: 188199\n",
      "Progress: 184141 out of: 188199\n",
      "Progress: 184801 out of: 188199\n",
      "Progress: 185461 out of: 188199\n",
      "Progress: 186121 out of: 188199\n",
      "Progress: 186781 out of: 188199\n",
      "Progress: 187441 out of: 188199\n",
      "Progress: 188101 out of: 188199\n",
      "Completed - keyword_spot cmd script\n"
     ]
    }
   ],
   "source": [
    "create_keyword_spotting_cmd_scripts(exp_path=exp_path, wav_file_list=wav_file_list[:33], protos=protos,\n",
    "                                    exp_name=exp_name, num_splits=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['001',\n",
       " '002',\n",
       " '005',\n",
       " '006',\n",
       " '007',\n",
       " '009',\n",
       " '010',\n",
       " '011',\n",
       " '012',\n",
       " '013',\n",
       " '014',\n",
       " '015',\n",
       " '018',\n",
       " '021',\n",
       " '022',\n",
       " '023',\n",
       " '024',\n",
       " '025',\n",
       " '026',\n",
       " '027',\n",
       " '028',\n",
       " '029',\n",
       " '030',\n",
       " '031',\n",
       " '032',\n",
       " '033',\n",
       " '034',\n",
       " '035',\n",
       " '036',\n",
       " '037',\n",
       " '038',\n",
       " '039',\n",
       " '040']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_file_list[:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5703"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(protos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
