{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import cPickle as pickle\n",
    "import json\n",
    "import subprocess\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "from collections import namedtuple\n",
    "from itertools import izip\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"config.json\") as json_data_file:\n",
    "    config = json.load(json_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing CALLHOME for ZRTools\n",
    "\n",
    "\n",
    "- Created: 26-Oct-2016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mapping for start time for each segment\n",
    "\n",
    "Format: Dictionary  \n",
    "key: {key: value}  \n",
    "*file: {file.seg.wav: start time}*  \n",
    "Name: segment_start.dict, segment_start.txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_segments_file(seg_fname):\n",
    "    segment_map = {}\n",
    "    with open(seg_fname, \"r\") as seg_f:\n",
    "        for i, line in enumerate(seg_f):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            try:\n",
    "                line_items = line.strip().split()\n",
    "                seg_key = line_items[0]\n",
    "                file_id = line_items[1]\n",
    "                if file_id not in segment_map:\n",
    "                    segment_map[file_id] = {}\n",
    "                seg_start = int(float(line_items[6])*100)\n",
    "                segment_map[file_id][seg_key] = seg_start\n",
    "            except ValueError:\n",
    "                print(\"Incorrect line format at line: %d\" % i)\n",
    "    return segment_map\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read segment map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_segment_map():\n",
    "    segment_map = read_segments_file('../segments.txt')\n",
    "    pickle.dump(segment_map, open(config['es']['segment_dict_fname'], \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VAD files for merged wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_merged_vad_from_ed(vad_file_id, segment_map, seg_vad_path, merged_vad_path):\n",
    "    total_dur_10ms = 0\n",
    "    total_dur_10ms_ge500ms = 0\n",
    "    with open(os.path.join(merged_vad_path, vad_file_id+\".vad\"), \"w\") as vad_f:\n",
    "        print(\"creating vad %s ...\" % vad_file_id)\n",
    "        for i, (seg_id, seg_start) in enumerate(sorted(segment_map[vad_file_id].items(), key=lambda t:t[0])):\n",
    "            with open(os.path.join(seg_vad_path, seg_id+\".vad\"), \"r\") as seg_vad_f:\n",
    "                for line in seg_vad_f:\n",
    "                    line_items = map(int, line.strip().split())\n",
    "                    start = seg_start+line_items[0]\n",
    "                    end = seg_start+line_items[1]\n",
    "                    total_dur_10ms += (end-start)\n",
    "                    total_dur_10ms_ge500ms += ((end - start) if (end-start) >= 50 else 0)\n",
    "                    out_line = (\"%d %d\\n\" %(start, end))\n",
    "                    vad_f.write(out_line)\n",
    "                # end for\n",
    "            # end reading seg file\n",
    "        # end looping over all segments\n",
    "    # end writing vad file\n",
    "    return total_dur_10ms, total_dur_10ms_ge500ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new directory for merged vads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merged_ed_vads_path = \"../mergedVads\"\n",
    "# seg_vad_path = \"../vad\"\n",
    "# if not os.path.exists(merged_ed_vads_path):\n",
    "#     os.makedirs(merged_ed_vads_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create merged vad for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# total_dur_10ms, total_dur_10ms_ge500ms = 0, 0\n",
    "# for vad_file_id in segment_map:\n",
    "#     t1, t2 = create_merged_vad(vad_file_id, segment_map, seg_vad_path, merged_vads_path)\n",
    "#     total_dur_10ms += t1\n",
    "#     total_dur_10ms_ge500ms += t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(total_dur_10ms, total_dur_10ms_ge500ms)\n",
    "# print(map(lambda t: \"{0:.3f}\".format((t / 100.0 / 3600)), [total_dur_10ms, total_dur_10ms_ge500ms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_wavs_path = \"../mergeWavs\"\n",
    "plp_path = \"../plp\"\n",
    "plp_norm_path = \"../std_plp\"\n",
    "if not os.path.exists(plp_path):\n",
    "    os.makedirs(plp_path)\n",
    "if not os.path.exists(plp_norm_path):\n",
    "    os.makedirs(plp_norm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_file_lst(file_lst_fname):\n",
    "    prefix = \"../corpora/callhome/mergeWavs\"\n",
    "    wav_file_list = [os.path.join(prefix, wav_file) for \\\n",
    "                     wav_file in os.listdir(merged_wavs_path) if wav_file.endswith(\".wav\")]\n",
    "    wav_file_list_string = \"\\n\".join(wav_file_list)\n",
    "    with open(file_lst_fname, \"w\") as out_f:\n",
    "        out_f.write(wav_file_list_string)\n",
    "    print(\"Finished writing files.lst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create_file_lst(config[\"es\"][\"lst_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_plp(wav_fname, plp_fname):\n",
    "    FEACALC = config['base'][\"feacalc\"]\n",
    "    subprocess.call([FEACALC,\"-plp\", \\\n",
    "                    \"12\", \"-cep\", \"13\", \"-dom\", \"cep\", \"-deltaorder\", \\\n",
    "                    \"2\", \"-dither\", \"-frqaxis\", \"bark\", \"-samplerate\", \\\n",
    "                    \"8000\", \"-win\", \"25\", \"-step\", \"10\", \"-ip\", \\\n",
    "                    \"MSWAVE\", \"-rasta\", \"false\", \"-compress\", \\\n",
    "                    \"true\", \"-op\", \"swappedraw\", \"-o\", plp_fname, wav_fname])\n",
    "\n",
    "    \n",
    "def normalize_plp(plp_fname, vad_fname, plp_norm_fname):\n",
    "    STANDFEAT = config['base'][\"standfeat\"]\n",
    "    # Standardize binary file, for VAD regions only\n",
    "    subprocess.call([STANDFEAT, \"-D\", \"39\", \"-infile\", \\\n",
    "                    plp_fname, \"-outfile\", plp_norm_fname, \\\n",
    "                    \"-vadfile\", vad_fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_and_normalize_plps():\n",
    "    for i, file_id in enumerate(segment_map):\n",
    "        wav_fname = os.path.join(merged_wavs_path, file_id+\".wav\")\n",
    "        vad_fname = os.path.join(merged_fa_vads_path, file_id+\".vad\")\n",
    "        plp_fname = os.path.join(plp_path, file_id+\".binary\")\n",
    "        plp_norm_fname = os.path.join(plp_norm_path, file_id+\".std.binary\")\n",
    "\n",
    "        #print(file_id, wav_fname, vad_fname, plp_fname, plp_norm_fname)\n",
    "\n",
    "        # create PLP\n",
    "        if i % 20 == 0:\n",
    "            print(\"plp for file %s \" % file_id)\n",
    "\n",
    "        #if not os.path.exists(plp_fname):\n",
    "        create_plp(wav_fname, plp_fname)\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(\"normalizing plp %s\" % file_id)\n",
    "\n",
    "        #if not os.path.exists(plp_norm_fname):\n",
    "        normalize_plp(plp_fname, vad_fname, plp_norm_fname)\n",
    "    print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LSH files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsh_path = \"../lsh\"\n",
    "if not os.path.exists(lsh_path):\n",
    "    os.makedirs(lsh_path)\n",
    "lsh_proj_fname = os.path.join(lsh_path, \"proj_S64xD39_seed1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lsh_proj_file(lsh_proj_fname):\n",
    "    subprocess.call([config['base'][\"lsh_genproj\"], \\\n",
    "                     \"-D\",\"39\",\"-S\",\"64\",\"-seed\", \\\n",
    "                     \"1\",\"-projfile\", lsh_proj_fname])\n",
    "\n",
    "def create_lsh_file(plp_norm_fname, vad_fname, lsh_proj_fname, lsh_fname):\n",
    "    LSH = config['base'][\"lsh\"]\n",
    "    subprocess.call([LSH, \"-D\", \"39\", \"-S\", \"64\", \\\n",
    "                    \"-projfile\", lsh_proj_fname, \\\n",
    "                    \"-featfile\", plp_norm_fname, \"-sigfile\", \\\n",
    "                    lsh_fname, \"-vadfile\", vad_fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(lsh_proj_fname):\n",
    "    create_lsh_proj_file(lsh_proj_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_lsh_files():\n",
    "    for i, file_id in enumerate(segment_map):\n",
    "        wav_fname = os.path.join(merged_wavs_path, file_id+\".wav\")\n",
    "        vad_fname = os.path.join(merged_fa_vads_path, file_id+\".vad\")\n",
    "        plp_norm_fname = os.path.join(plp_norm_path, file_id+\".std.binary\")\n",
    "        lsh_fname = os.path.join(lsh_path, file_id+\".std.lsh64\")\n",
    "\n",
    "        #print(file_id, wav_fname, vad_fname, plp_fname, plp_norm_fname)\n",
    "\n",
    "        # create LSH\n",
    "        if i % 20 == 0:\n",
    "            print(\"lsh for file %s \" % file_id)\n",
    "\n",
    "        #if not os.path.exists(lsh_fname):\n",
    "        create_lsh_file(plp_norm_fname, vad_fname, lsh_proj_fname, lsh_fname)\n",
    "\n",
    "    print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ZRTools discovery command files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_path = '../exp'\n",
    "if not os.path.exists(exp_path):\n",
    "    os.makedirs(exp_path)\n",
    "\n",
    "# List of wav files\n",
    "segment_map = pickle.load(open(config['es']['segment_dict_fname'], \"rb\"))\n",
    "wav_file_list = sorted(segment_map.keys())\n",
    "exp_name = 'callhome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_files_base():\n",
    "    with open(os.path.join(exp_path, 'files.base'), \"w\") as out_f:\n",
    "        for wav_file in wav_file_list:\n",
    "            out_f.write(wav_file+'\\n')\n",
    "    print(\"Generated files.base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_discovery_cmd_scripts(exp_path, wav_file_list, exp_name, num_splits=1):\n",
    "    disc_file_split_base = \"disc_{0:d}.cmd\"\n",
    "    disc_file_split = os.path.join(exp_path, disc_file_split_base)\n",
    "    disc_split_file = os.path.join(exp_path, \"disc_split.txt\")\n",
    "    num_files = len(wav_file_list)\n",
    "    exp_local_path = os.path.join(\"exp\", exp_name)\n",
    "    cmd_string = \"scripts/plebdisc_filepair \\\"{0:s}\\\" \\\"{1:s}\\\" {2:s} 39\\n\"\n",
    "\n",
    "    total_lines = num_files * num_files\n",
    "    lines_per_file = total_lines // num_splits\n",
    "    smallfile = None\n",
    "    curr_line = 0\n",
    "    curr_file_num = 0\n",
    "\n",
    "    for i in xrange(num_files) :\n",
    "        if i % 20 == 0:\n",
    "            print(\"Progress: {0:d} out of: {1:d}\".format(curr_line+1, total_lines))\n",
    "        for j in xrange(num_files):\n",
    "            out_line = cmd_string.format(wav_file_list[i], \\\n",
    "                                              wav_file_list[j], \\\n",
    "                                              exp_local_path)\n",
    "            if curr_line % lines_per_file == 0:\n",
    "                if smallfile:\n",
    "                    smallfile.close()\n",
    "                small_filename = disc_file_split.format(curr_file_num)\n",
    "                smallfile = open(small_filename, \"w\")\n",
    "                curr_file_num += 1\n",
    "            smallfile.write(out_line)\n",
    "            curr_line += 1\n",
    "    if smallfile:\n",
    "        smallfile.close()\n",
    "\n",
    "    # Making a list of commands to execute the split disc list\n",
    "    full_split_cmd_string = \"nice sh {0:s} 1> {1:s} 2>{2:s} &\\n\"\n",
    "    split_cmd = os.path.join(exp_local_path, \"matches\",\"{0:s}.{1:d}\")\n",
    "    with open(disc_split_file, \"w\") as out_f:\n",
    "        for i in xrange(curr_file_num):\n",
    "            curr_split_file = os.path.join(exp_local_path, disc_file_split_base.format(i))\n",
    "            split_cmd_out = split_cmd.format(\"out\", i)\n",
    "            #split_cmd_err = split_cmd.format(\"err\", i)\n",
    "            split_cmd_err = \"/dev/null\"\n",
    "\n",
    "            out_line = \"nice sh \"\n",
    "            out_f.write(full_split_cmd_string.format(curr_split_file, \\\n",
    "                                                    split_cmd_out, \\\n",
    "                                                    split_cmd_err))\n",
    "\n",
    "    print(\"Completed - disc.cmd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create_discovery_cmd_scripts(exp_path=exp_path, wav_file_list=wav_file_list, exp_name=exp_name, num_splits=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read transcripts, and translations into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Align = namedtuple('Align', ['word', 'start', 'end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_alignment_file(align_fname, stopwords_corpus=None):\n",
    "    align_list = []\n",
    "    with open(align_fname, \"r\") as align_f:\n",
    "        for line in align_f:\n",
    "            line_items = line.strip().split()\n",
    "            if len(line_items) != 3:\n",
    "                raise ValueError\n",
    "            start, end = map(lambda v: int(float(v)*100), line_items[1:3])\n",
    "            if (not stopwords_corpus) or \\\n",
    "            (stopwords_corpus and line_items[0].lower().decode(\"utf-8\") not in stopwords_corpus):\n",
    "                align_list.append(Align(*[line_items[0], start, end]))\n",
    "    if sorted(align_list, key=lambda t: t.start) != align_list and not align_fname.endswith(\"en\"):\n",
    "        raise IOError    \n",
    "            \n",
    "    return align_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_words_path = '../wav2es-words/'\n",
    "en_words_path = '../wav2eng-words/'\n",
    "align_dict_fname = config['es']['align_dict_fname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # test code\n",
    "# stopwords_es = set(stopwords.words('spanish'))\n",
    "# stopwords_en = set(stopwords.words('english'))\n",
    "# display(read_alignment_file('../wav2es-words/001.001.es'))\n",
    "# display(read_alignment_file('../wav2es-words/001.001.es', stopwords_corpus=stopwords_es))\n",
    "# display(read_alignment_file('../wav2eng-words/001.001.en'))\n",
    "# display(read_alignment_file('../wav2eng-words/001.001.en', stopwords_corpus=stopwords_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_list(file_path, file_ext):\n",
    "    return [os.path.splitext(f)[0] for f in os.listdir(file_path) if f.endswith(file_ext)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "set(['009.025'])\n"
     ]
    }
   ],
   "source": [
    "es_file_list = get_file_list(es_words_path, 'es')\n",
    "en_file_list = get_file_list(en_words_path, 'en')\n",
    "\n",
    "print(sorted(es_file_list) == sorted(en_file_list))\n",
    "print(set(es_file_list)-set(en_file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_alignment_dict():\n",
    "    align_dict = {}\n",
    "    stopwords_es = set(stopwords.words('spanish'))\n",
    "    stopwords_en = set(stopwords.words('english'))\n",
    "    for file_id in segment_map:\n",
    "        #print(\"Processing file: %s\" % file_id)\n",
    "        align_dict[file_id] = {}\n",
    "        for seg_id in segment_map[file_id]:\n",
    "            align_dict[file_id][seg_id] = {}\n",
    "            es_fname = os.path.join(es_words_path, seg_id+\".es\")\n",
    "            en_fname = os.path.join(en_words_path, seg_id+\".en\")\n",
    "            align_dict[file_id][seg_id][\"es\"] = read_alignment_file(es_fname)\n",
    "            align_dict[file_id][seg_id][\"en\"] = read_alignment_file(en_fname)\n",
    "            align_dict[file_id][seg_id][\"es_cnt\"] = read_alignment_file(es_fname, stopwords_corpus=stopwords_es)\n",
    "            align_dict[file_id][seg_id][\"en_cnt\"] = read_alignment_file(en_fname, stopwords_corpus=stopwords_en)\n",
    "    return align_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# align_dict = create_alignment_dict()\n",
    "# pickle.dump(align_dict, open(align_dict_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VAD from alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "align_dict = pickle.load(open(align_dict_fname, \"rb\"))\n",
    "segment_map = pickle.load(open(config['es']['segment_dict_fname'], \"rb\"))\n",
    "# has_500ms_fa_vad_dict_fname = config['es']['has_500ms_fa_vad_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_uttr_vad_from_alignment(align_dict, vad_path):\n",
    "    has_500ms_dur = {}\n",
    "    total_dur_10ms = 0\n",
    "    total_dur_10ms_ge500ms = 0\n",
    "    for i, vad_file_id in enumerate(align_dict):\n",
    "        if i % 20 == 0:\n",
    "            print(\"Created vad for %d files id\" % i)\n",
    "        for seg_id in align_dict[vad_file_id]:\n",
    "            with open(os.path.join(vad_path, seg_id+\".vad\"), \"w\") as vad_f:\n",
    "                dur_10ms = 0\n",
    "                dur_10ms_ge500ms = 0\n",
    "                vad_list = []\n",
    "                # start index\n",
    "                s = 0\n",
    "                # create a local list of alignment values\n",
    "                align_list = align_dict[vad_file_id][seg_id]['es']\n",
    "                for j in xrange(len(align_list)):\n",
    "                    # if 1st or last element, add to vad_list\n",
    "                    if ((j+1) == len(align_list)) or (align_list[j].end != align_list[j+1].start):\n",
    "                        vad_list.append(((align_list[s].start), (align_list[j].end)))\n",
    "                        s=j+1\n",
    "                # write vad list to file        \n",
    "                for vad_tup in vad_list:\n",
    "                    start = vad_tup[0]\n",
    "                    end = vad_tup[1]\n",
    "                    dur_10ms += (end-start)\n",
    "                    dur_10ms_ge500ms += ((end - start) if (end-start) >= 50 else 0)\n",
    "                    out_line = (\"%d %d\\n\" %(start, end))\n",
    "                    vad_f.write(out_line)\n",
    "                \n",
    "                # set whether atleast one vad region of 500 ms\n",
    "                has_500ms_dur[seg_id] = (dur_10ms_ge500ms > 0)\n",
    "                # compute total durations\n",
    "                total_dur_10ms += dur_10ms\n",
    "                total_dur_10ms_ge500ms += dur_10ms_ge500ms \n",
    "                    \n",
    "            # end for\n",
    "        # end looping over all segments\n",
    "    # end writing vad file\n",
    "    return total_dur_10ms, total_dur_10ms_ge500ms, has_500ms_dur\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_merged_vad_from_alignment(vad_file_id, align_dict, segment_map, vad_path):\n",
    "    total_dur_10ms = 0\n",
    "    total_dur_10ms_ge500ms = 0\n",
    "    with open(os.path.join(vad_path, vad_file_id+\".vad\"), \"w\") as vad_f:\n",
    "        print(\"creating vad %s ...\" % vad_file_id)\n",
    "        for i, (seg_id, seg_start) in enumerate(sorted(segment_map[vad_file_id].items(), key=lambda t:t[0])):\n",
    "            vad_list = []\n",
    "            # start index\n",
    "            s = 0\n",
    "            # create a local list of alignment values\n",
    "            align_list = align_dict[vad_file_id][seg_id]['es']\n",
    "            for j in xrange(len(align_list)):\n",
    "                # if 1st or last element, add to vad_list\n",
    "                if ((j+1) == len(align_list)) or (align_list[j].end != align_list[j+1].start):\n",
    "                    vad_list.append(((seg_start+align_list[s].start), (seg_start+align_list[j].end)))\n",
    "                    s=j+1\n",
    "            # write vad list to file        \n",
    "            for vad_tup in vad_list:\n",
    "                start = vad_tup[0]\n",
    "                end = vad_tup[1]\n",
    "                total_dur_10ms += (end-start)\n",
    "                total_dur_10ms_ge500ms += ((end - start) if (end-start) >= 50 else 0)\n",
    "                out_line = (\"%d %d\\n\" %(start, end))\n",
    "                vad_f.write(out_line)\n",
    "            # end for\n",
    "        # end looping over all segments\n",
    "    # end writing vad file\n",
    "    return total_dur_10ms, total_dur_10ms_ge500ms\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new directory for merged vads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uttr_fa_vads_path = config['es']['es_uttr_fa_vad']\n",
    "if not os.path.exists(uttr_fa_vads_path):\n",
    "    os.makedirs(uttr_fa_vads_path)\n",
    "\n",
    "merged_fa_vads_path = config['es']['es_merge_fa_vad']\n",
    "if not os.path.exists(merged_fa_vads_path):\n",
    "    os.makedirs(merged_fa_vads_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# t1, t2, has_500ms_dur = create_uttr_vad_from_alignment(align_dict, uttr_fa_vads_path)\n",
    "# # print(t1, t2)\n",
    "# print(map(lambda t: \"{0:.3f} hrs\".format((t / 100.0 / 3600)), [t1, t2]))\n",
    "# print(\"saving dict: %s\" % has_500ms_fa_vad_dict_fname)\n",
    "# pickle.dump(has_500ms_dur, open(has_500ms_fa_vad_dict_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # check how many utterances have atleast 500ms VAD\n",
    "# print(\"total utterances: %d\" % len(has_500ms_dur))\n",
    "# uttrs_with_500ms = {k:v for k, v in has_500ms_dur.items() if v}\n",
    "# print(\"with 500ms: %d\" % len(uttrs_with_500ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save dev file with only 500ms segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dev_500ms_fname = config['es']['mt_dev_500ms_files']\n",
    "# dev_fname = config['es']['mt_dev_test_files']\n",
    "# with open(dev_fname, \"r\") as dev_f, open(dev_500ms_fname, \"w\") as dev_500ms_f:\n",
    "#     for line in dev_f:\n",
    "#         if line.strip() in has_500ms_dur and has_500ms_dur[line.strip()]:\n",
    "#             dev_500ms_f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !wc $dev_500ms_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create merged vad for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_merged_vads():\n",
    "    total_dur_10ms, total_dur_10ms_ge500ms = 0, 0\n",
    "    for vad_file_id in segment_map:\n",
    "        t1, t2 = create_vad_from_alignment(vad_file_id, align_dict, segment_map, merged_fa_vads_path)\n",
    "        total_dur_10ms += t1\n",
    "        total_dur_10ms_ge500ms += t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(total_dur_10ms, total_dur_10ms_ge500ms)\n",
    "# print(map(lambda t: \"{0:.3f}\".format((t / 100.0 / 3600)), [total_dur_10ms, total_dur_10ms_ge500ms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_gold_feats(align_dict, gold_feats_dict_fname, es_key=\"es\"):\n",
    "    gold_feats_dict = {}\n",
    "    for fid in align_dict:\n",
    "        for sid in align_dict[fid]:\n",
    "            gold_feats_dict[sid] = {}\n",
    "            if align_dict[fid][sid][es_key] == []:\n",
    "                # Only es_cnt can be empty, in which case include stop words\n",
    "                gold_feats_dict[sid] = [w.word for w in align_dict[fid][sid]['es']]\n",
    "            else:\n",
    "                gold_feats_dict[sid] = [w.word for w in align_dict[fid][sid][es_key]]\n",
    "    print(\"Saving gold features using key: %s\" % es_key)\n",
    "    pickle.dump(gold_feats_dict, open(gold_feats_dict_fname, \"wb\"))\n",
    "    print(\"finished ...\")\n",
    "    return gold_feats_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gold_feats():\n",
    "    align_dict = pickle.load(open(align_dict_fname, \"rb\"))\n",
    "    gold_feats_dict_fname = config['es']['gold_feats']\n",
    "    gold_feats_dict = create_gold_feats(align_dict, gold_feats_dict_fname, es_key=\"es_cnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check English translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_words = [a.word for fid in align_dict for sid in align_dict[fid] for a in align_dict[fid][sid]['es']]\n",
    "es_cnt_words = [a.word for fid in align_dict for sid in align_dict[fid] for a in align_dict[fid][sid]['es_cnt']]\n",
    "en_words = [a.word for fid in align_dict for sid in align_dict[fid] for a in align_dict[fid][sid]['en']]\n",
    "en_cnt_words = [a.word for fid in align_dict for sid in align_dict[fid] for a in align_dict[fid][sid]['en_cnt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es_words_freq = Counter(es_words)\n",
    "es_cnt_words_freq = Counter(es_cnt_words)\n",
    "en_words_freq = Counter(en_words)\n",
    "en_cnt_words_freq = Counter(en_cnt_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('THE', 5178), ('AND', 4629), ('THAT', 4080), ('I', 3849), ('TO', 3359), ('YES', 3345), (\"'T\", 2265), ('YOU', 2256), ('NO', 2135), (\"'S\", 2030)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(en_words_freq.items(), reverse=True, key=lambda t:t[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('YES', 3345), (\"'T\", 2265), (\"'S\", 2030), ('WELL', 1829), ('AH', 1349), ('KNOW', 1100), ('OH', 1066), ('SEE', 934), ('YEAH', 904), ('LIKE', 889)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(en_cnt_words_freq.items(), reverse=True, key=lambda t:t[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('QUE', 7089), ('NO', 6110), ('Y', 5037), ('A', 4310), ('DE', 4009), ('S\\xc3\\xad', 3667), ('LA', 3425), ('YA', 2782), ('EL', 2680), ('ES', 2587)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(es_words_freq.items(), reverse=True, key=lambda t:t[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AH', 1831), ('PUES', 1236), ('BUENO', 1186), ('BIEN', 1183), ('SI', 1045), ('<LAUGH>', 987), ('MMM', 976), ('AS\\xc3\\xad', 781), ('ENTONCES', 775), ('CLARO', 729)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(es_cnt_words_freq.items(), reverse=True, key=lambda t:t[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"'T\", 2265), (\"'S\", 2030), (\"'R\", 1), (\"'D\", 34), (\"'M\", 627), (\"'TS\", 1), (\"'VE\", 184), (\"'\", 40), (\"'OEUVRES\", 1), (\"'RE\", 269), (\"'CLOCK\", 4), (\"'LL\", 540), (\"O'CLOCK\", 1), (\"'AM\", 8)]\n"
     ]
    }
   ],
   "source": [
    "print([(w,f) for w, f in en_cnt_words_freq.items() if \"'\" in w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<SNEEZE>', 4), ('<COUGH>', 11), ('<LAUGH>', 987), ('<BREATH>', 16), ('<NOISE>', 450), ('<BACKGROUND>', 107)]\n"
     ]
    }
   ],
   "source": [
    "print([(w,f) for w, f in es_words_freq.items() if \"<\" in w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<SNEEZE>', 4), ('<COUGH>', 11), ('<LAUGH>', 987), ('<BREATH>', 16), ('<NOISE>', 450), ('<BACKGROUND>', 107)]\n"
     ]
    }
   ],
   "source": [
    "print([(w,f) for w, f in es_cnt_words_freq.items() if \"<\" in w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Key-word spotting, prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_proto_list():\n",
    "    protos = []\n",
    "    with open(config['proto']['protos_list'], \"r\") as f:\n",
    "        for line in f:\n",
    "            protos.append(line.strip())\n",
    "    return protos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_vad_norm_plp_for_protos(protos):\n",
    "    if not os.path.exists(config['proto']['vad_path']):\n",
    "        os.makedirs(config['proto']['vad_path'])\n",
    "    if not os.path.exists(config['proto']['norm_plp_path']):\n",
    "        os.makedirs(config['proto']['norm_plp_path'])\n",
    "    if not os.path.exists(config['proto']['proto_float32_path']):\n",
    "        os.makedirs(config['proto']['proto_float32_path'])\n",
    "    if not os.path.exists(config['proto']['lsh_path']):\n",
    "        os.makedirs(config['proto']['lsh_path'])\n",
    "        \n",
    "    sys.stderr.flush()\n",
    "    with tqdm(total=len(protos)) as pbar:\n",
    "        for i, pid in enumerate(protos, start=1):\n",
    "            try:\n",
    "                pid_base = os.path.splitext(pid)[0]\n",
    "                plp_fname = os.path.join(config['proto']['proto_path'], pid)\n",
    "                plp_f32_fname = os.path.join(config['proto']['proto_float32_path'], pid)\n",
    "                plp_norm_fname = (os.path.join(config['proto']['norm_plp_path'], \n",
    "                                          \"{0:s}.std.binary\".format(pid_base)))\n",
    "                vad_fname = (os.path.join(config['proto']['vad_path'], \"{0:s}.vad\".format(pid_base)))\n",
    "                lsh_fname = (os.path.join(config['proto']['lsh_path'], \n",
    "                                          \"{0:s}.std.lsh64\".format(pid_base)))\n",
    "                # read npy file to get shape\n",
    "                x = np.load(plp_fname)\n",
    "                # ZRTools use float32\n",
    "                y = x.astype(np.float32)\n",
    "                y.tofile(plp_f32_fname)\n",
    "\n",
    "                # create vad file\n",
    "                with open(vad_fname, \"w\") as f:\n",
    "                    f.write(\"0\\t{0:d}\\n\".format(y.shape[0]))\n",
    "\n",
    "                # normalize plp\n",
    "                normalize_plp(plp_f32_fname, vad_fname, plp_norm_fname)\n",
    "                #print(os.path.getsize(plp_fname))\n",
    "\n",
    "                # create lsh\n",
    "                create_lsh_file(plp_norm_fname, vad_fname, lsh_proj_fname, lsh_fname)\n",
    "                \n",
    "                # update progress\n",
    "                pbar.set_description(\"processing proto {0:s}\".format(pid_base))\n",
    "                pbar.update(1)\n",
    "            except:\n",
    "                print(\" problem in file: {0:s}\".format(pid), end=\",\")\n",
    "    print(\"completed\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_lsh_for_call_norm_plps():\n",
    "    if not os.path.exists(config['proto']['call_vad_path']):\n",
    "        print(\"VAD files not found in folder: {0:s}\".format(config['proto']['call_vad_path']))\n",
    "        return\n",
    "    if not os.path.exists(config['proto']['call_plp_path']):\n",
    "        print(\"PLP files not found in folder: {0:s}\".format(config['proto']['call_plp_path']))\n",
    "        return\n",
    "    if not os.path.exists(config['proto']['call_lsh_path']):\n",
    "        os.makedirs(config['proto']['call_lsh_path'])\n",
    "    \n",
    "    sys.stderr.flush()\n",
    "    calls = [f for f in os.listdir(config['proto']['call_plp_path']) if f.endswith(\".std.binary\")]\n",
    "    with tqdm(total=len(calls)) as pbar:\n",
    "        for i, call in enumerate(calls, start=1):\n",
    "            try:\n",
    "                call_base = call.replace(\".std.binary\", \"\")\n",
    "                plp_norm_fname = (os.path.join(config['proto']['call_plp_path'], call))\n",
    "                vad_fname = (os.path.join(config['proto']['call_vad_path'], \n",
    "                                          \"{0:s}.vad\".format(call_base)))\n",
    "                lsh_fname = (os.path.join(config['proto']['call_lsh_path'], \n",
    "                                          \"{0:s}.std.lsh64\".format(call_base)))\n",
    "\n",
    "                # read npy file - normalized plp\n",
    "                x = np.fromfile(plp_norm_fname)\n",
    "                # ZRTools use float32\n",
    "                y = x.astype(np.float32)\n",
    "                y.tofile(plp_norm_fname)\n",
    "\n",
    "                # create lsh\n",
    "                create_lsh_file(plp_norm_fname, vad_fname, lsh_proj_fname, lsh_fname)\n",
    "                \n",
    "                # update progress\n",
    "                pbar.set_description(\"processing proto {0:s}\".format(call_base))\n",
    "                pbar.update(1)\n",
    "                #print(i)\n",
    "            except:\n",
    "                print(\"problem in file: {0:s}\".format(call), end=\", \")\n",
    "    print(\"completed\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10TH.npy', '13.npy', '155.npy', '20TH.npy', '5TH.npy']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protos = read_proto_list()\n",
    "protos[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto CATALOGS:  14%|█▍        | 805/5703 [00:20<02:09, 37.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: CASTAñEDA.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto DOOR:  27%|██▋       | 1549/5703 [00:40<01:45, 39.24it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: DOñIHUE.npy, problem in file: DOñINHUE.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto FETCH:  34%|███▎      | 1924/5703 [00:49<01:35, 39.78it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: FERNáNDEZ.npy, problem in file: FERNáN.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto HIGHLY:  42%|████▏     | 2388/5703 [01:01<01:28, 37.65it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: HERNáNDEZ.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto HUGS:  43%|████▎     | 2470/5703 [01:03<01:21, 39.91it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: HUARáS.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto ILLUSION:  44%|████▍     | 2515/5703 [01:04<01:21, 39.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: IGUAZú.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto JACKASS:  47%|████▋     | 2679/5703 [01:08<01:14, 40.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: IVáN.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto JUANITA:  48%|████▊     | 2735/5703 [01:10<01:11, 41.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: JOSé.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto LATELY:  50%|████▉     | 2841/5703 [01:13<01:11, 40.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: LARRAñAGA.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto MARIANELA:  54%|█████▍    | 3107/5703 [01:19<01:03, 40.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: MARAñON.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "processing proto MARIBI:  55%|█████▍    | 3112/5703 [01:20<01:03, 40.74it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: MARíA.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto MENA:  56%|█████▌    | 3204/5703 [01:22<01:02, 39.69it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: MéLIDA.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto MISSIONARY:  58%|█████▊    | 3288/5703 [01:24<00:59, 40.30it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: MISIóN.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto MONTERREY:  58%|█████▊    | 3328/5703 [01:25<01:00, 39.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: MóNICA.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto NIKI:  61%|██████    | 3458/5703 [01:28<00:56, 39.68it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: NICOLáS.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto NU:  62%|██████▏   | 3516/5703 [01:30<00:52, 41.67it/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: ´.npy, problem in file: ¨.npy, problem in file: ¡.npy, problem in file: ​.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto OCTAVIO:  62%|██████▏   | 3541/5703 [01:30<00:54, 39.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: OCAñA.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto PRY:  71%|███████▏  | 4067/5703 [01:44<00:39, 40.93it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: protos.list,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto REACTION:  73%|███████▎  | 4173/5703 [01:47<00:40, 38.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " problem in file: RAúL.npy,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto ZULEMA: 100%|█████████▉| 5679/5703 [02:26<00:00, 38.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_vad_norm_plp_for_protos(protos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto 048: 100%|██████████| 104/104 [00:05<00:00, 19.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_lsh_for_call_norm_plps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_keyword_spotting_cmd_scripts(exp_path, wav_file_list, protos, exp_name, num_splits=1):\n",
    "    disc_file_split_base = \"keyword_spot_{0:d}.cmd\"\n",
    "    disc_file_split = os.path.join(exp_path, disc_file_split_base)\n",
    "    disc_split_file = os.path.join(exp_path, \"keyword_spot_split.txt\")\n",
    "    \n",
    "    num_files = len(wav_file_list)\n",
    "    num_protos = len(protos)\n",
    "    \n",
    "    exp_local_path = os.path.join(\"exp\", exp_name)\n",
    "    cmd_string = \"scripts/plebdisc_filepair_keyword_spotting \\\"{0:s}\\\" \\\"{1:s}\\\" {2:s} 39\\n\"\n",
    "\n",
    "    total_lines = num_files * num_protos\n",
    "    lines_per_file = total_lines // num_splits\n",
    "    smallfile = None\n",
    "    curr_line = 0\n",
    "    curr_file_num = 0\n",
    "    \n",
    "    sys.stderr.flush()\n",
    "    with tqdm(total=num_protos) as pbar:\n",
    "        for i in xrange(num_protos):\n",
    "            pid_base = os.path.splitext(protos[i])[0]\n",
    "            pbar.update(1)\n",
    "#             if i % 20 == 0:\n",
    "#                 print(\"Progress: {0:d} out of: {1:d}\".format(curr_line+1, total_lines))\n",
    "            for j in xrange(num_files):\n",
    "                out_line = cmd_string.format(pid_base, wav_file_list[j], exp_local_path)\n",
    "                if curr_line % lines_per_file == 0:\n",
    "                    if smallfile:\n",
    "                        smallfile.close()\n",
    "                    small_filename = disc_file_split.format(curr_file_num)\n",
    "                    smallfile = open(small_filename, \"w\")\n",
    "                    curr_file_num += 1\n",
    "                smallfile.write(out_line)\n",
    "                curr_line += 1\n",
    "    if smallfile:\n",
    "        smallfile.close()\n",
    "\n",
    "    # Making a list of commands to execute the split disc list\n",
    "    full_split_cmd_string = \"nice sh {0:s} 1> {1:s} 2>{2:s} &\\n\"\n",
    "    split_cmd = os.path.join(exp_local_path, \"matches\",\"{0:s}.{1:d}\")\n",
    "    with open(disc_split_file, \"w\") as out_f:\n",
    "        for i in xrange(curr_file_num):\n",
    "            curr_split_file = os.path.join(exp_local_path, disc_file_split_base.format(i))\n",
    "            split_cmd_out = split_cmd.format(\"keyword_spot_out\", i)\n",
    "            #split_cmd_err = split_cmd.format(\"err\", i)\n",
    "            split_cmd_err = \"/dev/null\"\n",
    "\n",
    "            out_line = \"nice sh \"\n",
    "            out_f.write(full_split_cmd_string.format(curr_split_file, \\\n",
    "                                                    split_cmd_out, \\\n",
    "                                                    split_cmd_err))\n",
    "\n",
    "    print(\"Completed - keyword_spot cmd script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5703/5703 [00:00<00:00, 18350.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed - keyword_spot cmd script\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_keyword_spotting_cmd_scripts(exp_path=exp_path, wav_file_list=wav_file_list[:33], protos=protos,\n",
    "                                    exp_name=exp_name, num_splits=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['001',\n",
       " '002',\n",
       " '005',\n",
       " '006',\n",
       " '007',\n",
       " '009',\n",
       " '010',\n",
       " '011',\n",
       " '012',\n",
       " '013',\n",
       " '014',\n",
       " '015',\n",
       " '018',\n",
       " '021',\n",
       " '022',\n",
       " '023',\n",
       " '024',\n",
       " '025',\n",
       " '026',\n",
       " '027',\n",
       " '028',\n",
       " '029',\n",
       " '030',\n",
       " '031',\n",
       " '032',\n",
       " '033',\n",
       " '034',\n",
       " '035',\n",
       " '036',\n",
       " '037',\n",
       " '038',\n",
       " '039',\n",
       " '040']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_file_list[:33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5703"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(protos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto 999999: 100%|██████████| 1000000/1000000 [00:02<00:00, 463305.73it/s]\n"
     ]
    }
   ],
   "source": [
    "haha = range(1000000)\n",
    "with tqdm(total=len(haha)) as pbar:\n",
    "    sys.stderr.flush()\n",
    "    for i in haha:\n",
    "        pbar.set_description(\"processing proto {0:d}\".format(i))\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
