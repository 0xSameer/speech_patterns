{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import subprocess\n",
    "from IPython.display import display\n",
    "from IPython.display import Audio\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"config.json\") as json_data_file:\n",
    "    config = json.load(json_data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing CALLHOME for ZRTools\n",
    "\n",
    "\n",
    "- Created: 26-Oct-2016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mapping for start time for each segment\n",
    "\n",
    "Format: Dictionary  \n",
    "key: {key: value}  \n",
    "*file: {file.seg.wav: start time}*  \n",
    "Name: segment_start.dict, segment_start.txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_segments_file(seg_fname):\n",
    "    segment_map = {}\n",
    "    with open(seg_fname, \"r\") as seg_f:\n",
    "        for i, line in enumerate(seg_f):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            try:\n",
    "                line_items = line.strip().split()\n",
    "                seg_key = line_items[0]\n",
    "                file_id = line_items[1]\n",
    "                if file_id not in segment_map:\n",
    "                    segment_map[file_id] = {}\n",
    "                seg_start = int(float(line_items[6])*100)\n",
    "                segment_map[file_id][seg_key] = seg_start\n",
    "            except ValueError:\n",
    "                print(\"Incorrect line format at line: %d\" % i)\n",
    "    return segment_map\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read segment map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_segment_map():\n",
    "    segment_map = read_segments_file('../segments.txt')\n",
    "    pickle.dump(segment_map, open(config['es']['segment_dict_fname'], \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VAD files for merged wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_merged_vad_from_ed(vad_file_id, segment_map, seg_vad_path, merged_vad_path):\n",
    "    total_dur_10ms = 0\n",
    "    total_dur_10ms_ge500ms = 0\n",
    "    with open(os.path.join(merged_vad_path, vad_file_id+\".vad\"), \"w\") as vad_f:\n",
    "        print(\"creating vad %s ...\" % vad_file_id)\n",
    "        for i, (seg_id, seg_start) in enumerate(sorted(segment_map[vad_file_id].items(), key=lambda t:t[0])):\n",
    "            with open(os.path.join(seg_vad_path, seg_id+\".vad\"), \"r\") as seg_vad_f:\n",
    "                for line in seg_vad_f:\n",
    "                    line_items = map(int, line.strip().split())\n",
    "                    start = seg_start+line_items[0]\n",
    "                    end = seg_start+line_items[1]\n",
    "                    total_dur_10ms += (end-start)\n",
    "                    total_dur_10ms_ge500ms += ((end - start) if (end-start) >= 50 else 0)\n",
    "                    out_line = (\"%d %d\\n\" %(start, end))\n",
    "                    vad_f.write(out_line)\n",
    "                # end for\n",
    "            # end reading seg file\n",
    "        # end looping over all segments\n",
    "    # end writing vad file\n",
    "    return total_dur_10ms, total_dur_10ms_ge500ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new directory for merged vads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merged_ed_vads_path = \"../mergedVads\"\n",
    "# seg_vad_path = \"../vad\"\n",
    "# if not os.path.exists(merged_ed_vads_path):\n",
    "#     os.makedirs(merged_ed_vads_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create merged vad for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# total_dur_10ms, total_dur_10ms_ge500ms = 0, 0\n",
    "# for vad_file_id in segment_map:\n",
    "#     t1, t2 = create_merged_vad(vad_file_id, segment_map, seg_vad_path, merged_vads_path)\n",
    "#     total_dur_10ms += t1\n",
    "#     total_dur_10ms_ge500ms += t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(total_dur_10ms, total_dur_10ms_ge500ms)\n",
    "# print(map(lambda t: \"{0:.3f}\".format((t / 100.0 / 3600)), [total_dur_10ms, total_dur_10ms_ge500ms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create PLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_wavs_path = \"../mergeWavs\"\n",
    "plp_path = \"../plp\"\n",
    "plp_norm_path = \"../std_plp\"\n",
    "if not os.path.exists(plp_path):\n",
    "    os.makedirs(plp_path)\n",
    "if not os.path.exists(plp_norm_path):\n",
    "    os.makedirs(plp_norm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_file_lst(file_lst_fname):\n",
    "    prefix = \"../corpora/callhome/mergeWavs\"\n",
    "    wav_file_list = [os.path.join(prefix, wav_file) for \\\n",
    "                     wav_file in os.listdir(merged_wavs_path) if wav_file.endswith(\".wav\")]\n",
    "    wav_file_list_string = \"\\n\".join(wav_file_list)\n",
    "    with open(file_lst_fname, \"w\") as out_f:\n",
    "        out_f.write(wav_file_list_string)\n",
    "    print(\"Finished writing files.lst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create_file_lst(config[\"es\"][\"lst_file\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_plp(wav_fname, plp_fname):\n",
    "    FEACALC = config['base'][\"feacalc\"]\n",
    "    subprocess.call([FEACALC,\"-plp\", \\\n",
    "                    \"12\", \"-cep\", \"13\", \"-dom\", \"cep\", \"-deltaorder\", \\\n",
    "                    \"2\", \"-dither\", \"-frqaxis\", \"bark\", \"-samplerate\", \\\n",
    "                    \"8000\", \"-win\", \"25\", \"-step\", \"10\", \"-ip\", \\\n",
    "                    \"MSWAVE\", \"-rasta\", \"false\", \"-compress\", \\\n",
    "                    \"true\", \"-op\", \"swappedraw\", \"-o\", plp_fname, wav_fname])\n",
    "\n",
    "    \n",
    "def normalize_plp(plp_fname, vad_fname, plp_norm_fname):\n",
    "    STANDFEAT = config['base'][\"standfeat\"]\n",
    "    # Standardize binary file, for VAD regions only\n",
    "    subprocess.call([STANDFEAT, \"-D\", \"39\", \"-infile\", \\\n",
    "                    plp_fname, \"-outfile\", plp_norm_fname, \\\n",
    "                    \"-vadfile\", vad_fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_and_normalize_plps():\n",
    "    for i, file_id in enumerate(segment_map):\n",
    "        wav_fname = os.path.join(merged_wavs_path, file_id+\".wav\")\n",
    "        vad_fname = os.path.join(merged_fa_vads_path, file_id+\".vad\")\n",
    "        plp_fname = os.path.join(plp_path, file_id+\".binary\")\n",
    "        plp_norm_fname = os.path.join(plp_norm_path, file_id+\".std.binary\")\n",
    "\n",
    "        #print(file_id, wav_fname, vad_fname, plp_fname, plp_norm_fname)\n",
    "\n",
    "        # create PLP\n",
    "        if i % 20 == 0:\n",
    "            print(\"plp for file %s \" % file_id)\n",
    "\n",
    "        #if not os.path.exists(plp_fname):\n",
    "        create_plp(wav_fname, plp_fname)\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(\"normalizing plp %s\" % file_id)\n",
    "\n",
    "        #if not os.path.exists(plp_norm_fname):\n",
    "        normalize_plp(plp_fname, vad_fname, plp_norm_fname)\n",
    "    print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LSH files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsh_path = \"../lsh\"\n",
    "if not os.path.exists(lsh_path):\n",
    "    os.makedirs(lsh_path)\n",
    "lsh_proj_fname = os.path.join(lsh_path, \"proj_S64xD39_seed1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lsh_proj_file(lsh_proj_fname):\n",
    "    subprocess.call([config['base'][\"lsh_genproj\"], \\\n",
    "                     \"-D\",\"39\",\"-S\",\"64\",\"-seed\", \\\n",
    "                     \"1\",\"-projfile\", lsh_proj_fname])\n",
    "\n",
    "def create_lsh_file(plp_norm_fname, vad_fname, lsh_proj_fname, lsh_fname):\n",
    "    LSH = config['base'][\"lsh\"]\n",
    "    subprocess.call([LSH, \"-D\", \"39\", \"-S\", \"64\", \\\n",
    "                    \"-projfile\", lsh_proj_fname, \\\n",
    "                    \"-featfile\", plp_norm_fname, \"-sigfile\", \\\n",
    "                    lsh_fname, \"-vadfile\", vad_fname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(lsh_proj_fname):\n",
    "    create_lsh_proj_file(lsh_proj_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_lsh_files():\n",
    "    for i, file_id in enumerate(segment_map):\n",
    "        wav_fname = os.path.join(merged_wavs_path, file_id+\".wav\")\n",
    "        vad_fname = os.path.join(merged_fa_vads_path, file_id+\".vad\")\n",
    "        plp_norm_fname = os.path.join(plp_norm_path, file_id+\".std.binary\")\n",
    "        lsh_fname = os.path.join(lsh_path, file_id+\".std.lsh64\")\n",
    "\n",
    "        #print(file_id, wav_fname, vad_fname, plp_fname, plp_norm_fname)\n",
    "\n",
    "        # create LSH\n",
    "        if i % 20 == 0:\n",
    "            print(\"lsh for file %s \" % file_id)\n",
    "\n",
    "        #if not os.path.exists(lsh_fname):\n",
    "        create_lsh_file(plp_norm_fname, vad_fname, lsh_proj_fname, lsh_fname)\n",
    "\n",
    "    print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ZRTools discovery command files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_path = '../exp'\n",
    "if not os.path.exists(exp_path):\n",
    "    os.makedirs(exp_path)\n",
    "\n",
    "# List of wav files\n",
    "segment_map = pickle.load(open(config['es']['segment_dict_fname'], \"rb\"))\n",
    "wav_file_list = sorted(segment_map.keys())\n",
    "exp_name = 'callhome'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_files_base():\n",
    "    with open(os.path.join(exp_path, 'files.base'), \"w\") as out_f:\n",
    "        for wav_file in wav_file_list:\n",
    "            out_f.write(wav_file+'\\n')\n",
    "    print(\"Generated files.base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_discovery_cmd_scripts(exp_path, wav_file_list, exp_name, num_splits=1):\n",
    "    disc_file_split_base = \"disc_{0:d}.cmd\"\n",
    "    disc_file_split = os.path.join(exp_path, disc_file_split_base)\n",
    "    disc_split_file = os.path.join(exp_path, \"disc_split.txt\")\n",
    "    num_files = len(wav_file_list)\n",
    "    exp_local_path = os.path.join(\"exp\", exp_name)\n",
    "    cmd_string = \"scripts/plebdisc_filepair \\\"{0:s}\\\" \\\"{1:s}\\\" {2:s} 39\\n\"\n",
    "\n",
    "    total_lines = num_files * num_files\n",
    "    lines_per_file = total_lines // num_splits\n",
    "    smallfile = None\n",
    "    curr_line = 0\n",
    "    curr_file_num = 0\n",
    "\n",
    "    for i in xrange(num_files) :\n",
    "        if i % 20 == 0:\n",
    "            print(\"Progress: {0:d} out of: {1:d}\".format(curr_line+1, total_lines))\n",
    "        for j in xrange(num_files):\n",
    "            out_line = cmd_string.format(wav_file_list[i], \\\n",
    "                                              wav_file_list[j], \\\n",
    "                                              exp_local_path)\n",
    "            if curr_line % lines_per_file == 0:\n",
    "                if smallfile:\n",
    "                    smallfile.close()\n",
    "                small_filename = disc_file_split.format(curr_file_num)\n",
    "                smallfile = open(small_filename, \"w\")\n",
    "                curr_file_num += 1\n",
    "            smallfile.write(out_line)\n",
    "            curr_line += 1\n",
    "    if smallfile:\n",
    "        smallfile.close()\n",
    "\n",
    "    # Making a list of commands to execute the split disc list\n",
    "    full_split_cmd_string = \"nice sh {0:s} 1> {1:s} 2>{2:s} &\\n\"\n",
    "    split_cmd = os.path.join(exp_local_path, \"matches\",\"{0:s}.{1:d}\")\n",
    "    with open(disc_split_file, \"w\") as out_f:\n",
    "        for i in xrange(curr_file_num):\n",
    "            curr_split_file = os.path.join(exp_local_path, disc_file_split_base.format(i))\n",
    "            split_cmd_out = split_cmd.format(\"out\", i)\n",
    "            #split_cmd_err = split_cmd.format(\"err\", i)\n",
    "            split_cmd_err = \"/dev/null\"\n",
    "\n",
    "            out_line = \"nice sh \"\n",
    "            out_f.write(full_split_cmd_string.format(curr_split_file, \\\n",
    "                                                    split_cmd_out, \\\n",
    "                                                    split_cmd_err))\n",
    "\n",
    "    print(\"Completed - disc.cmd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create_discovery_cmd_scripts(exp_path=exp_path, wav_file_list=wav_file_list, exp_name=exp_name, num_splits=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read transcripts, and translations into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Align = namedtuple('Align', ['word', 'start', 'end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_alignment_file(align_fname, stopwords_corpus=None):\n",
    "    align_list = []\n",
    "    with open(align_fname, \"r\") as align_f:\n",
    "        for line in align_f:\n",
    "            line_items = line.strip().split()\n",
    "            if len(line_items) != 3:\n",
    "                raise ValueError\n",
    "            start, end = map(lambda v: int(float(v)*100), line_items[1:3])\n",
    "            if (not stopwords_corpus) or \\\n",
    "            (stopwords_corpus and line_items[0].lower() not in stopwords_corpus):\n",
    "                align_list.append(Align(*[line_items[0], start, end]))\n",
    "    if sorted(align_list, key=lambda t: t.start) != align_list and not align_fname.endswith(\"en\"):\n",
    "        raise IOError    \n",
    "            \n",
    "    return align_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_words_path = '../wav2es-words/'\n",
    "en_words_path = '../wav2eng-words/'\n",
    "align_dict_fname = config['es']['align_dict_fname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # test code\n",
    "# stopwords_es = set(stopwords.words('spanish'))\n",
    "# stopwords_en = set(stopwords.words('english'))\n",
    "# display(read_alignment_file('../wav2es-words/001.001.es'))\n",
    "# display(read_alignment_file('../wav2es-words/001.001.es', stopwords_corpus=stopwords_es))\n",
    "# display(read_alignment_file('../wav2eng-words/001.001.en'))\n",
    "# display(read_alignment_file('../wav2eng-words/001.001.en', stopwords_corpus=stopwords_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_list(file_path, file_ext):\n",
    "    return [os.path.splitext(f)[0] for f in os.listdir(file_path) if f.endswith(file_ext)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "{'009.025'}\n"
     ]
    }
   ],
   "source": [
    "es_file_list = get_file_list(es_words_path, 'es')\n",
    "en_file_list = get_file_list(en_words_path, 'en')\n",
    "\n",
    "print(sorted(es_file_list) == sorted(en_file_list))\n",
    "print(set(es_file_list)-set(en_file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_alignment_dict():\n",
    "    align_dict = {}\n",
    "    stopwords_es = set(stopwords.words('spanish'))\n",
    "    stopwords_en = set(stopwords.words('english'))\n",
    "    for file_id in sorted(segment_map):\n",
    "        print(\"Processing file: %s\" % file_id)\n",
    "        align_dict[file_id] = {}\n",
    "        for seg_id in segment_map[file_id]:\n",
    "            align_dict[file_id][seg_id] = {}\n",
    "            es_fname = os.path.join(es_words_path, seg_id+\".es\")\n",
    "            en_fname = os.path.join(en_words_path, seg_id+\".en\")\n",
    "            align_dict[file_id][seg_id][\"es\"] = read_alignment_file(es_fname)\n",
    "            align_dict[file_id][seg_id][\"en\"] = read_alignment_file(en_fname)\n",
    "            align_dict[file_id][seg_id][\"es_cnt\"] = read_alignment_file(es_fname, stopwords_corpus=stopwords_es)\n",
    "            align_dict[file_id][seg_id][\"en_cnt\"] = read_alignment_file(en_fname, stopwords_corpus=stopwords_en)\n",
    "    return align_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: 090\n",
      "Processing file: 091\n",
      "Processing file: 092\n",
      "Processing file: 093\n",
      "Processing file: 094\n",
      "Processing file: 095\n",
      "Processing file: 096\n",
      "Processing file: 097\n",
      "Processing file: 010\n",
      "Processing file: 011\n",
      "Processing file: 012\n",
      "Processing file: 013\n",
      "Processing file: 014\n",
      "Processing file: 015\n",
      "Processing file: 018\n",
      "Processing file: 025\n",
      "Processing file: 024\n",
      "Processing file: 027\n",
      "Processing file: 026\n",
      "Processing file: 021\n",
      "Processing file: 023\n",
      "Processing file: 022\n",
      "Processing file: 029\n",
      "Processing file: 028\n",
      "Processing file: 115\n",
      "Processing file: 114\n",
      "Processing file: 038\n",
      "Processing file: 039\n",
      "Processing file: 111\n",
      "Processing file: 110\n",
      "Processing file: 113\n",
      "Processing file: 112\n",
      "Processing file: 032\n",
      "Processing file: 033\n",
      "Processing file: 030\n",
      "Processing file: 031\n",
      "Processing file: 036\n",
      "Processing file: 037\n",
      "Processing file: 034\n",
      "Processing file: 035\n",
      "Processing file: 051\n",
      "Processing file: 108\n",
      "Processing file: 049\n",
      "Processing file: 048\n",
      "Processing file: 047\n",
      "Processing file: 046\n",
      "Processing file: 045\n",
      "Processing file: 044\n",
      "Processing file: 043\n",
      "Processing file: 042\n",
      "Processing file: 041\n",
      "Processing file: 040\n",
      "Processing file: 058\n",
      "Processing file: 059\n",
      "Processing file: 103\n",
      "Processing file: 054\n",
      "Processing file: 056\n",
      "Processing file: 057\n",
      "Processing file: 050\n",
      "Processing file: 100\n",
      "Processing file: 052\n",
      "Processing file: 053\n",
      "Processing file: 101\n",
      "Processing file: 106\n",
      "Processing file: 107\n",
      "Processing file: 104\n",
      "Processing file: 105\n",
      "Processing file: 061\n",
      "Processing file: 060\n",
      "Processing file: 063\n",
      "Processing file: 062\n",
      "Processing file: 065\n",
      "Processing file: 064\n",
      "Processing file: 067\n",
      "Processing file: 066\n",
      "Processing file: 117\n",
      "Processing file: 116\n",
      "Processing file: 076\n",
      "Processing file: 077\n",
      "Processing file: 075\n",
      "Processing file: 072\n",
      "Processing file: 073\n",
      "Processing file: 070\n",
      "Processing file: 071\n",
      "Processing file: 078\n",
      "Processing file: 079\n",
      "Processing file: 119\n",
      "Processing file: 118\n",
      "Processing file: 089\n",
      "Processing file: 088\n",
      "Processing file: 083\n",
      "Processing file: 082\n",
      "Processing file: 081\n",
      "Processing file: 087\n",
      "Processing file: 086\n",
      "Processing file: 085\n",
      "Processing file: 084\n",
      "Processing file: 002\n",
      "Processing file: 001\n",
      "Processing file: 007\n",
      "Processing file: 006\n",
      "Processing file: 005\n",
      "Processing file: 009\n",
      "Processing file: 120\n"
     ]
    }
   ],
   "source": [
    "align_dict = create_alignment_dict()\n",
    "pickle.dump(align_dict, open(align_dict_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create VAD from alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "align_dict = pickle.load(open(align_dict_fname, \"rb\"))\n",
    "segment_map = pickle.load(open(config['es']['segment_dict_fname'], \"rb\"))\n",
    "# has_500ms_fa_vad_dict_fname = config['es']['has_500ms_fa_vad_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_uttr_vad_from_alignment(align_dict, vad_path):\n",
    "    has_500ms_dur = {}\n",
    "    total_dur_10ms = 0\n",
    "    total_dur_10ms_ge500ms = 0\n",
    "    for i, vad_file_id in enumerate(align_dict):\n",
    "        if i % 20 == 0:\n",
    "            print(\"Created vad for %d files id\" % i)\n",
    "        for seg_id in align_dict[vad_file_id]:\n",
    "            with open(os.path.join(vad_path, seg_id+\".vad\"), \"w\") as vad_f:\n",
    "                dur_10ms = 0\n",
    "                dur_10ms_ge500ms = 0\n",
    "                vad_list = []\n",
    "                # start index\n",
    "                s = 0\n",
    "                # create a local list of alignment values\n",
    "                align_list = align_dict[vad_file_id][seg_id]['es']\n",
    "                for j in xrange(len(align_list)):\n",
    "                    # if 1st or last element, add to vad_list\n",
    "                    if ((j+1) == len(align_list)) or (align_list[j].end != align_list[j+1].start):\n",
    "                        vad_list.append(((align_list[s].start), (align_list[j].end)))\n",
    "                        s=j+1\n",
    "                # write vad list to file        \n",
    "                for vad_tup in vad_list:\n",
    "                    start = vad_tup[0]\n",
    "                    end = vad_tup[1]\n",
    "                    dur_10ms += (end-start)\n",
    "                    dur_10ms_ge500ms += ((end - start) if (end-start) >= 50 else 0)\n",
    "                    out_line = (\"%d %d\\n\" %(start, end))\n",
    "                    vad_f.write(out_line)\n",
    "                \n",
    "                # set whether atleast one vad region of 500 ms\n",
    "                has_500ms_dur[seg_id] = (dur_10ms_ge500ms > 0)\n",
    "                # compute total durations\n",
    "                total_dur_10ms += dur_10ms\n",
    "                total_dur_10ms_ge500ms += dur_10ms_ge500ms \n",
    "                    \n",
    "            # end for\n",
    "        # end looping over all segments\n",
    "    # end writing vad file\n",
    "    return total_dur_10ms, total_dur_10ms_ge500ms, has_500ms_dur\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_merged_vad_from_alignment(vad_file_id, align_dict, segment_map, vad_path):\n",
    "    total_dur_10ms = 0\n",
    "    total_dur_10ms_ge500ms = 0\n",
    "    with open(os.path.join(vad_path, vad_file_id+\".vad\"), \"w\") as vad_f:\n",
    "        print(\"creating vad %s ...\" % vad_file_id)\n",
    "        for i, (seg_id, seg_start) in enumerate(sorted(segment_map[vad_file_id].items(), key=lambda t:t[0])):\n",
    "            vad_list = []\n",
    "            # start index\n",
    "            s = 0\n",
    "            # create a local list of alignment values\n",
    "            align_list = align_dict[vad_file_id][seg_id]['es']\n",
    "            for j in xrange(len(align_list)):\n",
    "                # if 1st or last element, add to vad_list\n",
    "                if ((j+1) == len(align_list)) or (align_list[j].end != align_list[j+1].start):\n",
    "                    vad_list.append(((seg_start+align_list[s].start), (seg_start+align_list[j].end)))\n",
    "                    s=j+1\n",
    "            # write vad list to file        \n",
    "            for vad_tup in vad_list:\n",
    "                start = vad_tup[0]\n",
    "                end = vad_tup[1]\n",
    "                total_dur_10ms += (end-start)\n",
    "                total_dur_10ms_ge500ms += ((end - start) if (end-start) >= 50 else 0)\n",
    "                out_line = (\"%d %d\\n\" %(start, end))\n",
    "                vad_f.write(out_line)\n",
    "            # end for\n",
    "        # end looping over all segments\n",
    "    # end writing vad file\n",
    "    return total_dur_10ms, total_dur_10ms_ge500ms\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new directory for merged vads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uttr_fa_vads_path = config['es']['es_uttr_fa_vad']\n",
    "if not os.path.exists(uttr_fa_vads_path):\n",
    "    os.makedirs(uttr_fa_vads_path)\n",
    "\n",
    "merged_fa_vads_path = config['es']['es_merge_fa_vad']\n",
    "if not os.path.exists(merged_fa_vads_path):\n",
    "    os.makedirs(merged_fa_vads_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# t1, t2, has_500ms_dur = create_uttr_vad_from_alignment(align_dict, uttr_fa_vads_path)\n",
    "# # print(t1, t2)\n",
    "# print(map(lambda t: \"{0:.3f} hrs\".format((t / 100.0 / 3600)), [t1, t2]))\n",
    "# print(\"saving dict: %s\" % has_500ms_fa_vad_dict_fname)\n",
    "# pickle.dump(has_500ms_dur, open(has_500ms_fa_vad_dict_fname, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # check how many utterances have atleast 500ms VAD\n",
    "# print(\"total utterances: %d\" % len(has_500ms_dur))\n",
    "# uttrs_with_500ms = {k:v for k, v in has_500ms_dur.items() if v}\n",
    "# print(\"with 500ms: %d\" % len(uttrs_with_500ms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save dev file with only 500ms segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dev_500ms_fname = config['es']['mt_dev_500ms_files']\n",
    "# dev_fname = config['es']['mt_dev_test_files']\n",
    "# with open(dev_fname, \"r\") as dev_f, open(dev_500ms_fname, \"w\") as dev_500ms_f:\n",
    "#     for line in dev_f:\n",
    "#         if line.strip() in has_500ms_dur and has_500ms_dur[line.strip()]:\n",
    "#             dev_500ms_f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !wc $dev_500ms_fname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create merged vad for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_merged_vads():\n",
    "    total_dur_10ms, total_dur_10ms_ge500ms = 0, 0\n",
    "    for vad_file_id in segment_map:\n",
    "        t1, t2 = create_vad_from_alignment(vad_file_id, align_dict, segment_map, merged_fa_vads_path)\n",
    "        total_dur_10ms += t1\n",
    "        total_dur_10ms_ge500ms += t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(total_dur_10ms, total_dur_10ms_ge500ms)\n",
    "# print(map(lambda t: \"{0:.3f}\".format((t / 100.0 / 3600)), [total_dur_10ms, total_dur_10ms_ge500ms]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_gold_feats(align_dict, gold_feats_dict_fname, es_key=\"es\"):\n",
    "    gold_feats_dict = {}\n",
    "    for fid in align_dict:\n",
    "        for sid in align_dict[fid]:\n",
    "            gold_feats_dict[sid] = {}\n",
    "            if align_dict[fid][sid][es_key] == []:\n",
    "                # Only es_cnt can be empty, in which case include stop words\n",
    "                gold_feats_dict[sid] = [w.word for w in align_dict[fid][sid]['es']]\n",
    "            else:\n",
    "                gold_feats_dict[sid] = [w.word for w in align_dict[fid][sid][es_key]]\n",
    "    print(\"Saving gold features using key: %s\" % es_key)\n",
    "    pickle.dump(gold_feats_dict, open(gold_feats_dict_fname, \"wb\"))\n",
    "    print(\"finished ...\")\n",
    "    return gold_feats_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gold_feats():\n",
    "    align_dict = pickle.load(open(align_dict_fname, \"rb\"))\n",
    "    gold_feats_dict_fname = config['es']['gold_feats']\n",
    "    gold_feats_dict = create_gold_feats(align_dict, gold_feats_dict_fname, es_key=\"es_cnt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check English translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "es_words = [a.word for fid in align_dict for sid in align_dict[fid] for a in align_dict[fid][sid]['es']]\n",
    "es_cnt_words = [a.word for fid in align_dict for sid in align_dict[fid] for a in align_dict[fid][sid]['es_cnt']]\n",
    "en_words = [a.word for fid in align_dict for sid in align_dict[fid] for a in align_dict[fid][sid]['en']]\n",
    "en_cnt_words = [a.word for fid in align_dict for sid in align_dict[fid] for a in align_dict[fid][sid]['en_cnt']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es_words_freq = Counter(es_words)\n",
    "es_cnt_words_freq = Counter(es_cnt_words)\n",
    "en_words_freq = Counter(en_words)\n",
    "en_cnt_words_freq = Counter(en_cnt_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('THE', 5178), ('AND', 4629), ('THAT', 4080), ('I', 3849), ('TO', 3359), ('YES', 3345), (\"'T\", 2265), ('YOU', 2256), ('NO', 2135), (\"'S\", 2030)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(en_words_freq.items(), reverse=True, key=lambda t:t[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('YES', 3345), (\"'T\", 2265), (\"'S\", 2030), ('WELL', 1829), ('AH', 1349), ('KNOW', 1100), ('OH', 1066), ('SEE', 934), ('YEAH', 904), ('GOING', 889)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(en_cnt_words_freq.items(), reverse=True, key=lambda t:t[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('QUE', 7089), ('NO', 6110), ('Y', 5037), ('A', 4310), ('DE', 4009), ('Sí', 3667), ('LA', 3425), ('YA', 2782), ('EL', 2680), ('ES', 2587)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(es_words_freq.items(), reverse=True, key=lambda t:t[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AH', 1831), ('PUES', 1236), ('BUENO', 1186), ('BIEN', 1183), ('SI', 1045), ('<LAUGH>', 987), ('MMM', 976), ('ASí', 781), ('ENTONCES', 775), ('CLARO', 729)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(es_cnt_words_freq.items(), reverse=True, key=lambda t:t[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"'M\", 627), (\"'S\", 2030), (\"'T\", 2265), (\"'LL\", 540), (\"'VE\", 184), (\"'D\", 34), (\"'RE\", 269), (\"'AM\", 8), (\"'\", 40), (\"'CLOCK\", 4), (\"O'CLOCK\", 1), (\"'OEUVRES\", 1), (\"'R\", 1), (\"'TS\", 1)]\n"
     ]
    }
   ],
   "source": [
    "print([(w,f) for w, f in en_cnt_words_freq.items() if \"'\" in w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<NOISE>', 450), ('<BACKGROUND>', 107), ('<LAUGH>', 987), ('<COUGH>', 11), ('<BREATH>', 16), ('<SNEEZE>', 4)]\n"
     ]
    }
   ],
   "source": [
    "print([(w,f) for w, f in es_words_freq.items() if \"<\" in w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<NOISE>', 450), ('<BACKGROUND>', 107), ('<LAUGH>', 987), ('<COUGH>', 11), ('<BREATH>', 16), ('<SNEEZE>', 4)]\n"
     ]
    }
   ],
   "source": [
    "print([(w,f) for w, f in es_cnt_words_freq.items() if \"<\" in w])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Key-word spotting, prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_proto_list():\n",
    "    protos = []\n",
    "    with open(config['proto']['protos_list'], \"r\") as f:\n",
    "        for line in f:\n",
    "            protos.append(line.strip())\n",
    "    return protos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_vad_norm_plp_for_protos(protos):\n",
    "    if not os.path.exists(config['proto']['vad_path']):\n",
    "        os.makedirs(config['proto']['vad_path'])\n",
    "    if not os.path.exists(config['proto']['proto_plp_binary']):\n",
    "        os.makedirs(config['proto']['proto_plp_binary'])\n",
    "    if not os.path.exists(config['proto']['lsh_path']):\n",
    "        os.makedirs(config['proto']['lsh_path'])\n",
    "        \n",
    "    sys.stderr.flush()\n",
    "    with tqdm(total=len(protos)) as pbar:\n",
    "        for i, pid in enumerate(protos, start=1):\n",
    "            try:\n",
    "                pid_base = os.path.splitext(pid)[0]\n",
    "                plp_fname = os.path.join(config['proto']['proto_path'], pid)\n",
    "                plp_binary_fname = (os.path.join(config['proto']['proto_plp_binary'], \n",
    "                                          \"{0:s}.std.binary\".format(pid_base)))\n",
    "                vad_fname = (os.path.join(config['proto']['vad_path'], \"{0:s}.vad\".format(pid_base)))\n",
    "                lsh_fname = (os.path.join(config['proto']['lsh_path'], \n",
    "                                          \"{0:s}.std.lsh64\".format(pid_base)))\n",
    "                # read npy file to get shape\n",
    "                x = np.load(plp_fname)\n",
    "                # ZRTools use float32, save as binary file\n",
    "                y = x.ravel()\n",
    "                y.tofile(plp_binary_fname)\n",
    "                #np.save(open(plp_binary_fname, \"wb\"), y, allow_pickle=False)\n",
    "\n",
    "                # create vad file\n",
    "                with open(vad_fname, \"w\") as f:\n",
    "                    f.write(\"0\\t{0:d}\\n\".format(y.shape[0]))\n",
    "\n",
    "                # create lsh\n",
    "                create_lsh_file(plp_binary_fname, vad_fname, lsh_proj_fname, lsh_fname)\n",
    "                \n",
    "                # update progress\n",
    "                pbar.set_description(\"processing proto {0:s}\".format(pid_base))\n",
    "                pbar.update(1)\n",
    "            except:\n",
    "                print(\" problem in file: {0:s}\".format(pid), end=\",\")\n",
    "    print(\"completed\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_lsh_for_call_norm_plps():\n",
    "    if not os.path.exists(config['proto']['call_vad_path']):\n",
    "        print(\"VAD files not found in folder: {0:s}\".format(config['proto']['call_vad_path']))\n",
    "        return\n",
    "    if not os.path.exists(config['proto']['call_plp_path']):\n",
    "        print(\"PLP files not found in folder: {0:s}\".format(config['proto']['call_plp_path']))\n",
    "        return\n",
    "    if not os.path.exists(config['proto']['call_lsh_path']):\n",
    "        os.makedirs(config['proto']['call_lsh_path'])\n",
    "    if not os.path.exists(config['proto']['call_plp_binary']):\n",
    "        os.makedirs(config['proto']['call_plp_binary'])\n",
    "    \n",
    "    sys.stderr.flush()\n",
    "    calls = [f for f in os.listdir(config['proto']['call_plp_path']) if f.endswith(\".npy\")]\n",
    "    with tqdm(total=len(calls)) as pbar:\n",
    "        for i, call in enumerate(calls, start=1):\n",
    "            try:\n",
    "                call_base = call.replace(\".plp.npy\", \"\")\n",
    "                plp_fname = (os.path.join(config['proto']['call_plp_path'], call))\n",
    "                plp_binary_fname = (os.path.join(config['proto']['call_plp_binary'], \n",
    "                                          \"{0:s}.std.binary\".format(call_base)))\n",
    "                vad_fname = (os.path.join(config['proto']['call_vad_path'], \n",
    "                                          \"{0:s}.vad\".format(call_base)))\n",
    "                lsh_fname = (os.path.join(config['proto']['call_lsh_path'], \n",
    "                                          \"{0:s}.std.lsh64\".format(call_base)))\n",
    "\n",
    "                # read npy file - normalized plp\n",
    "                x = np.load(plp_fname)\n",
    "                # ZRTools use float32\n",
    "                y = x.ravel()\n",
    "                y.tofile(plp_binary_fname)\n",
    "                #np.savetxt(open(plp_binary_fname, \"w\"), y, allow_pickle=False)\n",
    "\n",
    "                # create lsh\n",
    "                create_lsh_file(plp_binary_fname, vad_fname, lsh_proj_fname, lsh_fname)\n",
    "                \n",
    "                # update progress\n",
    "                pbar.set_description(\"processing proto {0:s}\".format(call_base))\n",
    "                pbar.update(1)\n",
    "                #print(i)\n",
    "            except:\n",
    "                print(\"problem in file: {0:s}\".format(call), end=\", \")\n",
    "    print(\"completed\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lsh_for_protos():\n",
    "    if not os.path.exists(config['proto']['lsh_path']):\n",
    "        os.makedirs(config['proto']['lsh_path'])\n",
    "        \n",
    "    protos = [f.replace(\".std.binary\", \"\") for f in os.listdir(config['proto']['proto_plp_binary']) \n",
    "              if f.endswith(\".std.binary\")]\n",
    "    sys.stderr.flush()\n",
    "    with tqdm(total=len(protos)) as pbar:\n",
    "        for proto in protos:\n",
    "            try:\n",
    "                plp_fname = os.path.join(config['proto']['proto_plp_binary'], \n",
    "                                         \"{0:s}.std.binary\".format(proto))\n",
    "                vad_fname = os.path.join(config['proto']['vad_path'], \n",
    "                                         \"{0:s}.vad\".format(proto))\n",
    "                lsh_fname = os.path.join(config['proto']['lsh_path'], \n",
    "                                         \"{0:s}.lsh\".format(proto))\n",
    "            \n",
    "                # print(plp_fname, vad_fname, lsh_fname)\n",
    "                # create lsh\n",
    "                create_lsh_file(plp_fname, vad_fname, lsh_proj_fname, lsh_fname)\n",
    "\n",
    "                # update progress\n",
    "                pbar.set_description(\"processing proto {0:s}\".format(proto))\n",
    "                pbar.update(1)\n",
    "            except:\n",
    "                print(\"problem in file:\", proto)\n",
    "    print(\"Finished lsh for protos\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto GOSSIP:  19%|█▉        | 1053/5458 [00:11<00:46, 94.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: LARRAñAGA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto FOLLOW:  23%|██▎       | 1247/5458 [00:13<00:42, 99.03it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: FERNáN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto ROBBERY:  24%|██▍       | 1337/5458 [00:14<00:43, 95.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: OCAñA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto GEORGINA:  27%|██▋       | 1458/5458 [00:15<00:39, 102.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: HERNáNDEZ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto FURROW:  30%|██▉       | 1621/5458 [00:17<00:38, 99.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: DOñINHUE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto DAUGHTERS:  33%|███▎      | 1790/5458 [00:18<00:36, 99.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: MARAñON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto BIKE:  42%|████▏     | 2284/5458 [00:24<00:35, 88.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: ¡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto NURSERY:  46%|████▌     | 2500/5458 [00:26<00:29, 99.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: ​\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto ISN:  48%|████▊     | 2604/5458 [00:27<00:29, 95.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: IGUAZú\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto PREGNANT:  53%|█████▎    | 2893/5458 [00:30<00:26, 97.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: FERNáNDEZ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto COMMENT:  57%|█████▋    | 3134/5458 [00:33<00:22, 101.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: ´\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto LETTING:  59%|█████▊    | 3197/5458 [00:34<00:24, 93.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: DOñIHUE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto KUNG:  67%|██████▋   | 3680/5458 [00:39<00:21, 81.53it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: NICOLáS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto SUSPENSION:  72%|███████▏  | 3920/5458 [00:41<00:16, 96.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: JOSé\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto GIRLFRINED:  75%|███████▌  | 4118/5458 [00:44<00:14, 91.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: CASTAñEDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto VARIANT:  78%|███████▊  | 4281/5458 [00:45<00:11, 102.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: MARíA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto ASIAN:  81%|████████  | 4415/5458 [00:47<00:10, 94.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: IVáN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto MIAMI:  82%|████████▏ | 4495/5458 [00:48<00:10, 91.95it/s]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: RAúL\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto DISRESPECTFUL:  89%|████████▉ | 4884/5458 [00:52<00:05, 101.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: ¨\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto EXCHANGE:  92%|█████████▏| 5023/5458 [00:53<00:04, 99.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: MóNICA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto RELAX:  94%|█████████▍| 5123/5458 [00:54<00:03, 85.31it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: MISIóN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto OBVIOSLY:  99%|█████████▉| 5397/5458 [00:57<00:00, 95.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problem in file: MéLIDA\n",
      "problem in file: HUARáS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto LOVE: 100%|█████████▉| 5435/5458 [00:57<00:00, 93.77it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished lsh for protos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_lsh_for_protos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lsh_for_calls():\n",
    "    if not os.path.exists(config['proto']['call_lsh_path']):\n",
    "        os.makedirs(config['proto']['call_lsh_path'])\n",
    "        \n",
    "    calls = [f.replace(\".std.binary\", \"\") for f in os.listdir(config['proto']['call_plp_binary']) \n",
    "              if f.endswith(\".std.binary\")]\n",
    "    sys.stderr.flush()\n",
    "    with tqdm(total=len(calls)) as pbar:\n",
    "        for call in calls:\n",
    "            try:\n",
    "                plp_fname = os.path.join(config['proto']['call_plp_binary'], \n",
    "                                         \"{0:s}.std.binary\".format(call))\n",
    "                vad_fname = os.path.join(config['proto']['call_vad_path'], \n",
    "                                         \"{0:s}.vad\".format(call))\n",
    "                lsh_fname = os.path.join(config['proto']['call_lsh_path'], \n",
    "                                         \"{0:s}.lsh\".format(call))\n",
    "            \n",
    "                #print(plp_fname, vad_fname, lsh_fname)\n",
    "                # create lsh\n",
    "                create_lsh_file(plp_fname, vad_fname, lsh_proj_fname, lsh_fname)\n",
    "\n",
    "                # update progress\n",
    "                pbar.set_description(\"processing proto {0:s}\".format(call))\n",
    "                pbar.update(1)\n",
    "            except:\n",
    "                print(\"problem in file: {0:s}\", call)\n",
    "    print(\"Finished lsh for protos\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing proto 110.175: 100%|██████████| 1314/1314 [00:14<00:00, 88.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished lsh for protos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_lsh_for_calls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# protos = read_proto_list()\n",
    "# protos[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create_vad_norm_plp_for_protos(protos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create_lsh_for_call_norm_plps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "protos = [f.replace(\".lsh\", \"\") for f in os.listdir(config['proto']['lsh_path']) \n",
    "              if f.endswith(\".lsh\")]\n",
    "calls = [f.replace(\".lsh\", \"\") for f in os.listdir(config['proto']['call_lsh_path']) \n",
    "              if f.endswith(\".lsh\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_keyword_spotting_cmd_scripts(exp_path, wav_file_list, protos, exp_name, num_splits=1):\n",
    "    disc_file_split_base = \"keyword_spot_{0:d}.cmd\"\n",
    "    disc_file_split = os.path.join(exp_path, disc_file_split_base)\n",
    "    disc_split_file = os.path.join(exp_path, \"keyword_spot_split.txt\")\n",
    "    \n",
    "    num_files = len(wav_file_list)\n",
    "    num_protos = len(protos)\n",
    "    \n",
    "    exp_local_path = os.path.join(\"exp\", exp_name)\n",
    "    cmd_string = \"scripts/plebdisc_filepair_keyword_spotting \\\"{0:s}\\\" \\\"{1:s}\\\" {2:s} 39\\n\"\n",
    "\n",
    "    total_lines = num_files * num_protos\n",
    "    lines_per_file = total_lines // num_splits\n",
    "    smallfile = None\n",
    "    curr_line = 0\n",
    "    curr_file_num = 0\n",
    "    \n",
    "    sys.stderr.flush()\n",
    "    with tqdm(total=num_protos) as pbar:\n",
    "        for i in xrange(num_protos):\n",
    "            pid_base = os.path.splitext(protos[i])[0]\n",
    "            pbar.update(1)\n",
    "#             if i % 20 == 0:\n",
    "#                 print(\"Progress: {0:d} out of: {1:d}\".format(curr_line+1, total_lines))\n",
    "            for j in xrange(num_files):\n",
    "                out_line = cmd_string.format(pid_base, wav_file_list[j], exp_local_path)\n",
    "                if curr_line % lines_per_file == 0:\n",
    "                    if smallfile:\n",
    "                        smallfile.close()\n",
    "                    small_filename = disc_file_split.format(curr_file_num)\n",
    "                    smallfile = open(small_filename, \"w\")\n",
    "                    curr_file_num += 1\n",
    "                smallfile.write(out_line)\n",
    "                curr_line += 1\n",
    "    if smallfile:\n",
    "        smallfile.close()\n",
    "\n",
    "    # Making a list of commands to execute the split disc list\n",
    "    full_split_cmd_string = \"nice sh {0:s} 1> {1:s} 2>{2:s} &\\n\"\n",
    "    split_cmd = os.path.join(exp_local_path, \"keyword_matches\",\"{0:s}.{1:d}\")\n",
    "    with open(disc_split_file, \"w\") as out_f:\n",
    "        for i in xrange(curr_file_num):\n",
    "            curr_split_file = os.path.join(exp_local_path, \"keyword\", disc_file_split_base.format(i))\n",
    "            split_cmd_out = split_cmd.format(\"keyword_spot_out\", i)\n",
    "            #split_cmd_err = split_cmd.format(\"err\", i)\n",
    "            split_cmd_err = \"/dev/null\"\n",
    "\n",
    "            out_line = \"nice sh \"\n",
    "            out_f.write(full_split_cmd_string.format(curr_split_file, \\\n",
    "                                                    split_cmd_out, \\\n",
    "                                                    split_cmd_err))\n",
    "\n",
    "    print(\"Completed - keyword_spot cmd script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5434/5434 [00:14<00:00, 378.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed - keyword_spot cmd script\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_keyword_spotting_cmd_scripts(exp_path=exp_path, wav_file_list=calls, protos=protos,\n",
    "                                    exp_name=exp_name, num_splits=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'../antonissameer/week2/data/subset-test/lsh'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['proto']['call_lsh_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'052.069',\n",
       " u'101.068',\n",
       " u'046.207',\n",
       " u'118.202',\n",
       " u'059.121',\n",
       " u'077.142',\n",
       " u'116.020',\n",
       " u'118.020',\n",
       " u'046.226',\n",
       " u'077.068']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calls[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5434"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(protos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "haha = range(1000000)\n",
    "with tqdm(total=len(haha)) as pbar:\n",
    "    sys.stderr.flush()\n",
    "    for i in haha:\n",
    "        pbar.set_description(\"processing proto {0:d}\".format(i))\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
